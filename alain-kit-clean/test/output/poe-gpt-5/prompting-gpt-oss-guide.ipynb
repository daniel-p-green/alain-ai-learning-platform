{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1c61d0c",
      "metadata": {},
      "source": [
        "# Prompting GPT-OSS & Getting Started\n",
        "\n",
        "Welcome! This beginner-friendly notebook will help you:\n",
        "- Understand what GPT-OSS is and why you might use it\n",
        "- Learn basic prompt structure and formatting\n",
        "- Try your first prompts interactively\n",
        "\n",
        "Tip: You don't need any prior experience. Just run cells from top to bottom."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7962edb",
      "metadata": {},
      "source": [
        "## What is GPT-OSS?\n",
        "GPT-OSS stands for “GPT-style, Open-Source and Self-hostable” language models. Examples include Llama, Mistral, Phi, and others released under open licenses. They are designed to understand and generate text like proprietary GPT models but can be run locally or via open APIs.\n",
        "\n",
        "Why use GPT-OSS?\n",
        "- Openness: Inspect, fine-tune, and customize models.\n",
        "- Control: Run locally for privacy, or choose your own hosting.\n",
        "- Cost flexibility: Avoid vendor lock-in; use commodity hardware or competitive providers.\n",
        "- Community: Rapid improvement, tools, and examples from open-source contributors.\n",
        "\n",
        "Common ways to access GPT-OSS:\n",
        "- Local runtimes (e.g., llama.cpp, ollama)\n",
        "- Open model hubs and inference APIs (e.g., Hugging Face Inference Endpoints, vLLM servers)\n",
        "- Managed OSS providers that host open models for you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5031d84",
      "metadata": {},
      "source": [
        "## Basic Prompt Structure\n",
        "Good prompts are clear, specific, and give the model the right context. A simple structure:\n",
        "1) Role/Context: Who is the model and what is the scenario?\n",
        "2) Task: What do you want exactly?\n",
        "3) Constraints: Length, style, language, format.\n",
        "4) Examples (optional): Show a small input → output example.\n",
        "5) Inputs: The actual data to process.\n",
        "\n",
        "Example prompt:\n",
        "- Role: “You are a helpful study coach for beginners.”\n",
        "- Task: “Explain gradient descent in simple terms.”\n",
        "- Constraints: “Use a short paragraph and one bullet list. Avoid equations.”\n",
        "- Example (optional): “When I ask about an algorithm, give steps I can follow.”\n",
        "- Input: “Topic: gradient descent”\n",
        "\n",
        "Formatting tips:\n",
        "- Use headings or bullet points for clarity.\n",
        "- Ask for a specific output format (e.g., JSON, list, steps).\n",
        "- State what to avoid (e.g., no code, or no jargon)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2b010b7",
      "metadata": {},
      "source": [
        "## Prompt Patterns You Can Reuse\n",
        "- Instruction: “Do X with Y.”\n",
        "- Chain-of-thought hints (lightweight): “List the steps you would take, then provide the final answer.”\n",
        "- Role-play: “Act as a mentor; ask me 3 clarifying questions first.”\n",
        "- Constrained output: “Return JSON with keys: summary, steps, pitfalls.”\n",
        "- Few-shot examples: Show 1–3 small examples to guide style and format.\n",
        "\n",
        "Keep it concise. The clearer your prompt, the better the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aea2ddf",
      "metadata": {},
      "source": [
        "## Getting Started: Choose an Access Method\n",
        "Below are two simple ways to try GPT-OSS:\n",
        "1) Local with Ollama (easy setup):\n",
        "   - Install from https://ollama.com\n",
        "   - Pull a model, e.g., `ollama pull llama3`\n",
        "   - Run from terminal: `ollama run llama3`\n",
        "   - Or use the Python client.\n",
        "2) Hosted via Hugging Face Inference API:\n",
        "   - Create an account, get an access token\n",
        "   - Pick a text-generation model\n",
        "   - Call the endpoint from Python.\n",
        "\n",
        "We’ll show minimal Python examples for both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b2e569",
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [],
      "source": [
        "# Option A: Using Ollama locally\n",
        "# Prerequisites:\n",
        "# - Install Ollama: https://ollama.com\n",
        "# - Pull a model, e.g.: `ollama pull llama3`\n",
        "\n",
        "import json, sys, subprocess, shutil\n",
        "\n",
        "def ollama_available():\n",
        "    return shutil.which(\"ollama\") is not None\n",
        "\n",
        "def ollama_chat(model: str, prompt: str) -> str:\n",
        "    \"\"\"Call a local Ollama model with a simple prompt.\"\"\"\n",
        "    if not ollama_available():\n",
        "        raise RuntimeError(\"Ollama not found. Install from https://ollama.com and pull a model.\")\n",
        "    # Use 'ollama run' for a quick single-turn prompt\n",
        "    proc = subprocess.run(\n",
        "        [\"ollama\", \"run\", model], input=prompt.encode(), stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
        "    )\n",
        "    if proc.returncode != 0:\n",
        "        raise RuntimeError(proc.stderr.decode(errors=\"ignore\"))\n",
        "    return proc.stdout.decode(errors=\"ignore\").strip()\n",
        "\n",
        "print(\"Ollama available:\", ollama_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47482278",
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [],
      "source": [
        "# Option B: Using Hugging Face Inference API (hosted)\n",
        "# Prerequisites:\n",
        "# - pip install requests\n",
        "# - Set your token: export HF_TOKEN=your_token_here\n",
        "\n",
        "import os, requests\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
        "\n",
        "def hf_generate(model_id: str, prompt: str, max_new_tokens: int = 200):\n",
        "    if not HF_TOKEN:\n",
        "        raise RuntimeError(\"Set HF_TOKEN environment variable with your access token.\")\n",
        "    url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\"max_new_tokens\": max_new_tokens}\n",
        "    }\n",
        "    r = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    # Many models return a list of dicts with 'generated_text'\n",
        "    if isinstance(data, list) and data and \"generated_text\" in data[0]:\n",
        "        return data[0][\"generated_text\"]\n",
        "    # Some endpoints return a dict with 'generated_text' or 'content'\n",
        "    if isinstance(data, dict):\n",
        "        return data.get(\"generated_text\") or json.dumps(data, ensure_ascii=False)\n",
        "    return json.dumps(data, ensure_ascii=False)\n",
        "\n",
        "print(\"HF token set:\", bool(HF_TOKEN))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f031fa40",
      "metadata": {},
      "source": [
        "## Your First Prompt (Interactive)\n",
        "We’ll craft a simple, well-structured prompt and send it to a model. Choose one method below.\n",
        "\n",
        "Prompt goal: Explain a concept clearly, with constraints and a simple format.\n",
        "\n",
        "Try editing the prompt variables to see how style and clarity change the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8dfa25f",
      "metadata": {
        "tags": [
          "interactive"
        ]
      },
      "outputs": [],
      "source": [
        "# Define a clean, structured prompt\n",
        "role = \"You are a helpful study coach for beginners.\"\n",
        "task = \"Explain the concept in simple terms that a high-school student can understand.\"\n",
        "constraints = (\n",
        "    \"Use: 1 short paragraph, then 3 bullet points. \"\n",
        "    \"Avoid equations and heavy jargon. Keep it under 120 words.\"\n",
        ")\n",
        "concept = \"What is overfitting in machine learning?\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "{role}\n",
        "\n",
        "Task: {task}\n",
        "Constraints: {constraints}\n",
        "Input: {concept}\n",
        "Format:\n",
        "- Paragraph\n",
        "- 3 bullet points\n",
        "\"\"\".strip()\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed7f0f1",
      "metadata": {
        "tags": [
          "interactive"
        ]
      },
      "outputs": [],
      "source": [
        "# Run with Ollama (local) OR Hugging Face (hosted)\n",
        "\n",
        "USE_OLLAMA = False       # Set True if using Ollama locally\n",
        "OLLAMA_MODEL = \"llama3\"  # Or another pulled model, e.g., \"mistral\"\n",
        "HF_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Change to any instruct-capable model\n",
        "\n",
        "try:\n",
        "    if USE_OLLAMA:\n",
        "        response = ollama_chat(OLLAMA_MODEL, prompt)\n",
        "    else:\n",
        "        response = hf_generate(HF_MODEL_ID, prompt)\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)\n",
        "    print(\"Tip: Check your configuration (Ollama installed? HF token set? Correct model ID?).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a248b02",
      "metadata": {},
      "source": [
        "## Improving Your Prompts\n",
        "Use this quick checklist:\n",
        "- Goal: Did I state exactly what I want?\n",
        "- Audience: Did I set the role and reading level?\n",
        "- Constraints: Length, style, do/don’t.\n",
        "- Format: Headings, bullets, or JSON?\n",
        "- Examples: 1–2 small examples if the model struggles.\n",
        "- Iteration: Tweak one element at a time and compare results.\n",
        "\n",
        "Try modifying the prompt below to practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d0be40",
      "metadata": {
        "tags": [
          "practice"
        ]
      },
      "outputs": [],
      "source": [
        "# Practice: Tweak constraints and concept\n",
        "constraints = (\n",
        "    \"Explain in exactly 2 short paragraphs, then a numbered list of 3 tips. \"\n",
        "    \"Use plain language, avoid buzzwords.\"\n",
        ")\n",
        "concept = \"How do decision trees work?\"\n",
        "\n",
        "prompt2 = f\"\"\"\n",
        "You are a clear, friendly tutor.\n",
        "Task: Teach the concept to a curious beginner.\n",
        "Constraints: {constraints}\n",
        "Input: {concept}\n",
        "Output format:\n",
        "1) Paragraph\n",
        "2) Paragraph\n",
        "3) 3 numbered tips\n",
        "\"\"\".strip()\n",
        "\n",
        "print(prompt2)\n",
        "\n",
        "# Run if you want:\n",
        "# response2 = hf_generate(HF_MODEL_ID, prompt2)\n",
        "# print(response2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f0105bc",
      "metadata": {},
      "source": [
        "## Common Pitfalls (and Fixes)\n",
        "- Vague requests → Be specific about output and constraints.\n",
        "- Overly long prompts → Keep only necessary context.\n",
        "- Missing format instructions → Specify bullets, JSON, or sections.\n",
        "- One-shot and done → Iterate. Change one thing at a time.\n",
        "- Wrong model type → Use an instruct-tuned model for following directions.\n",
        "\n",
        "You’re ready to explore! Keep prompts short, structured, and test different phrasings."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    },
    "title": "Prompting GPT-OSS & Getting Started"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
