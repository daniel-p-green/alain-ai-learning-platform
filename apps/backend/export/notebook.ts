import { saveNotebookWithMetadata } from "../utils/notebook-paths";

export type NBCell = {
  cell_type: "markdown" | "code";
  metadata: Record<string, any>;
  source: string[];
  outputs?: any[];
  execution_count?: number | null;
};

export type Notebook = {
  cells: NBCell[];
  metadata: Record<string, any>;
  nbformat: number;
  nbformat_minor: number;
};

export function buildNotebook(
  meta: { title: string; description: string; provider: string; model: string },
  steps: Array<{ step_order: number; title: string; content: string; code_template: string | null; model_params?: any }>,
  assessments: Array<{ step_order: number; question: string; options: string[]; correct_index: number; explanation: string | null }>,
  maker?: { name: string; org_type: string; homepage?: string | null; license?: string | null; repo?: string | null } | null,
  teacherModelUsed?: 'GPT-OSS-20B' | 'GPT-OSS-120B',
  teacherDowngraded?: boolean,
): Notebook {
  const cells: NBCell[] = [];

  // 0) Attribution comment cell
  const creationDate = new Date().toISOString().split('T')[0]; // YYYY-MM-DD format
  const attributionComment = [
    `<!-- \n`,
    `Generated by ALAIN (Applied Learning AI Notebooks) on ${creationDate}\n`,
    `Teacher Model: ${teacherModelUsed || 'GPT-OSS-20B'}\n`,
    `Provider: ${meta.provider}\n`,
    `Target Model: ${meta.model}\n`,
    `Learn more: https://github.com/daniel-p-green/alain-ai-learning-platform/\n`,
    `-->\n`
  ];
  cells.push({ cell_type: "markdown", metadata: {}, source: attributionComment });

cells.push({
  cell_type: "markdown",
  metadata: {},
  source: [`> Generated by ALAIN (Applied Learning AI Notebooks) â€” ${creationDate}.\n`]
});

  // 1) Intro
  const intro: string[] = [
    `# ${meta.title}\n`,
    `\n`,
    `${meta.description}\n`,
    `\n`,
    `> Provider: \`${meta.provider}\`  â€¢  Model: \`${meta.model}\`\n`,
    `Runtime: ${meta.provider}\n`,
    `\n`,
    `This notebook was generated by ALAIN (Applied Learning AI Notebooks). It calls AI models via OpenAI-compatible APIs (no arbitrary code).\n`
  ];
  if (maker && maker.name) {
    intro.push(`\n---\n\n`);
    intro.push(`### Model Maker\n`);
    intro.push(`${maker.name} (${maker.org_type})\n`);
    const bullets: string[] = [];
    if (maker.license) bullets.push(`- License: ${maker.license}`);
    if (maker.homepage) bullets.push(`- Homepage: ${maker.homepage}`);
    if (maker.repo) bullets.push(`- Repository: ${maker.repo}`);
    if (bullets.length) intro.push(bullets.join("\n") + "\n");
  }
  cells.push({ cell_type: "markdown", metadata: {}, source: intro });

  // 1b) Environment detection (Colab/local)
  cells.push({
    cell_type: "code",
    metadata: {},
    source: [
      "# ðŸ”§ Environment Detection and Setup\\n",
      "import sys\\n",
      "import os\\n",
      "\\n",
      "# Detect environment\\n",
      "IN_COLAB = 'google.colab' in sys.modules\\n",
      "env_label = 'Google Colab' if IN_COLAB else 'Local'\\n",
      "print(f'Environment: {env_label}')\\n",
      "\\n",
      "# Setup environment-specific configurations\\n",
      "if IN_COLAB:\\n",
      "    print('ðŸ“ Colab-specific optimizations enabled')\\n",
      "    try:\\n",
      "        from google.colab import output\\n",
      "        output.enable_custom_widget_manager()\\n",
      "    except Exception:\\n",
      "        pass\\n"
    ],
    outputs: [],
    execution_count: null,
  });

  // Reproducibility tips (inline)
  intro.push("\n---\n\n");
  intro.push("### Reproducibility Tips\n");
  intro.push("- Avoid network access in core cells.\n");
  intro.push("- Seed randomness where applicable (e.g., numpy, random).\n");
  intro.push("- Pin package versions in your own environment if needed.\n");
  intro.push("- Set `OPENAI_BASE_URL` and `OPENAI_API_KEY` via env (or Colab userdata).\n");
  intro.push("- Widgets optional: text-based MCQs are provided if widgets are unavailable.\n");

  // Provider base URL setup
  const providerBase = meta.provider === 'poe' ? 'https://api.poe.com/v1' : 'http://localhost:1234/v1';

  // 2) Requirements.txt cell
  cells.push({
    cell_type: "code",
    metadata: {},
    source: [
      "# Create requirements.txt for reproducible environment\n",
      "%%writefile requirements.txt\n",
      "openai>=1.34.0\n",
      "ipywidgets>=8.0.0\n",
      "requests>=2.31.0\n",
      "python-dotenv>=1.0.0\n",
      "numpy>=1.24.0\n",
      "pandas>=2.0.0\n",
    ],
    outputs: [],
    execution_count: null,
  });

  // 2b) .env.example file
  cells.push({
    cell_type: "code",
    metadata: {},
    source: [
      "# Create .env.example template\n",
      "%%writefile .env.example\n",
      "# Copy this file to .env and fill in your actual values\n",
      "OPENAI_API_KEY=your_api_key_here\n",
      `OPENAI_BASE_URL=${providerBase}\n`,
      "POE_API_KEY=your_poe_key_here\n",
      "# For local models (LM Studio/Ollama), any non-empty string works for API_KEY\n",
    ],
    outputs: [],
    execution_count: null,
  });

  // 2c) Install dependencies
  cells.push({
    cell_type: "code",
    metadata: {},
    source: [
      "# Install dependencies from requirements.txt\n",
      "!pip install -q -r requirements.txt\n",
      "print('âœ… Dependencies installed successfully')\n",
    ],
    outputs: [],
    execution_count: null,
  });

  // 2c) API Keys and .env Files (primer)
  cells.push({
    cell_type: "markdown",
    metadata: {},
    source: [
      "## API Keys and .env Files\n",
      "\n",
      "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\n",
      "\n",
      "- Why .env? Keeps secrets out of source control and tutorials.\n",
      "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\n",
      "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\n",
      "- Find your keys: Poe-compatible provider dashboard; Hugging Face token: https://huggingface.co/settings/tokens.\n",
      "\n",
      "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work.\n"
    ]
  });

  // 2d) Load and manage secrets from .env
  cells.push({
    cell_type: "code",
    metadata: {},
    source: [
      "# ðŸ” Load and manage secrets from .env\\n",
      "# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n",
      "# Location: place your .env files next to this notebook (recommended) or at project root.\\n",
      "# Disable writing: set SAVE_TO_ENV = False below.\\n",
      "import os, pathlib\\n",
      "from getpass import getpass\\n",
      "\\n",
      "# Install python-dotenv if missing\\n",
      "try:\\n",
      "    import dotenv  # type: ignore\\n",
      "except Exception:\\n",
      "    import sys, subprocess\\n",
      "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n",
      "    import dotenv  # type: ignore\\n",
      "\\n",
      "# Prefer .env.local over .env\\n",
      "cwd = pathlib.Path.cwd()\\n",
      "env_local = cwd / '.env.local'\\n",
      "env_file = cwd / '.env'\\n",
      "chosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\n",
      "if chosen:\\n",
      "    dotenv.load_dotenv(dotenv_path=str(chosen))\\n",
      "    print(f'Loaded env from {chosen.name}')\\n",
      "else:\\n",
      "    print('No .env.local or .env found; will prompt for keys.')\\n",
      "\\n",
      "# Keys we might use in this notebook\\n",
      "keys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\n",
      "missing = [k for k in keys if not os.environ.get(k)]\\n",
      "for k in missing:\\n",
      "    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n",
      "    if val:\\n",
      "        os.environ[k] = val\\n",
      "\\n",
      "# Decide whether to persist to .env.local for convenience\\n",
      "SAVE_TO_ENV = True  # set False to disable writing\\n",
      "if SAVE_TO_ENV:\\n",
      "    target = env_local\\n",
      "    existing = {}\\n",
      "    if target.exists():\\n",
      "        try:\\n",
      "            for line in target.read_text().splitlines():\\n",
      "                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n",
      "                    continue\\n",
      "                k,v = line.split('=',1)\\n",
      "                existing[k.strip()] = v.strip()\\n",
      "        except Exception:\\n",
      "            pass\\n",
      "    for k in keys:\\n",
      "        v = os.environ.get(k)\\n",
      "        if v:\\n",
      "            existing[k] = v\\n",
      "    lines = []\\n",
      "    for k,v in existing.items():\\n",
      "        # Always quote; escape backslashes and double quotes for safety\\n",
      "        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n",
      "        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n",
      "        vv = f'\"{escaped}\"'\\n",
      "        lines.append(f\"{k}={vv}\")\\n",
      "    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n",
      "    try:\\n",
      "        target.chmod(0o600)  # 600\\n",
      "    except Exception:\\n",
      "        pass\\n",
      "    print(f'ðŸ” Wrote secrets to {target.name} (permissions 600)')\\n",
      "\\n",
      "# Simple recap (masked)\\n",
      "def mask(v):\\n",
      "    if not v: return 'âˆ…'\\n",
      "    return v[:3] + 'â€¦' + v[-2:] if len(v) > 6 else 'â€¢â€¢â€¢'\\n",
      "for k in keys:\\n",
      "    print(f'{k}:', mask(os.environ.get(k)))\\n",
    ],
    outputs: [],
    execution_count: null,
  });

  // 3) Provider env/config
  // Provide a sensible default for local OpenAI-compatible runtimes.
  // LM Studio commonly runs on :1234; Ollama on :11434.
  const envSource = [
    "# Configure OpenAI-compatible client (maps POE_API_KEY if present)\n",
    "import os\n",
    "from getpass import getpass\n",
    "# Try to read secrets from Colab userdata if available\n",
    "try:\n",
    "  from google.colab import userdata  # type: ignore\n",
    "  _poe = userdata.get('POE_API_KEY')\n",
    "  _openai = userdata.get('OPENAI_API_KEY')\n",
    "except Exception:\n",
    "  _poe = None; _openai = None\n",
    `PROVIDER = "${meta.provider}"  # "poe" or "openai-compatible"\n`,
    `os.environ.setdefault("OPENAI_BASE_URL", "${providerBase}")\n`,
    `poe = _poe or os.getenv("POE_API_KEY")\n`,
    `if poe:\n    os.environ.setdefault("OPENAI_BASE_URL", "https://api.poe.com/v1")\n    os.environ.setdefault("OPENAI_API_KEY", poe)\n`,
    `elif _openai and not os.getenv("OPENAI_API_KEY"):\n    os.environ["OPENAI_API_KEY"] = _openai\n`,
    "# Local-friendly defaults to avoid prompting beginners\n",
    "if (PROVIDER == 'openai-compatible') and not os.environ.get('OPENAI_API_KEY'):\n",
    "  base = os.environ.get('OPENAI_BASE_URL','')\n",
    "  if 'localhost:1234' in base or '127.0.0.1:1234' in base:\n",
    "    os.environ['OPENAI_API_KEY'] = 'lm-studio'\n",
    "  elif 'localhost:11434' in base or '127.0.0.1:11434' in base:\n",
    "    os.environ['OPENAI_API_KEY'] = 'ollama'\n",
    "# Fallback interactive prompt if still missing\n",
    "if not os.environ.get('OPENAI_API_KEY'):\n",
    "  os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY or OPENAI_API_KEY (hidden): ')\n",
    "# OPENAI_BASE_URL and OPENAI_API_KEY environment variables are set above\n",
  ];
  cells.push({ cell_type: "code", metadata: {}, source: envSource, outputs: [], execution_count: null });

  // 3b) Pre-flight connection check
  cells.push({
    cell_type: "code",
    metadata: {},
    source: [
      "# Pre-flight check: verify API connectivity\n",
      "from openai import OpenAI\n",
      "import os, sys\n",
      "base = os.environ.get('OPENAI_BASE_URL')\n",
      "key = os.environ.get('OPENAI_API_KEY')\n",
      "if not base or not key:\n",
      "    print('âŒ Missing OPENAI_BASE_URL or OPENAI_API_KEY. Set them above.')\n",
      "else:\n",
      "    try:\n",
      "        client = OpenAI(base_url=base, api_key=key)\n",
      "        # lightweight call: list models or small completion\n",
      "        ok = False\n",
      "        try:\n",
      "            _ = client.models.list()\n",
      "            ok = True\n",
      "        except Exception:\n",
      "            # Fallback to a 1-token chat call\n",
      "            _ = client.chat.completions.create(model=\"${meta.model}\", messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
      "            ok = True\n",
      "        if ok:\n",
      "            print('âœ… API key is working and connected to provider.')\n",
      "    except Exception as e:\n",
      "        print('âŒ Connection failed. Please check your API key and base URL.\\n', e)\n",
    ],
    outputs: [],
    execution_count: null,
  });

  // 4) Quick smoke test
  cells.push({
    cell_type: "code",
    metadata: {},
    source: [
      "# Quick smoke test\n",
      "from openai import OpenAI\n",
      "import os\n",
      "base = os.environ.get('OPENAI_BASE_URL')\n",
      "key = os.environ.get('OPENAI_API_KEY')\n",
      "assert base and key, 'Please set OPENAI_BASE_URL and OPENAI_API_KEY env vars'\n",
      "client = OpenAI(base_url=base, api_key=key)\n",
      `resp = client.chat.completions.create(model="${meta.model}", messages=[{"role":"user","content":"Hello from ALAIN"}], max_tokens=32)\n`,
      "print(resp.choices[0].message.content)\n",
    ],
    outputs: [],
    execution_count: null,
  });

  // 4b) Optional: Tool Use demo (works well on LM Studio with native tool models)
  if (meta.provider === 'openai-compatible') {
    cells.push({
      cell_type: "code",
      metadata: {},
      source: [
        "# Tool Use demo (LM Studio recommended)\n",
        "# This shows how to pass OpenAI-compatible 'tools' and handle tool_calls.\n",
        "from openai import OpenAI\n",
        "import os, json, datetime\n",
        "client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "tools = [{\n",
        "  'type': 'function',\n",
        "  'function': {\n",
        "    'name': 'get_current_time',\n",
        "    'description': 'Return the current local time as an ISO string',\n",
        "    'parameters': { 'type': 'object', 'properties': {} }\n",
        "  }\n",
        "}]\n",
        `messages = [{"role":"user","content":"Tell me a joke, then use a tool to tell the current time."}]\n`,
        `resp = client.chat.completions.create(model="${meta.model}", messages=messages, tools=tools)\n`,
        "m = resp.choices[0].message\n",
        "print('finish_reason:', resp.choices[0].finish_reason)\n",
        "if getattr(m, 'tool_calls', None):\n",
        "    for tc in m.tool_calls:\n",
        "        if tc.function and tc.function.name == 'get_current_time':\n",
        "            tool_out = datetime.datetime.now().isoformat()\n",
        "            # Feed tool result back to the model for a final answer\n",
        "            messages.append({'role': 'assistant', 'tool_calls': [tc]})\n",
        "            messages.append({'role': 'tool', 'content': tool_out})\n",
        `            final = client.chat.completions.create(model="${meta.model}", messages=messages)\n`,
        "            print(final.choices[0].message.content)\n",
        "else:\n",
        "    # No tool call requested; show normal content\n",
        "    print(m.content)\n",
      ],
      outputs: [],
      execution_count: null,
    });
  }

  // Utility to find assessments per step
  function listAssessments(step_order: number) {
    return assessments.filter(a => a.step_order === step_order);
  }

  // 5) Lesson steps
  for (const s of steps) {
    // Step markdown
    cells.push({
      cell_type: "markdown",
      metadata: {},
      source: [`## Step ${s.step_order}: ${s.title}\n`, `\n`, `${s.content}\n`],
    });

    // Step execution scaffold (uses global client from smoke test)
    const prompt = (s.code_template || '').endsWith("\n") ? (s.code_template || '') : (s.code_template || '') + "\n";
    const t = s.model_params?.temperature ?? 0.7;
    const stepSource = [
      "# Run the step prompt using the configured provider\n",
      `PROMPT = """\n${prompt}"""\n`,
      "from openai import OpenAI\n",
      "import os\n",
      "client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
      `resp = client.chat.completions.create(model="${meta.model}", messages=[{"role":"user","content":PROMPT}], temperature=${t}, max_tokens=400)\n`,
      "print(resp.choices[0].message.content)\n",
    ];
    cells.push({ cell_type: "code", metadata: {}, source: stepSource, outputs: [], execution_count: null });

    // MCQs (if any)
    const qs = listAssessments(s.step_order);
    for (const a of qs) {
      const mcq = [
        `# Assessment for Step ${s.step_order}\n`,
        `question = ${JSON.stringify(a.question)}\n`,
        `options = ${JSON.stringify(a.options)}\n`,
        `correct_index = ${a.correct_index}\n`,
        `print('Q:', question)\n`,
        `for i, o in enumerate(options):\n    print(f"{i}. {o}")\n`,
        `choice = 0  # <- change this to your answer index\n`,
        `print('Correct!' if choice == correct_index else 'Incorrect')\n`,
        `${a.explanation ? `print('Explanation:', ${JSON.stringify(a.explanation)})\n` : ''}`,
      ];
      cells.push({ cell_type: "code", metadata: {}, source: mcq, outputs: [], execution_count: null });

      const widget = [
        `# Interactive quiz for Step ${s.step_order}\n`,
        `import ipywidgets as widgets\n`,
        `from IPython.display import display, Markdown\n`,
        `q = ${JSON.stringify(a.question)}\n`,
        `opts = ${JSON.stringify(a.options)}\n`,
        `correct = ${a.correct_index}\n`,
        `rb = widgets.RadioButtons(options=[(o, i) for i, o in enumerate(opts)], description='', disabled=False)\n`,
        `btn = widgets.Button(description='Submit Answer')\n`,
        `out = widgets.Output()\n`,
        `def on_click(b):\n`,
        `  with out:\n`,
        `    out.clear_output()\n`,
        `    sel = rb.value if hasattr(rb, 'value') else 0\n`,
        `    if sel == correct:\n`,
        `      display(Markdown('**Correct!**'${a.explanation ? ` + ' â€” ' + ${JSON.stringify(a.explanation)}` : ''}))\n`,
        `    else:\n`,
        `      display(Markdown('Incorrect, please try again.'))\n`,
        `btn.on_click(on_click)\n`,
        `display(Markdown(f"### {q}"))\n`,
        `display(rb, btn, out)\n`,
      ];
      cells.push({ cell_type: "code", metadata: {}, source: widget, outputs: [], execution_count: null });
    }
  }

  return {
    cells,
    metadata: {
      kernelspec: { name: "python3", language: "python", display_name: "Python 3" },
      language_info: { name: "python" },
      teacher_model_used: teacherModelUsed || 'GPT-OSS-20B',
      teacher_downgraded: !!teacherDowngraded,
      title: meta.title,
      provider: meta.provider,
      model: meta.model,
    },
    nbformat: 4,
    nbformat_minor: 5,
  };
}
