import { saveNotebookWithMetadata } from "../utils/notebook-paths";

export type NBCell = {
  cell_type: "markdown" | "code";
  metadata: Record<string, any>;
  source: string[];
  outputs?: any[];
  execution_count?: number | null;
};

export type Notebook = {
  cells: NBCell[];
  metadata: Record<string, any>;
  nbformat: number;
  nbformat_minor: number;
};

export function buildNotebook(
  meta: { title: string; description: string; provider: string; model: string },
  steps: Array<{ step_order: number; title: string; content: string; code_template: string | null; model_params?: any }>,
  assessments: Array<{ step_order: number; question: string; options: string[]; correct_index: number; explanation: string | null }>,
  maker?: { name: string; org_type: string; homepage?: string | null; license?: string | null; repo?: string | null } | null,
  teacherModelUsed?: 'GPT-OSS-20B' | 'GPT-OSS-120B',
  teacherDowngraded?: boolean,
): Notebook {
  const cells: NBCell[] = [];

  // 0) Attribution comment cell
  const creationDate = new Date().toISOString().split('T')[0]; // YYYY-MM-DD format
  const attributionComment = [
    `<!-- \n`,
    `Generated by ALAIN (Applied Learning AI Notebooks) on ${creationDate}\n`,
    `Teacher Model: ${teacherModelUsed || 'GPT-OSS-20B'}\n`,
    `Provider: ${meta.provider}\n`,
    `Target Model: ${meta.model}\n`,
    `Learn more: https://github.com/daniel-p-green/alain-ai-learning-platform/\n`,
    `-->\n`
  ];
  cells.push({ cell_type: "markdown", metadata: {}, source: attributionComment });

  // 1) Intro
  const intro: string[] = [
    `# ${meta.title}\n`,
    `\n`,
    `${meta.description}\n`,
    `\n`,
    `> Provider: \`${meta.provider}\`  •  Model: \`${meta.model}\`\n`,
    `Runtime: ${meta.provider}\n`,
    `\n`,
    `This notebook was generated by ALAIN. It calls AI models via OpenAI-compatible APIs (no arbitrary code).\n`
  ];
  if (maker && maker.name) {
    intro.push(`\n---\n\n`);
    intro.push(`### Model Maker\n`);
    intro.push(`${maker.name} (${maker.org_type})\n`);
    const bullets: string[] = [];
    if (maker.license) bullets.push(`- License: ${maker.license}`);
    if (maker.homepage) bullets.push(`- Homepage: ${maker.homepage}`);
    if (maker.repo) bullets.push(`- Repository: ${maker.repo}`);
    if (bullets.length) intro.push(bullets.join("\n") + "\n");
  }
  cells.push({ cell_type: "markdown", metadata: {}, source: intro });

  // Reproducibility tips (inline)
  intro.push("\n---\n\n");
  intro.push("### Reproducibility Tips\n");
  intro.push("- Avoid network access in core cells.\n");
  intro.push("- Seed randomness where applicable (e.g., numpy, random).\n");
  intro.push("- Pin package versions in your own environment if needed.\n");
  intro.push("- Set `OPENAI_BASE_URL` and `OPENAI_API_KEY` via env (or Colab userdata).\n");
  intro.push("- Widgets optional: text-based MCQs are provided if widgets are unavailable.\n");

  // 2) Setup: install deps
  cells.push({
    cell_type: "code",
    metadata: {},
    source: [
      "# Install client SDKs (if missing)\n",
      "!pip -q install openai>=1.34.0\n",
    ],
    outputs: [],
    execution_count: null,
  });

  // 3) Provider env/config
  const providerBase = meta.provider === 'poe' ? 'https://api.poe.com/v1' : 'YOUR_OPENAI_BASE_URL';
  const envSource = [
    "# Configure OpenAI-compatible client\n",
    "import os\n",
    "from getpass import getpass\n",
    "# Try to read secrets from Colab userdata if available\n",
    "try:\n",
    "  from google.colab import userdata  # type: ignore\n",
    "  _poe = userdata.get('POE_API_KEY')\n",
    "  _openai = userdata.get('OPENAI_API_KEY')\n",
    "except Exception:\n",
    "  _poe = None; _openai = None\n",
    `PROVIDER = "${meta.provider}"  # "poe" or "openai-compatible"\n`,
    `os.environ.setdefault("OPENAI_BASE_URL", "${providerBase}")\n`,
    `# Set your API key. For Poe, set POE_API_KEY in the Colab environment or paste below.\n`,
    `os.environ.setdefault("OPENAI_API_KEY", _poe or _openai or os.getenv("POE_API_KEY") or os.getenv("OPENAI_API_KEY") or "")\n`,
    "if not os.environ.get('OPENAI_API_KEY'):\n",
    "  os.environ['OPENAI_API_KEY'] = getpass('Enter API key (input hidden): ')\n",
    "# OPENAI_BASE_URL and OPENAI_API_KEY environment variables are set above\n",
  ];
  cells.push({ cell_type: "code", metadata: {}, source: envSource, outputs: [], execution_count: null });

  // 3b) Pre-flight connection check
  cells.push({
    cell_type: "code",
    metadata: {},
    source: [
      "# Pre-flight check: verify API connectivity\n",
      "from openai import OpenAI\n",
      "import os, sys\n",
      "base = os.environ.get('OPENAI_BASE_URL')\n",
      "key = os.environ.get('OPENAI_API_KEY')\n",
      "if not base or not key:\n",
      "    print('❌ Missing OPENAI_BASE_URL or OPENAI_API_KEY. Set them above.')\n",
      "else:\n",
      "    try:\n",
      "        client = OpenAI(base_url=base, api_key=key)\n",
      "        # lightweight call: list models or small completion\n",
      "        ok = False\n",
      "        try:\n",
      "            _ = client.models.list()\n",
      "            ok = True\n",
      "        except Exception:\n",
      "            # Fallback to a 1-token chat call\n",
      "            _ = client.chat.completions.create(model=\"${meta.model}\", messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
      "            ok = True\n",
      "        if ok:\n",
      "            print('✅ API key is working and connected to provider.')\n",
      "    except Exception as e:\n",
      "        print('❌ Connection failed. Please check your API key and base URL.\\n', e)\n",
    ],
    outputs: [],
    execution_count: null,
  });

  // 4) Quick smoke test
  cells.push({
    cell_type: "code",
    metadata: {},
    source: [
      "# Quick smoke test\n",
      "from openai import OpenAI\n",
      "import os\n",
      "base = os.environ.get('OPENAI_BASE_URL')\n",
      "key = os.environ.get('OPENAI_API_KEY')\n",
      "assert base and key, 'Please set OPENAI_BASE_URL and OPENAI_API_KEY env vars'\n",
      "client = OpenAI(base_url=base, api_key=key)\n",
      `resp = client.chat.completions.create(model="${meta.model}", messages=[{"role":"user","content":"Hello from ALAIN"}], max_tokens=32)\n`,
      "print(resp.choices[0].message.content)\n",
    ],
    outputs: [],
    execution_count: null,
  });

  // 4b) Optional: Tool Use demo (works well on LM Studio with native tool models)
  if (meta.provider === 'openai-compatible') {
    cells.push({
      cell_type: "code",
      metadata: {},
      source: [
        "# Tool Use demo (LM Studio recommended)\n",
        "# This shows how to pass OpenAI-compatible 'tools' and handle tool_calls.\n",
        "from openai import OpenAI\n",
        "import os, json, datetime\n",
        "client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "tools = [{\n",
        "  'type': 'function',\n",
        "  'function': {\n",
        "    'name': 'get_current_time',\n",
        "    'description': 'Return the current local time as an ISO string',\n",
        "    'parameters': { 'type': 'object', 'properties': {} }\n",
        "  }\n",
        "}]\n",
        `messages = [{"role":"user","content":"Tell me a joke, then use a tool to tell the current time."}]\n`,
        `resp = client.chat.completions.create(model="${meta.model}", messages=messages, tools=tools)\n`,
        "m = resp.choices[0].message\n",
        "print('finish_reason:', resp.choices[0].finish_reason)\n",
        "if getattr(m, 'tool_calls', None):\n",
        "    for tc in m.tool_calls:\n",
        "        if tc.function and tc.function.name == 'get_current_time':\n",
        "            tool_out = datetime.datetime.now().isoformat()\n",
        "            # Feed tool result back to the model for a final answer\n",
        "            messages.append({'role': 'assistant', 'tool_calls': [tc]})\n",
        "            messages.append({'role': 'tool', 'content': tool_out})\n",
        `            final = client.chat.completions.create(model="${meta.model}", messages=messages)\n`,
        "            print(final.choices[0].message.content)\n",
        "else:\n",
        "    # No tool call requested; show normal content\n",
        "    print(m.content)\n",
      ],
      outputs: [],
      execution_count: null,
    });
  }

  // Utility to find assessments per step
  function listAssessments(step_order: number) {
    return assessments.filter(a => a.step_order === step_order);
  }

  // 5) Lesson steps
  for (const s of steps) {
    // Step markdown
    cells.push({
      cell_type: "markdown",
      metadata: {},
      source: [`## Step ${s.step_order}: ${s.title}\n`, `\n`, `${s.content}\n`],
    });

    // Step execution scaffold (uses global client from smoke test)
    const prompt = (s.code_template || '').endsWith("\n") ? (s.code_template || '') : (s.code_template || '') + "\n";
    const t = s.model_params?.temperature ?? 0.7;
    const stepSource = [
      "# Run the step prompt using the configured provider\n",
      `PROMPT = """\n${prompt}"""\n`,
      "from openai import OpenAI\n",
      "import os\n",
      "client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
      `resp = client.chat.completions.create(model="${meta.model}", messages=[{"role":"user","content":PROMPT}], temperature=${t}, max_tokens=400)\n`,
      "print(resp.choices[0].message.content)\n",
    ];
    cells.push({ cell_type: "code", metadata: {}, source: stepSource, outputs: [], execution_count: null });

    // MCQs (if any)
    const qs = listAssessments(s.step_order);
    for (const a of qs) {
      const mcq = [
        `# Assessment for Step ${s.step_order}\n`,
        `question = ${JSON.stringify(a.question)}\n`,
        `options = ${JSON.stringify(a.options)}\n`,
        `correct_index = ${a.correct_index}\n`,
        `print('Q:', question)\n`,
        `for i, o in enumerate(options):\n    print(f"{i}. {o}")\n`,
        `choice = 0  # <- change this to your answer index\n`,
        `print('Correct!' if choice == correct_index else 'Incorrect')\n`,
        `${a.explanation ? `print('Explanation:', ${JSON.stringify(a.explanation)})\n` : ''}`,
      ];
      cells.push({ cell_type: "code", metadata: {}, source: mcq, outputs: [], execution_count: null });

      const widget = [
        `# Interactive quiz for Step ${s.step_order}\n`,
        `import ipywidgets as widgets\n`,
        `from IPython.display import display, Markdown\n`,
        `q = ${JSON.stringify(a.question)}\n`,
        `opts = ${JSON.stringify(a.options)}\n`,
        `correct = ${a.correct_index}\n`,
        `rb = widgets.RadioButtons(options=[(o, i) for i, o in enumerate(opts)], description='', disabled=False)\n`,
        `btn = widgets.Button(description='Submit Answer')\n`,
        `out = widgets.Output()\n`,
        `def on_click(b):\n`,
        `  with out:\n`,
        `    out.clear_output()\n`,
        `    sel = rb.value if hasattr(rb, 'value') else 0\n`,
        `    if sel == correct:\n`,
        `      display(Markdown('**Correct!**'${a.explanation ? ` + ' — ' + ${JSON.stringify(a.explanation)}` : ''}))\n`,
        `    else:\n`,
        `      display(Markdown('Incorrect, please try again.'))\n`,
        `btn.on_click(on_click)\n`,
        `display(Markdown(f"### {q}"))\n`,
        `display(rb, btn, out)\n`,
      ];
      cells.push({ cell_type: "code", metadata: {}, source: widget, outputs: [], execution_count: null });
    }
  }

  return {
    cells,
    metadata: {
      kernelspec: { name: "python3", language: "python", display_name: "Python 3" },
      language_info: { name: "python" },
      teacher_model_used: teacherModelUsed || 'GPT-OSS-20B',
      teacher_downgraded: !!teacherDowngraded,
      title: meta.title,
      provider: meta.provider,
      model: meta.model,
    },
    nbformat: 4,
    nbformat_minor: 5,
  };
}
