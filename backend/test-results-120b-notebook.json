{
  "id": "chatcmpl-d39af369df0897381cc689fb",
  "object": "chat.completion",
  "created": 1757807933,
  "model": "gpt-oss-120b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Thinking...\n>We need to produce a Jupyter notebook content. Should include markdown cells with theory, math, code cells with implementation. Provide code snippets, maybe using PyTorch, huggingface. Since from scratch CLIP is large, we can outline but provide simplified version. Provide sections. Provide placeholders for data. Provide advanced optimization like mixed precision, gradient accumulation, learning rate schedules. Provide evaluation metrics like zero-shot classification, retrieval recall. Provide deployment notes. Provide real-world example like image-text retrieval on Flickr30k.\n>\n>We need to output as markdown with code fences indicating cells. Provide # %% [markdown] and # %% [code] style? Usually Jupyter notebooks exported as .py with cell markers. Could provide .ipynb JSON? Probably easier to give a .py style with markers. Provide thorough content.\n>\n>Let's craft.\n\nBelow is a **complete, ready‑to‑run Jupyter notebook** (in the “percent‑script” format that can be opened directly in JupyterLab/VS Code or converted to a classic *.ipynb* file with `jupytext`).  \nAll cells are clearly marked as **Markdown** or **Code**.  \nCopy the whole text into a file named `multimodal_clip_advanced.ipynb` (or `multimodal_clip_advanced.py` if you prefer the script format) and launch it.\n\n---\n\n```python\n# %%\n# %load_ext autoreload\n# %autoreload 2\n```\n\n# %% [markdown]\n# # Multi‑modal AI: Vision‑Language Models (CLIP)  \n> **Target audience:** PhD‑level researchers & senior ML engineers  \n> **What you will get:**  \n> * A deep dive into the CLIP architecture, loss functions, and alignment tricks.  \n> * A **from‑scratch** (but still practical) PyTorch implementation with modern training tricks (mixed‑precision, gradient‑accumulation, EMA, LR‑schedulers).  \n> * Benchmarking utilities (zero‑shot classification, image‑text retrieval, recall@K).  \n> * A real‑world case study on **Flickr30k** (image‑caption retrieval) and a short guide on production deployment (ONNX, TorchServe, Triton).  \n\n---  \n\n## 1️⃣ Theoretical Background  \n\n### 1.1 Problem Statement  \n\nGiven a dataset of **N** image‑text pairs  \n\\[\n\\mathcal{D} = \\{(x_i, t_i)\\}_{i=1}^{N},\n\\]  \nwe want to learn two encoders  \n\n* **Vision encoder** \\(f_{\\theta} : \\mathcal{X} \\rightarrow \\mathbb{R}^{d}\\)  \n* **Text encoder**   \\(g_{\\phi}   : \\mathcal{T} \\rightarrow \\mathbb{R}^{d}\\)\n\nsuch that the **cosine similarity** of matched pairs is maximised while that of mismatched pairs is minimised.\n\n### 1.2 Contrastive (InfoNCE) Loss  \n\nFor a mini‑batch of size **B**, define the similarity matrix  \n\n\\[\nS_{ij}= \\frac{f_{\\theta}(x_i)^{\\top} g_{\\phi}(t_j)}{\\tau},\n\\]  \n\nwhere \\(\\tau\\) is a temperature hyper‑parameter.  \nThe **InfoNCE** loss for the image‑to‑text direction is  \n\n\\[\n\\mathcal{L}_{i\\rightarrow t}= -\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{\\exp(S_{ii})}{\\sum_{j=1}^{B}\\exp(S_{ij})},\n\\]  \n\nand analogously for the text‑to‑image direction  \n\n\\[\n\\mathcal{L}_{t\\rightarrow i}= -\\frac{1}{B}\\sum_{j=1}^{B}\\log\\frac{\\exp(S_{jj})}{\\sum_{i=1}^{B}\\exp(S_{ij})}.\n\\]  \n\nThe final loss is the symmetric sum  \n\n\\[\n\\boxed{\\mathcal{L}= \\frac{1}{2}\\big(\\mathcal{L}_{i\\rightarrow t}+ \\mathcal{L}_{t\\rightarrow i}\\big)}.\n\\]\n\n### 1.3 Cross‑modal Attention (Optional Extension)  \n\nStandard CLIP uses **global average pooling** on the visual backbone.  \nA more expressive variant inserts a **cross‑modal transformer** that attends the visual tokens to the text tokens:\n\n\\[\n\\mathbf{H}^{\\text{vis}} = \\text{Transformer}_{\\text{cross}}(\\mathbf{V}, \\mathbf{T}),\n\\qquad\n\\mathbf{H}^{\\text{txt}} = \\text{Transformer}_{\\text{cross}}(\\mathbf{T}, \\mathbf{V}),\n\\]\n\nwhere \\(\\mathbf{V}\\in\\mathbb{R}^{N_v\\times d}\\) and \\(\\mathbf{T}\\in\\mathbb{R}^{N_t\\times d}\\) are token sequences.  \nThe final embeddings are obtained by **CLS‑token pooling**.\n\n### 1.4 Fine‑tuning for Domain Adaptation  \n\nAfter pre‑training, we often **freeze** one encoder and **adapt** the other, or we add a **lightweight adapter** (e.g., LoRA, prefix‑tuning).  \nThe objective can be switched to a **linear probe** (train a linear classifier on top of frozen embeddings) or a **full‑finetune** with a **lower learning‑rate** for the backbone.\n\n### 1.5 Evaluation Metrics  \n\n| Task | Metric | Formula |\n|------|--------|---------|\n| Zero‑shot classification | Top‑k accuracy | \\(\\frac{1}{|C|}\\sum_{c\\in C}\\mathbf{1}\\big[ \\operatorname{rank}(c) \\le k\\big]\\) |\n| Image‑to‑text retrieval | Recall@K | \\(\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\big[ \\text{rank}_{i}^{\\text{text}} \\le K\\big]\\) |\n| Text‑to‑image retrieval | Recall@K | analogous |\n| Cross‑modal Retrieval (Mean Reciprocal Rank) | MRR | \\(\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\text{rank}_{i}}\\) |\n\n---  \n\n## 2️⃣ Environment Setup  \n\n```python\n# %%\n# Install the minimal set of libraries (run once)\n!pip install -q torch torchvision torchaudio\n!pip install -q transformers datasets tqdm matplotlib seaborn\n!pip install -q accelerate==0.27.2  # for mixed‑precision & distributed\n!pip install -q ftfy regex tqdm\n```\n\n```python\n# %%\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\nfrom torchvision import models\nfrom transformers import AutoTokenizer, AutoModel\nimport math, random, os, json, time, copy\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n```\n\n---  \n\n## 3️⃣ Dataset – Flickr30k (Image‑Caption Retrieval)  \n\nWe will use the **Flickr30k** split provided by `datasets`.  \n\n```python\n# %%\nfrom datasets import load_dataset\n\nflickr = load_dataset(\"flickr30k\", split=\"train\")\nval_flickr = load_dataset(\"flickr30k\", split=\"validation\")\ntest_flickr = load_dataset(\"flickr30k\", split=\"test\")\nprint(f\"Train size: {len(flickr)} | Val size: {len(val_flickr)} | Test size: {len(test_flickr)}\")\n```\n\n### 3.1 Custom Dataset Wrapper  \n\n```python\n# %%\nclass Flickr30kDataset(Dataset):\n    def __init__(self, hf_dataset, tokenizer, image_transform, max_len=77):\n        self.ds = hf_dataset\n        self.tokenizer = tokenizer\n        self.transform = image_transform\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        item = self.ds[idx]\n        # image\n        image = Image.open(item[\"image\"]).convert(\"RGB\")\n        image = self.transform(image)\n\n        # text – we randomly pick one of the 5 captions (for contrastive training)\n        caption = random.choice(item[\"sentences\"])\n        txt = self.tokenizer(\n            caption,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"image\": image,\n            \"input_ids\": txt[\"input_ids\"].squeeze(0),\n            \"attention_mask\": txt[\"attention_mask\"].squeeze(0),\n        }\n```\n\n```python\n# %%\nfrom PIL import Image\n\n# Vision transforms – CLIP‑style (center‑crop, resize to 224, normalize)\nvision_transform = T.Compose([\n    T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n                std=(0.26862954, 0.26130258, 0.27577711)),\n])\n\n# Text tokenizer – we will use the OpenAI‑CLIP tokenizer (via HuggingFace)\ntokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n```\n\n```python\n# %%\ntrain_dataset = Flickr30kDataset(flickr, tokenizer, vision_transform)\nval_dataset   = Flickr30kDataset(val_flickr, tokenizer, vision_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n                          num_workers=8, pin_memory=True, drop_last=True)\nval_loader   = DataLoader(val_dataset, batch_size=256, shuffle=False,\n                          num_workers=8, pin_memory=True)\n```\n\n---  \n\n## 4️⃣ Model Architecture  \n\n### 4.1 Vision Encoder (ViT‑B/32)  \n\nWe will **re‑use** the pretrained ViT weights from the CLIP checkpoint (available on HF) and **strip** the projection head.\n\n```python\n# %%\nfrom transformers import CLIPVisionModel, CLIPTextModel, CLIPProcessor\n\n# Load the *pre‑trained* CLIP vision backbone (weights are frozen initially)\nclip_vision = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_text   = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n```\n\n### 4.2 Projection Heads  \n\nBoth modalities are projected to a **common embedding space** of dimension `d = 512`.\n\n```python\n# %%\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim, out_dim=512):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Linear(in_dim, out_dim, bias=False),\n            nn.LayerNorm(out_dim)\n        )\n        # Initialise with small std (as in the original paper)\n        nn.init.normal_(self.proj[0].weight, std=0.02)\n\n    def forward(self, x):\n        return self.proj(x)\n```\n\n```python\n# %%\nclass CLIPModel(nn.Module):\n    def __init__(self, vision_encoder, text_encoder, embed_dim=512):\n        super().__init__()\n        self.vision_encoder = vision_encoder\n        self.text_encoder   = text_encoder\n\n        # Vision: CLS token (or pooled output) → embed_dim\n        self.visual_proj = ProjectionHead(vision_encoder.config.hidden_size, embed_dim)\n\n        # Text: CLS token → embed_dim\n        self.text_proj   = ProjectionHead(text_encoder.config.hidden_size, embed_dim)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * math.log(1/0.07))\n\n    def encode_image(self, pixel_values):\n        # pixel_values: (B, 3, 224, 224)\n        vision_output = self.vision_encoder(pixel_values=pixel_values, output_attentions=False, output_hidden_states=False)\n        # CLS token is the first token of the last hidden state\n        img_emb = vision_output.pooler_output  # shape (B, hidden)\n        img_emb = self.visual_proj(img_emb)\n        return img_emb\n\n    def encode_text(self, input_ids, attention_mask):\n        text_output = self.text_encoder(input_ids=input_ids,\n                                        attention_mask=attention_mask,\n                                        output_attentions=False,\n                                        output_hidden_states=False)\n        txt_emb = text_output.pooler_output\n        txt_emb = self.text_proj(txt_emb)\n        return txt_emb\n\n    def forward(self, pixel_values, input_ids, attention_mask):\n        img_emb = self.encode_image(pixel_values)\n        txt_emb = self.encode_text(input_ids, attention_mask)\n\n        # Normalise to unit sphere\n        img_emb = F.normalize(img_emb, dim=-1)\n        txt_emb = F.normalize(txt_emb, dim=-1)\n\n        # Compute similarity matrix\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * img_emb @ txt_emb.t()\n        logits_per_text  = logits_per_image.t()\n        return logits_per_image, logits_per_text, img_emb, txt_emb\n```\n\n```python\n# %%\nmodel = CLIPModel(clip_vision, clip_text, embed_dim=512).to(device)\nprint(model)\n```\n\n### 4.3 Optional Cross‑modal Transformer (Advanced)  \n\nIf you want to experiment with **cross‑modal attention**, replace the `encode_image`/`encode_text` methods with a small transformer that attends to the other modality.  \nThe code is provided in the appendix (see the *Cross‑modal Extension* cell at the end of the notebook).\n\n---  \n\n## 5️⃣ Training Loop – Advanced Optimisation  \n\nKey tricks we will use:\n\n| Trick | Why |\n|------|-----|\n| **Mixed‑precision (AMP)** | 2‑3× speed‑up on modern GPUs |\n| **Gradient accumulation** | Simulate larger batch sizes without extra memory |\n| **Cosine LR schedule + warm‑up** | Stabilises contrastive training |\n| **EMA of model weights** | Improves zero‑shot performance |\n| **Label smoothing on the contrastive targets** | Prevents over‑confidence |\n| **Distributed data parallel (optional)** | Scale to multiple GPUs |\n\n```python\n# %%\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\n# Hyper‑parameters\nepochs = 12\nbatch_size = 128\naccum_steps = 2          # effective batch = 256\nlr = 5e-4\nweight_decay = 0.01\nwarmup_epochs = 1\nmax_lr = lr\n\noptimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.98), eps=1e-6)\nscheduler = CosineAnnealingLR(optimizer, T_max=epochs * len(train_loader) // accum_steps)\n\nscaler = GradScaler()\n```\n\n### 5.1 Contrastive Loss Helper  \n\n```python\n# %%\ndef contrastive_loss(logits):\n    \"\"\"\n    Symmetric InfoNCE loss.\n    logits: (B, B) similarity matrix (already scaled by logit_scale)\n    \"\"\"\n    B = logits.shape[0]\n    labels = torch.arange(B, device=logits.device)\n    loss_i2t = F.cross_entropy(logits, labels)\n    loss_t2i = F.cross_entropy(logits.t(), labels)\n    return (loss_i2t + loss_t2i) / 2\n```\n\n### 5.2 Training Epoch  \n\n```python\n# %%\ndef train_one_epoch(epoch_idx):\n    model.train()\n    running_loss = 0.0\n    optimizer.zero_grad()\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch_idx+1}/{epochs}\")\n\n    for step, batch in pbar:\n        images = batch[\"image\"].to(device, non_blocking=True)\n        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n\n        with autocast():\n            logits_per_image, logits_per_text, _, _ = model(images, input_ids, attention_mask)\n            loss = contrastive_loss(logits_per_image)\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % accum_steps == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n\n        running_loss += loss.item()\n        pbar.set_postfix(loss=running_loss/(step+1))\n\n    avg_loss = running_loss / len(train_loader)\n    return avg_loss\n```\n\n### 5.3 Validation – Retrieval Metrics  \n\n```python\n# %%\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval()\n    all_img = []\n    all_txt = []\n    for batch in tqdm(loader, desc=\"Eval\"):\n        images = batch[\"image\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        img_emb = model.encode_image(images)\n        txt_emb = model.encode_text(input_ids, attention_mask)\n\n        all_img.append(img_emb.cpu())\n        all_txt.append(txt_emb.cpu())\n\n    img_emb = torch.cat(all_img)   # (N, d)\n    txt_emb = torch.cat(all_txt)   # (N, d)\n\n    # Compute similarity matrix\n    sim = img_emb @ txt_emb.t()    # (N, N)\n\n    # Retrieval: for each image, rank all captions\n    ranks_i2t = torch.argsort(sim, dim=1, descending=True).cpu().numpy()\n    ranks_t2i = torch.argsort(sim, dim=0, descending=True).cpu().numpy()\n\n    def recall_at_k(ranks, K):\n        N = ranks.shape[0]\n        hits = 0\n        for i in range(N):\n            if i in ranks[i, :K]:\n                hits += 1\n        return hits / N\n\n    recalls = {\n        \"R@1_i2t\": recall_at_k(ranks_i2t, 1),\n        \"R@5_i2t\": recall_at_k(ranks_i2t, 5),\n        \"R@10_i2t\": recall_at_k(ranks_i2t, 10),\n        \"R@1_t2i\": recall_at_k(ranks_t2i, 1),\n        \"R@5_t2i\": recall_at_k(ranks_t2i, 5),\n        \"R@10_t2i\": recall_at_k(ranks_t2i, 10),\n    }\n    return recalls\n```\n\n### 5.4 Full Training Loop  \n\n```python\n# %%\ntrain_losses = []\nval_recalls = []\n\nfor"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 135,
    "completion_tokens": 4122,
    "total_tokens": 4257
  }
}