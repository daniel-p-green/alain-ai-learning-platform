{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# gpt-oss-20b: Getting Started with a 20 Billion‑Parameter GPT Model\n\nThis notebook guides absolute beginners through installing, loading, and using the open‑source 20B GPT model. Using simple analogies and step‑by‑step code, you’ll learn how to generate text, fine‑tune on a tiny dataset, and spot common pitfalls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:14:17.832Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Understand what a 20B GPT model is and why it matters.\n",
        "2. Install and configure the required libraries, including ipywidgets.\n",
        "3. Load the model and tokenizer, and generate text with a single prompt.\n",
        "4. Fine‑tune the model on a small custom dataset and evaluate its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic Python knowledge (variables, loops, functions).\n",
        "- A Jupyter Notebook environment (e.g., JupyterLab or Colab).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.40.0 accelerate>=0.28.0 datasets>=2.20.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.40.0\",\"accelerate>=0.28.0\",\"datasets>=2.20.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Introduction and Setup\n",
        "\n",
        "## What is the 20‑Billion‑Parameter GPT Model?\n",
        "\n",
        "Imagine a library that contains **20 billion** different books. Each book is a tiny piece of knowledge that the model can pull from when it’s asked a question. In the world of machine learning, those books are called *parameters*—tiny knobs that the model turns to decide what word comes next. The more knobs you have, the more nuanced the decisions can be, just like a larger library lets you find more specific information.\n",
        "\n",
        "The **GPT‑OSS‑20B** is a *transformer* architecture, which means it looks at the whole sentence (or paragraph) at once, rather than reading it word‑by‑word. Think of it as a super‑fast reader that can understand context from the entire text before deciding on the next word.\n",
        "\n",
        "### Why 20 Billion? Why Care?\n",
        "\n",
        "- **Expressiveness**: With 20 billion knobs, the model can capture subtle patterns in language that smaller models miss.\n",
        "- **Context window**: It can remember up to 4 096 tokens (roughly 2–3 paragraphs) in one go, which is great for longer stories or technical explanations.\n",
        "- **Open‑source**: You can run it locally (if you have the hardware) or on the cloud, giving you full control over data privacy.\n",
        "\n",
        "### Key Terms & Trade‑offs\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Parameter** | A weight in the neural network that the model learns during training. | Determines the model’s capacity to learn patterns. | More parameters → more memory and compute. |\n",
        "| **Transformer** | A neural network that uses self‑attention to weigh the importance of each token in a sequence. | Handles long‑range dependencies efficiently. | Requires careful memory management. |\n",
        "| **Tokenizer** | Converts raw text into a sequence of tokens that the model can understand. | Enables the model to process any language. | Tokenization errors can lead to mis‑generation. |\n",
        "| **Context window** | The maximum number of tokens the model can consider at once. | Limits how much text you can feed in a single pass. | Larger windows need more GPU memory. |\n",
        "| **Seed** | A starting point for random number generators. | Ensures reproducible results. | Different seeds can produce slightly different outputs. |\n",
        "\n",
        "**Rationale**: We’re balancing *model size* (20 B) with *hardware feasibility*. A 20 B model is large enough to produce high‑quality text but still small enough that many modern GPUs (e.g., 24 GB RTX 4090) can run it in inference mode. For training, you’ll need specialized hardware or distributed setups.\n",
        "\n",
        "## Quick Setup Checklist\n",
        "\n",
        "1. **Python 3.8+** – The libraries we’ll use are compatible with Python 3.8 and newer.\n",
        "2. **Jupyter Notebook/Lab** – We’ll run the code in a notebook for interactive exploration.\n",
        "3. **GPU (optional but recommended)** – A CUDA‑enabled GPU speeds up inference dramatically.\n",
        "4. **Environment Variables** – `HF_TOKEN` for Hugging Face model access; `OPENAI_API_KEY` if you want to compare with OpenAI’s API.\n",
        "\n",
        "## Install the Required Libraries\n",
        "\n",
        "Below is a single code cell that installs everything you need. It includes error handling and a quick check for CUDA availability.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies with error handling\n",
        "# Note: In a notebook, use !pip to run shell commands\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "\n",
        "# Helper function to install a package\n",
        "\n",
        "def install(package):\n",
        "    try:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", package]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "        except Exception as exc:\n",
        "            if IN_COLAB:\n",
        "                packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                if packages:\n",
        "                    try:\n",
        "                        import IPython\n",
        "                        ip = IPython.get_ipython()\n",
        "                        if ip is not None:\n",
        "                            ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                        else:\n",
        "                            import subprocess as _subprocess\n",
        "                            _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                    except Exception as colab_exc:\n",
        "                        print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                        raise\n",
        "                else:\n",
        "                    print('No packages specified for pip install; skipping fallback')\n",
        "            else:\n",
        "                raise\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error installing {package}: {e}\")\n",
        "\n",
        "# Upgrade pip first\n",
        "install(\"--upgrade pip\")\n",
        "\n",
        "# Install required packages\n",
        "packages = [\n",
        "    \"ipywidgets>=8.0.0\",\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"accelerate>=0.28.0\",\n",
        "    \"datasets>=2.20.0\"\n",
        "]\n",
        "for pkg in packages:\n",
        "    install(pkg)\n",
        "\n",
        "# Enable widgets extension (only needed in JupyterLab)\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"jupyter\", \"nbextension\", \"enable\", \"--py\", \"widgetsnbextension\", \"--sys-prefix\"])\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"Widgets extension already enabled or not needed.\")\n",
        "\n",
        "# Quick CUDA check\n",
        "try:\n",
        "    import torch\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"PyTorch not installed or CUDA check failed:\", e)\n",
        "\n",
        "print(\"All dependencies installed successfully.\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Installing Dependencies\n",
        "\n",
        "When you build a house, you need the right bricks, wood, and tools. In machine‑learning, the *bricks* are the Python libraries that give the model its power, the *wood* is the GPU that speeds up calculations, and the *tools* are the environment variables that let you pull the model from Hugging Face.\n",
        "\n",
        "Below we’ll install four key libraries:\n",
        "\n",
        "1. **ipywidgets** – lets us create interactive sliders and buttons in the notebook.\n",
        "2. **transformers** – the core library that contains the GPT‑OSS‑20B model and tokenizer.\n",
        "3. **accelerate** – handles distributed training and inference across multiple GPUs.\n",
        "4. **datasets** – a fast, memory‑efficient way to load and preprocess text data.\n",
        "\n",
        "We’ll also set up the `HF_TOKEN` environment variable so the library can authenticate with Hugging Face’s model hub.\n",
        "\n",
        "### Why these exact versions?\n",
        "\n",
        "- **Compatibility**: Newer releases sometimes drop support for older CUDA versions or change API names. Pinning to the latest stable releases (≥ 4.40.0 for transformers, ≥ 0.28.0 for accelerate, ≥ 2.20.0 for datasets) keeps the code working on most recent GPUs.\n",
        "- **Stability**: The 20B model was trained with a specific tokenization scheme that works best with the current `transformers` version.\n",
        "- **Reproducibility**: By installing the same versions, you guarantee that the same code will run the same way on any machine.\n",
        "\n",
        "### Extra explanatory paragraph – key terms and trade‑offs\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **pip** | Python’s package installer. | Lets you download and install libraries from the internet. | Requires internet access and can overwrite system packages if not isolated. |\n",
        "| **virtualenv / conda** | A sandboxed environment for Python packages. | Keeps project dependencies separate from the global Python install. | Adds a layer of setup but prevents version clashes. |\n",
        "| **CUDA** | NVIDIA’s parallel computing platform. | Enables GPU acceleration for deep‑learning workloads. | Requires a compatible GPU and driver; not all machines have one. |\n",
        "| **HF_TOKEN** | A secret key that authenticates you to Hugging Face. | Allows downloading private or large models. | Must be kept confidential; exposing it can lead to quota abuse. |\n",
        "| **seed** | A starting number for random number generators. | Guarantees that the same random choices (e.g., weight initialization) produce identical results. | Different seeds can lead to slightly different outputs; choose one and stick with it for experiments. |\n",
        "\n",
        "**Rationale**: We balance *ease of use* (install everything in one cell) with *control* (pinning versions). The trade‑off is that you might need to upgrade or downgrade if your GPU driver changes, but the code remains stable for the majority of users.\n",
        "\n",
        "## Quick Install Cell\n",
        "\n",
        "Run the cell below. It will:\n",
        "\n",
        "1. Upgrade `pip` to the latest version.\n",
        "2. Install the four libraries with error handling.\n",
        "3. Enable the widgets extension for JupyterLab.\n",
        "4. Print whether CUDA is available.\n",
        "\n",
        "Feel free to run it again if you encounter any errors – the helper function will retry the installation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies with robust error handling\n",
        "# This cell can be run multiple times without breaking the environment\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Helper to install a package and catch errors\n",
        "\n",
        "def install(package):\n",
        "    try:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", package]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "        except Exception as exc:\n",
        "            if IN_COLAB:\n",
        "                packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                if packages:\n",
        "                    try:\n",
        "                        import IPython\n",
        "                        ip = IPython.get_ipython()\n",
        "                        if ip is not None:\n",
        "                            ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                        else:\n",
        "                            import subprocess as _subprocess\n",
        "                            _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                    except Exception as colab_exc:\n",
        "                        print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                        raise\n",
        "                else:\n",
        "                    print('No packages specified for pip install; skipping fallback')\n",
        "            else:\n",
        "                raise\n",
        "        print(f\"✅ Successfully installed {package}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"⚠️  Failed to install {package}: {e}\")\n",
        "\n",
        "# Upgrade pip first\n",
        "install(\"--upgrade pip\")\n",
        "\n",
        "# List of required packages with minimum versions\n",
        "packages = [\n",
        "    \"ipywidgets>=8.0.0\",\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"accelerate>=0.28.0\",\n",
        "    \"datasets>=2.20.0\"\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    install(pkg)\n",
        "\n",
        "# Enable widgets extension (needed for JupyterLab)\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"jupyter\", \"nbextension\", \"enable\", \"--py\", \"widgetsnbextension\", \"--sys-prefix\"])\n",
        "    print(\"✅ Widgets extension enabled\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"⚠️  Widgets extension already enabled or not required\")\n",
        "\n",
        "# Quick CUDA check\n",
        "try:\n",
        "    import torch\n",
        "    cuda_status = torch.cuda.is_available()\n",
        "    print(f\"CUDA available: {cuda_status}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  PyTorch/CUDA check failed: {e}\")\n",
        "\n",
        "print(\"✅ All dependencies installed successfully.\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting the Hugging Face Token\n",
        "\n",
        "The 20B model is hosted on Hugging Face’s model hub. To download it, you need an authentication token. If you don’t have one, sign up at https://huggingface.co/, go to *Settings → Access Tokens*, and create a new token with *Read* scope.\n",
        "\n",
        "Once you have the token, you can set it in the notebook environment. This keeps the token out of the code repository and allows you to reuse the same token across sessions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set the HF_TOKEN environment variable\n",
        "# Replace \"YOUR_TOKEN_HERE\" with your actual Hugging Face token\n",
        "import os\n",
        "\n",
        "HF_TOKEN = \"YOUR_TOKEN_HERE\"\n",
        "if HF_TOKEN == \"YOUR_TOKEN_HERE\":\n",
        "    raise ValueError(\"Please replace YOUR_TOKEN_HERE with your actual Hugging Face token.\")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "print(\"✅ HF_TOKEN set in environment.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reproducibility Check\n",
        "\n",
        "We’ll set a random seed so that any stochastic operation (like model weight initialization or dropout) produces the same result every time you run the notebook. This is especially useful when you later fine‑tune the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set a global seed for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(f\"✅ Seed set to {SEED} for Python, NumPy, and PyTorch.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the 20‑Billion‑Parameter GPT Model and Tokenizer\n",
        "\n",
        "Imagine you’re opening a massive library that contains **20 billion** books. Each book is a tiny piece of knowledge the model can pull from when you ask a question. In practice, we *load* that library into memory and give the model a *tokenizer*—a tool that turns raw text into the numbered tokens the model understands.\n",
        "\n",
        "### Why do we need a tokenizer?\n",
        "\n",
        "A tokenizer is like a translator that converts words, punctuation, and even parts of words into a unique number. The model only works with numbers, so the tokenizer is the bridge between human language and the model’s internal representation.\n",
        "\n",
        "### Why do we care about `device_map` and `offload`?\n",
        "\n",
        "The 20B model is huge—about 80 GB of weights if you store them as 32‑bit floats. Most GPUs only have 24 GB, so we need to *offload* some of the weights to the CPU or use *quantization* (reducing precision) to fit everything in GPU memory. The `accelerate` library lets us automatically split the model across devices and keep the parts that change most (like the last layers) in the GPU for speed.\n",
        "\n",
        "### Extra explanatory paragraph – key terms and trade‑offs\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Tokenizer** | Converts text to token IDs. | Enables the model to process any language. | Tokenization errors can lead to mis‑generation. |\n",
        "| **device_map** | Dictates which parts of the model live on which device (GPU/CPU). | Allows large models to run on limited GPU memory. | More CPU‑GPU traffic can slow inference. |\n",
        "| **offload** | Moves less‑used layers to CPU memory. | Saves GPU RAM. | Increases latency due to data transfer. |\n",
        "| **torch_dtype** | Data type for model weights (e.g., `float16`, `bfloat16`). | Speeds up computation and reduces memory. | Lower precision can slightly degrade quality. |\n",
        "| **quantization** | Reduces weight precision to 8‑bit or 4‑bit. | Dramatically cuts memory usage. | Can introduce noticeable artifacts in output. |\n",
        "| **seed** | Starting point for random number generators. | Guarantees reproducible results. | Different seeds produce slightly different outputs. |\n",
        "\n",
        "**Rationale**: We balance *model fidelity* (keeping as many high‑precision weights as possible) with *hardware feasibility* (using offload and quantization to fit the model on a single 24 GB GPU). The trade‑offs are mainly between speed, memory, and output quality.\n",
        "\n",
        "### Quick sanity check\n",
        "\n",
        "Below we’ll load the tokenizer and the model with `accelerate`. The code is split into two short cells so you can see each step clearly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Load tokenizer and model with accelerate\n",
        "# -----------------------------------------------------\n",
        "# 1️⃣  Set a reproducible seed for any stochastic ops\n",
        "# 2️⃣  Load the tokenizer (fast, lightweight)\n",
        "# 3️⃣  Load the 20B model with device mapping and optional quantization\n",
        "# 4️⃣  Verify that the model is on the expected device(s)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "\n",
        "# 1️⃣ Reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2️⃣ Tokenizer\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(\"Tokenizer loaded. vocab size:\", tokenizer.vocab_size)\n",
        "\n",
        "# 3️⃣ Model – use accelerate to split across GPU/CPU\n",
        "print(\"Loading model… (this may take a few minutes)\\n\")\n",
        "# We use torch_dtype=float16 for speed; change to bfloat16 if your GPU supports it\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",  # let accelerate decide\n",
        "    offload_folder=\"/tmp/torch_offload\",  # where to store offloaded layers\n",
        "    offload_state_dict=True,\n",
        ")\n",
        "\n",
        "# 4️⃣ Verify device placement\n",
        "print(\"\\nDevice placement summary:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.device.type == \"cuda\":\n",
        "        print(f\"{name[:30]:30} -> GPU\")\n",
        "        break\n",
        "else:\n",
        "    print(\"No parameters on GPU – check your CUDA setup.\")\n",
        "\n",
        "print(\"\\nModel loaded successfully!\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Quick generation test\n",
        "# ---------------------------------\n",
        "# Encode a short prompt, generate a few tokens, and decode back to text.\n",
        "\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(model.device)  # move to the same device as the model\n",
        "\n",
        "# Generate 50 tokens with temperature 0.7 (controls randomness)\n",
        "generated_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=50,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated text:\\n\")\n",
        "print(generated_text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Generating Text with a Prompt\n",
        "\n",
        "### The “Ask‑Me‑Anything” Playground\n",
        "\n",
        "Think of the 20‑B model as a giant library that can write stories, answer questions, or even compose poetry. The only thing you need to do is give it a *prompt*—a starting sentence or question—and let it finish the rest. It’s like handing a blank notebook to a super‑creative friend and saying, “Write a story that starts with *Once upon a time*.” The friend will then write the rest, drawing on everything they’ve learned from the books in the library.\n",
        "\n",
        "### How the Model Turns a Prompt into Text\n",
        "\n",
        "1. **Tokenization** – The prompt is split into *tokens* (tiny pieces of words or punctuation). Each token gets a unique number.\n",
        "2. **Encoding** – Those numbers are fed into the transformer layers, which look at the whole prompt at once and decide what the next token should be.\n",
        "3. **Sampling** – The model doesn’t always pick the single most likely next token. Instead, we can tweak how random it is with parameters like **temperature** (how wild the choices are) and **top‑p** (how many top candidates to consider).\n",
        "4. **Decoding** – The chosen token numbers are turned back into readable text.\n",
        "\n",
        "### Extra Explanatory Paragraph – Key Terms & Trade‑offs\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Prompt** | The initial text you give the model. | Sets the context and direction of the output. | Too short → vague output; too long → may hit the context window limit. |\n",
        "| **Temperature** | Controls randomness (0 = deterministic, >1 = more random). | Higher values produce more creative but less coherent text. | Low temperature → safe but repetitive; high temperature → creative but possibly nonsensical. |\n",
        "| **Top‑p (nucleus sampling)** | Keeps only the smallest set of tokens whose cumulative probability exceeds *p*. | Balances diversity and quality. | Very low p → overly conservative; very high p → too many unlikely tokens. |\n",
        "| **Max New Tokens** | How many tokens the model should generate beyond the prompt. | Determines length of output. | Too many → risk of running out of context; too few → incomplete sentences. |\n",
        "| **Seed** | Starting point for random number generators. | Guarantees reproducible generation when the same seed is used. | Different seeds → slightly different outputs; useful for exploring variability. |\n",
        "\n",
        "**Rationale**: We expose the model to a prompt and let it generate text while giving you knobs (temperature, top‑p, max tokens) to control creativity vs. coherence. The trade‑offs are mainly between *predictability* and *novelty*. By tuning these parameters, you can produce anything from a factual answer to a whimsical story.\n",
        "\n",
        "### Quick Hands‑On Example\n",
        "\n",
        "Below we’ll define a small helper function that takes a prompt and a few generation settings, then prints the generated text. Feel free to experiment with the parameters to see how the output changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper function for text generation\n",
        "# -------------------------------------------------\n",
        "# 1️⃣  Imports and seed setup\n",
        "# 2️⃣  Encode the prompt\n",
        "# 3️⃣  Generate tokens with user‑defined settings\n",
        "# 4️⃣  Decode and print the result\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Reproducibility: set a global seed\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load tokenizer & model (assumes model already downloaded)\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(\"Loading model…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    offload_folder=\"/tmp/torch_offload\",\n",
        "    offload_state_dict=True,\n",
        ")\n",
        "\n",
        "# Generation function\n",
        "def generate_text(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 60,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.9,\n",
        "    seed: int | None = None,\n",
        ") -> str:\n",
        "    \"\"\"Generate text from a prompt using the 20‑B model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt: str\n",
        "        The starting text.\n",
        "    max_new_tokens: int\n",
        "        How many tokens to generate beyond the prompt.\n",
        "    temperature: float\n",
        "        Controls randomness.\n",
        "    top_p: float\n",
        "        Nucleus sampling threshold.\n",
        "    seed: int | None\n",
        "        Optional seed for reproducibility.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Quick demo\n",
        "prompt_text = \"Once upon a time, in a land far, far away\"\n",
        "print(\"\\nGenerated output:\\n\")\n",
        "print(generate_text(prompt_text, max_new_tokens=80, temperature=0.8, top_p=0.95))\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Prompt Widget\n",
        "\n",
        "Below is a tiny interactive widget that lets you type a prompt and instantly see the model’s response. It’s built with `ipywidgets` so you can play around without editing code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Interactive prompt widget\n",
        "# ---------------------------------------\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "prompt_input = widgets.Textarea(\n",
        "    value=\"Write a short poem about a sunrise.\",\n",
        "    placeholder=\"Type your prompt here…\",\n",
        "    description=\"Prompt:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"80px\"),\n",
        ")\n",
        "\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_button_click(_):\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "        print(\"Generating…\")\n",
        "        text = generate_text(prompt_input.value, max_new_tokens=120, temperature=0.9, top_p=0.9)\n",
        "        print(text)\n",
        "\n",
        "run_button = widgets.Button(description=\"Generate\", button_style=\"success\")\n",
        "run_button.on_click(on_button_click)\n",
        "\n",
        "display(prompt_input, run_button, output_area)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Understanding Tokenization and Context Window\n",
        "\n",
        "### Tokenization – The Model’s Alphabet\n",
        "\n",
        "Imagine you’re a librarian who can only read a book if every word is written in a special code. That code is the **tokenizer**. It splits raw text into *tokens*—tiny, numbered pieces that the model can understand. In practice, the tokenizer does more than just split on spaces: it handles punctuation, rare words, and even sub‑word pieces. For example, the word *“unbelievable”* might become the tokens `un`, `##bel`, `##ieve`, `##able` in a WordPiece‑style tokenizer.\n",
        "\n",
        "Why does this matter? Because the model’s internal math operates on numbers, not letters. The tokenizer is the bridge that turns your human prompt into a sequence of integers that the transformer can process.\n",
        "\n",
        "### Context Window – The Model’s Memory Span\n",
        "\n",
        "A transformer can look at **every token in the input at once**. However, it can only remember a limited number of tokens at a time. This limit is called the **context window**. For GPT‑OSS‑20B the window is 4 096 tokens. Think of it as a notebook that can hold 4 096 words (roughly 2–3 paragraphs). Anything beyond that is invisible to the model during a single forward pass.\n",
        "\n",
        "When you generate text, the model keeps the prompt plus the newly generated tokens in this window. If the total exceeds 4 096, the oldest tokens are dropped (a technique called *sliding window*). This is why very long prompts can truncate earlier parts of the conversation.\n",
        "\n",
        "### Extra Explanatory Paragraph – Key Terms & Trade‑offs\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Tokenizer** | Converts text to token IDs. | Enables the model to process any language. | Tokenization errors can lead to mis‑generation. |\n",
        "| **Token** | A single unit (word, sub‑word, punctuation). | The basic unit of model input. | More tokens → longer context needed. |\n",
        "| **Context Window** | Max number of tokens the model can attend to. | Determines how much text the model can consider. | Larger windows require more GPU memory. |\n",
        "| **Sliding Window** | Strategy to drop oldest tokens when exceeding the window. | Allows generation of longer sequences. | Can lose earlier context, affecting coherence. |\n",
        "| **Seed** | Starting point for random number generators. | Guarantees reproducible tokenization and generation. | Different seeds produce slightly different token counts. |\n",
        "\n",
        "**Rationale**: We balance *expressiveness* (more tokens → richer context) with *hardware feasibility* (GPU memory limits). The 4 096‑token window is a sweet spot for many use‑cases: it’s large enough for short stories or technical explanations, yet small enough to fit on a single 24 GB GPU when using float16 weights.\n",
        "\n",
        "### Quick Hands‑On: Token Count & Window Check\n",
        "\n",
        "Below we’ll load the tokenizer, encode a sample prompt, and print the number of tokens. We’ll also show how to truncate a prompt that exceeds the context window.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tokenization demo – count tokens and enforce context window\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Load tokenizer (already downloaded in previous steps)\n",
        "# 2️⃣  Encode a long prompt\n",
        "# 3️⃣  Show token count\n",
        "# 4️⃣  Truncate if > 4096 tokens\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Example prompt (about 5 000 words – will exceed 4096 tokens)\n",
        "long_prompt = (\n",
        "    \"Once upon a time, in a land far, far away, there lived a curious child named Alex. \"\n",
        "    * 200  # repeat to inflate length\n",
        ")\n",
        "\n",
        "# Encode\n",
        "encoded = tokenizer(long_prompt, return_tensors=\"pt\")\n",
        "input_ids = encoded.input_ids[0]\n",
        "print(f\"Total tokens in prompt: {len(input_ids)}\")\n",
        "\n",
        "# Context window limit\n",
        "MAX_TOKENS = 4096\n",
        "if len(input_ids) > MAX_TOKENS:\n",
        "    print(\"Prompt exceeds context window – truncating to last 4096 tokens.\")\n",
        "    input_ids = input_ids[-MAX_TOKENS:]\n",
        "    print(f\"Tokens after truncation: {len(input_ids)}\")\n",
        "else:\n",
        "    print(\"Prompt fits within context window.\")\n",
        "\n",
        "# Decode back to text (optional, just to show truncation effect)\n",
        "truncated_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "print(\"\\nFirst 200 characters of truncated prompt:\\n\", truncated_text[:200])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generation with context window awareness\n",
        "# --------------------------------------------\n",
        "# This cell shows how the model handles a prompt that exactly fills the window.\n",
        "# We generate a few tokens and observe that the model keeps the last 4096 tokens.\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Load model (float16 for speed, device_map auto)\n",
        "print(\"Loading model…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    offload_folder=\"/tmp/torch_offload\",\n",
        "    offload_state_dict=True,\n",
        ")\n",
        "\n",
        "# Prepare input_ids from previous truncation\n",
        "input_ids = input_ids.unsqueeze(0).to(model.device)  # batch dim\n",
        "\n",
        "# Generate 20 new tokens\n",
        "generated = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=20,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# Decode and print\n",
        "generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated continuation (20 tokens):\\n\", generated_text)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Fine‑Tuning on a Tiny Dataset\n",
        "\n",
        "### Why Fine‑Tune?  \n",
        "Think of the 20‑B model as a *master chef* who has learned a huge variety of recipes from a gigantic cookbook.  \n",
        "If you want the chef to specialize in *vegan lasagna*, you don’t need to teach them a brand‑new recipe from scratch; you just give them a few examples of vegan lasagna and let them tweak the seasoning.  \n",
        "That’s exactly what fine‑tuning does: it takes a pre‑trained model and nudges its weights so it performs better on a *specific* task or domain.\n",
        "\n",
        "### The LoRA Trick  \n",
        "Fine‑tuning a 20‑B model on a single GPU would normally require **80 GB** of memory—way beyond most machines.  \n",
        "**LoRA (Low‑Rank Adaptation)** is a lightweight hack that adds a *tiny* set of extra weights (a few megabytes) to the model.  \n",
        "During training only these new weights are updated; the original 20‑B weights stay frozen.  \n",
        "This is like adding a small spice jar to the chef’s pantry instead of re‑learning all the spices.\n",
        "\n",
        "### Extra Explanatory Paragraph – Key Terms & Trade‑offs\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Fine‑tuning** | Updating a pre‑trained model on a new dataset. | Adapts the model to a specific domain or style. | Can overfit if the dataset is too small. |\n",
        "| **LoRA** | Low‑rank adapters that are trained while freezing the base model. | Drastically reduces memory and compute. | Slightly less expressive than full fine‑tuning. |\n",
        "| **Dataset** | Collection of text examples for training. | Provides the signal the model learns from. | Quality and size directly affect performance. |\n",
        "| **Tokenizer** | Converts raw text to token IDs. | Needed for both training and inference. | Tokenization errors can mislead training. |\n",
        "| **Training loop** | Iteratively feeds batches to the model, computes loss, and updates weights. | Core of the learning process. | Longer loops mean more compute time. |\n",
        "| **Loss** | Numerical measure of prediction error. | Guides the optimizer. | Poorly chosen loss can mislead learning. |\n",
        "| **Optimizer** | Algorithm that updates weights based on gradients. | Controls learning speed and stability. | Aggressive optimizers can cause divergence. |\n",
        "| **Scheduler** | Adjusts learning rate over time. | Helps convergence. | Wrong schedule can stall training. |\n",
        "| **Seed** | Starting point for random number generators. | Guarantees reproducible experiments. | Different seeds produce slightly different results. |\n",
        "\n",
        "**Rationale**:  \n",
        "We use LoRA to keep the 20‑B model’s massive knowledge intact while still allowing it to specialize on a tiny dataset.  \n",
        "The trade‑off is a modest drop in flexibility for a huge gain in memory efficiency—exactly what you need when you only have a single 24 GB GPU.\n",
        "\n",
        "### Quick Hands‑On: Preparing a Tiny Dataset\n",
        "Below we’ll create a tiny synthetic dataset of *short stories* (you can replace it with your own data).  \n",
        "We’ll use the 🤗 `datasets` library to load, tokenize, and batch the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Load & preprocess a tiny dataset (≤30 lines)\n",
        "# -----------------------------------------------------\n",
        "# 1️⃣  Set a reproducible seed\n",
        "# 2️⃣  Create a toy dataset (you can replace with your own CSV/JSON)\n",
        "# 3️⃣  Tokenize and batch the data\n",
        "# 4️⃣  Prepare a DataCollator for language modeling\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
        "\n",
        "# 1️⃣ Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2️⃣ Toy dataset – 10 short stories\n",
        "stories = [\n",
        "    \"The sun set over the hills, painting the sky orange.\",\n",
        "    \"A curious cat named Whiskers discovered a hidden garden.\",\n",
        "    \"In the distant future, humans and robots co‑existed peacefully.\",\n",
        "    \"The old oak tree whispered secrets to the wind.\",\n",
        "    \"A brave knight rode into the dragon’s lair.\",\n",
        "    \"The city lights flickered as the storm approached.\",\n",
        "    \"A lonely astronaut sang a lullaby to the stars.\",\n",
        "    \"The river flowed gently through the valley.\",\n",
        "    \"A mysterious map led to a forgotten treasure.\",\n",
        "    \"The moon reflected on the calm lake.\",\n",
        "]\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": stories})\n",
        "\n",
        "# 3️⃣ Tokenizer\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Tokenize with truncation to 512 tokens (safe for fine‑tuning)\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])  # 512 tokens per example\n",
        "\n",
        "# 4️⃣ Data collator for causal LM\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "print(\"Dataset ready:\", len(tokenized), \"examples, each with\", tokenized[0]['input_ids'].shape[0], \"tokens\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine‑Tuning with LoRA  \n",
        "We’ll use 🤗 `transformers`’s `get_peft_model` to wrap the 20‑B model with LoRA adapters.  \n",
        "The training loop is handled by `Trainer`, which takes care of batching, gradient accumulation, and mixed‑precision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Fine‑tune with LoRA (≤30 lines)\n",
        "# -----------------------------------------------------\n",
        "# 1️⃣  Load the base model (weights frozen)\n",
        "# 2️⃣  Wrap with LoRA adapters\n",
        "# 3️⃣  Set up Trainer and run training\n",
        "\n",
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# 1️⃣ Load base model (float16 for speed)\n",
        "print(\"Loading base model…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    offload_folder=\"/tmp/torch_offload\",\n",
        "    offload_state_dict=True,\n",
        ")\n",
        "\n",
        "# Freeze base weights\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# 2️⃣ LoRA config – rank 8, alpha 16, dropout 0.05\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # only query/value projections\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "print(\"LoRA adapters added – trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "# 3️⃣ Training arguments – 2 epochs, small batch, gradient accumulation\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,  # effective batch size 8\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    evaluation_strategy=\"no\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting training…\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training complete – saving LoRA weights\")\n",
        "trainer.save_model(\"./lora-finetuned\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick Evaluation  \n",
        "After training we can generate a short continuation to see how the model has adapted.  \n",
        "We’ll use the same helper function from Step 4 but load the LoRA‑fine‑tuned weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate the fine‑tuned model\n",
        "# ---------------------------------------\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "print(\"Loading fine‑tuned model…\")\n",
        "finetuned = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./lora-finetuned\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        ")\n",
        "\n",
        "prompt = \"The brave knight rode into the dragon’s lair and\"  # incomplete sentence\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(finetuned.device)\n",
        "output_ids = finetuned.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=30,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "print(\"\\nGenerated continuation:\\n\", tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following best describes the 20B GPT model?\", [\"A small, rule‑based chatbot.\",\"A 20‑billion‑parameter transformer trained on diverse text.\",\"A convolutional neural network for image classification.\",\"A reinforcement learning agent for games.\"], 1, \"The 20B GPT model is a transformer with 20 billion parameters, trained on a large corpus of text.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the maximum number of tokens the 20B model can process in one pass?\", [\"512\",\"2048\",\"4096\",\"8192\"], 2, \"The 20B GPT model supports a context window of up to 4096 tokens.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:14:17.825Z",
      "title": "gpt-oss-20b: Getting Started with a 20 Billion‑Parameter GPT Model",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:14:17.832Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}