{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîß Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('üìù Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîê Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'üîè Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '‚àÖ'\\n    return v[:3] + '‚Ä¶' + v[-2:] if len(v) > 6 else '‚Ä¢‚Ä¢‚Ä¢'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üåê ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('‚úÖ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('‚ö†Ô∏è Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîé Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('‚ö†Ô∏è Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('‚úÖ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('‚ö†Ô∏è Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) ‚Äî 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying and Fine‚ÄëTuning GPT‚ÄëOss‚Äë20B in Jupyter: A Practitioner‚Äôs Guide\n\nThis notebook walks experienced ML practitioners through the end‚Äëto‚Äëend process of loading, configuring, and fine‚Äëtuning the 20B‚Äëparameter GPT‚ÄëOss model using Hugging Face libraries and LoRA adapters. It emphasizes practical setup, GPU memory management, and reproducible training workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ‚è±Ô∏è Estimated time to complete: 36‚Äì60 minutes (rough).  ",
        "\n> üïí Created (UTC): 2025-09-16T03:15:47.070Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Understand the architecture and tokenization of GPT‚ÄëOss‚Äë20B.\n",
        "2. Load the model from the Hugging Face Hub and configure GPU memory efficiently.\n",
        "3. Apply LoRA adapters for lightweight fine‚Äëtuning on custom datasets.\n",
        "4. Evaluate the fine‚Äëtuned model and interpret training metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+ with pip\n",
        "- Basic knowledge of PyTorch and Hugging Face Transformers\n",
        "- Access to a GPU with at least 24‚ÄØGB VRAM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 torch>=2.0.0 transformers>=4.40.0 accelerate>=0.28.0 datasets>=2.20.0 peft>=0.6.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"torch>=2.0.0\",\"transformers>=4.40.0\",\"accelerate>=0.28.0\",\"datasets>=2.20.0\",\"peft>=0.6.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('‚úÖ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Setup\n",
        "\n",
        "Welcome to the first step of our journey to fine‚Äëtune the 20‚Äëbillion‚Äëparameter GPT‚ÄëOss model. Think of GPT‚ÄëOss as a gigantic library of text‚Äîeach book is a *parameter* that helps the model understand language. Fine‚Äëtuning is like giving that library a new set of bookmarks so it can focus on a specific topic faster.\n",
        "\n",
        "In this section we will:\n",
        "\n",
        "1. **Verify the environment** ‚Äì make sure you have the right Python version, GPU, and the Hugging Face token.\n",
        "2. **Install the required libraries** ‚Äì `torch`, `transformers`, `accelerate`, `datasets`, `peft`, and `ipywidgets`.\n",
        "3. **Set a random seed** ‚Äì reproducibility is the bread‚Äëand‚Äëbutter of ML experiments.\n",
        "4. **Check GPU availability** ‚Äì we‚Äôll confirm that the notebook can see your 24‚ÄØGB GPU.\n",
        "\n",
        "### Why these steps matter\n",
        "\n",
        "* **Reproducibility** ‚Äì By fixing the random seed and using pinned library versions, you can share your notebook and others will get the same results.\n",
        "* **Memory management** ‚Äì GPT‚ÄëOss is huge; we‚Äôll pre‚Äëconfigure PyTorch to use the GPU efficiently.\n",
        "* **Error handling** ‚Äì Early checks prevent the frustrating ‚Äúmodule not found‚Äù or ‚Äúno GPU‚Äù errors that can stall a notebook.\n",
        "\n",
        "#### Key terms\n",
        "\n",
        "- **Parameter** ‚Äì a weight in the neural network; GPT‚ÄëOss has 20‚ÄØB of them.\n",
        "- **LoRA** ‚Äì Low‚ÄëRank Adaptation, a lightweight method that adds a small number of trainable matrices to the model.\n",
        "- **HF_TOKEN** ‚Äì your Hugging Face authentication token that grants access to private models.\n",
        "- **Accelerate** ‚Äì a library that abstracts device placement and mixed‚Äëprecision training.\n",
        "\n",
        "#### Trade‚Äëoffs\n",
        "\n",
        "- **Pinned versions** guarantee consistency but may miss out on bug fixes or performance improvements in newer releases.\n",
        "- **Setting a seed** can slightly reduce randomness, which is good for debugging but may hide stochastic effects that could be useful in some research settings.\n",
        "\n",
        "Let‚Äôs get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Install required packages (run once)\n",
        "# We use pip to install the exact versions that work with GPT‚ÄëOss‚Äë20B.\n",
        "# The `--quiet` flag keeps the output tidy.\n",
        "# If you already have the packages, pip will skip re‚Äëinstalling.\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "packages = [\n",
        "    \"torch>=2.0.0\",\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"accelerate>=0.28.0\",\n",
        "    \"datasets>=2.20.0\",\n",
        "    \"peft>=0.6.0\",\n",
        "    \"ipywidgets>=8.0.0\"\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"--quiet\"], check=True)\n",
        "\n",
        "# Enable Jupyter widgets (only needed once per environment)\n",
        "try:\n",
        "    subprocess.run([sys.executable, \"-m\", \"jupyter\", \"nbextension\", \"enable\", \"--py\", \"widgetsnbextension\"], check=True)\n",
        "except Exception as e:\n",
        "    print(\"Widget extension already enabled or failed to enable:\", e)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2Ô∏è‚É£ Verify environment and set seed\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set a fixed random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Print environment info\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")\n",
        "\n",
        "# Check Hugging Face token\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if HF_TOKEN is None:\n",
        "    print(\"‚ö†Ô∏è  HF_TOKEN environment variable not found.\\n\\tSet it with: export HF_TOKEN=YOUR_TOKEN\")\n",
        "else:\n",
        "    print(\"‚úÖ  HF_TOKEN found.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What you should see\n",
        "\n",
        "- The Python and PyTorch versions should match the pinned ones.\n",
        "- CUDA should report `True` and list your GPU name (e.g., NVIDIA RTX 4090).\n",
        "- A message confirming that `HF_TOKEN` is set.\n",
        "\n",
        "If any of these checks fail, pause the notebook and resolve the issue before moving on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Model Architecture Overview\n",
        "\n",
        "Think of GPT‚ÄëOss‚Äë20B as a **tower of bookshelves**. Each shelf holds a *layer* of the model, and each book on that shelf is a *parameter* that helps the model read and write text. The tower is built from the same type of shelf over and over, but the number of shelves (layers) and the size of each shelf (hidden dimension) vary.\n",
        "\n",
        "### 1Ô∏è‚É£ What makes up a transformer layer?\n",
        "\n",
        "| Component | Role | Analogy |\n",
        "|-----------|------|---------|\n",
        "| **Self‚ÄëAttention** | Lets the model look at every word in the sentence and decide which words matter most for predicting the next word. | A group of students in a classroom pointing at each other to decide who should speak next. |\n",
        "| **Feed‚ÄëForward Network (FFN)** | Applies a small neural net to each word‚Äôs representation, adding non‚Äëlinearity. | A tiny calculator that tweaks each student‚Äôs idea before it‚Äôs shared. |\n",
        "| **LayerNorm** | Normalizes the activations so the network stays stable. | A teacher making sure everyone‚Äôs volume is at a comfortable level. |\n",
        "| **Residual Connection** | Adds the input of the layer back to its output, helping gradients flow. | A safety net that keeps the original idea intact while the student refines it. |\n",
        "\n",
        "The **GPT‚ÄëOss‚Äë20B** architecture stacks **48** of these identical shelves. Each shelf has a hidden size of **12,288** and uses **96** attention heads. The total number of parameters is roughly **20‚ÄØbillion**, which is why we need a powerful GPU and careful memory management.\n",
        "\n",
        "### 2Ô∏è‚É£ Why this design?\n",
        "\n",
        "* **Depth (48 layers)** gives the model a long ‚Äúmemory‚Äù of past tokens, enabling it to capture complex language patterns.\n",
        "* **Width (12,288 hidden units)** allows each layer to hold a rich representation of the input.\n",
        "* **Large number of heads (96)** lets the model attend to many different relationships simultaneously.\n",
        "\n",
        "Trade‚Äëoffs:\n",
        "\n",
        "- **More layers** ‚Üí better performance but higher memory and compute cost.\n",
        "- **Wider layers** ‚Üí richer representations but also more parameters per layer.\n",
        "- **More heads** ‚Üí finer-grained attention but increased parallelism overhead.\n",
        "\n",
        "### 3Ô∏è‚É£ Quick sanity check in code\n",
        "\n",
        "Below we load the model configuration (no weights yet) and print a concise summary. This helps you confirm that the architecture matches the documentation before you start downloading the 20‚ÄØB weights.\n",
        "\n",
        "```python\n",
        "# 1Ô∏è‚É£ Load the config without downloading the huge weights\n",
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,  # GPT‚ÄëOss uses custom code on HF\n",
        "    use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        ")\n",
        "\n",
        "# 2Ô∏è‚É£ Print key hyper‚Äëparameters\n",
        "print(\"Model name:\", config.model_type)\n",
        "print(\"Number of layers (n_layer):\", config.n_layer)\n",
        "print(\"Hidden size (n_embd):\", config.n_embd)\n",
        "print(\"Number of attention heads (n_head):\", config.n_head)\n",
        "print(\"Total parameters (approx):\", config.num_parameters())\n",
        "\n",
        "# 3Ô∏è‚É£ Visualize the layer structure (simple text diagram)\n",
        "print(\"\\nLayer diagram:\\n\")\n",
        "for i in range(config.n_layer):\n",
        "    print(f\"Layer {i+1:02d}: Attention + FFN + LayerNorm + Residual\")\n",
        "```\n",
        "\n",
        "> **‚ö†Ô∏è Note**: The `AutoConfig` call only pulls the configuration file, which is tiny (~10‚ÄØKB). The actual weights are ~80‚ÄØGB and will be downloaded when you instantiate the model.\n",
        "\n",
        "### 4Ô∏è‚É£ Key terms defined\n",
        "\n",
        "- **Transformer**: A neural network architecture that relies on self‚Äëattention to process sequences.\n",
        "- **Self‚ÄëAttention**: Mechanism that lets each token weigh every other token in the sequence.\n",
        "- **Feed‚ÄëForward Network (FFN)**: A two‚Äëlayer MLP applied to each token‚Äôs representation.\n",
        "- **LayerNorm**: Normalization technique that stabilizes training by scaling activations.\n",
        "- **Residual Connection**: Adds the input of a layer to its output, aiding gradient flow.\n",
        "- **HF_TOKEN**: Hugging Face authentication token required to download private or large models.\n",
        "\n",
        "### 5Ô∏è‚É£ Why we expose the config early\n",
        "\n",
        "By inspecting the config before loading weights, you:\n",
        "\n",
        "1. **Avoid wasted bandwidth** ‚Äì if the config shows a mismatch (e.g., wrong number of layers), you can stop early.\n",
        "2. **Verify reproducibility** ‚Äì the config contains the exact hyper‚Äëparameters used by the authors.\n",
        "3. **Plan memory** ‚Äì knowing `n_embd` and `n_layer` lets you estimate VRAM usage.\n",
        "\n",
        "---\n",
        "\n",
        "**Next step**: In Step‚ÄØ3 we‚Äôll actually load the GPT‚ÄëOss‚Äë20B weights onto the GPU, taking care to keep memory usage in check.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick sanity check: print a few token embeddings\n",
        "# This demonstrates that the model can be instantiated without downloading all weights\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Load the model lazily (weights will be streamed as needed)\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",  # let accelerate decide placement\n",
        "    offload_folder=\"/tmp/torch_offload\",  # optional: offload to disk if GPU memory is tight\n",
        ")\n",
        "\n",
        "print(\"Model loaded on device:\", next(model.parameters()).device)\n",
        "\n",
        "# Inspect the first token embedding (just to confirm shapes)\n",
        "import torch\n",
        "sample_input = torch.tensor([[50256]])  # BOS token id\n",
        "with torch.no_grad():\n",
        "    outputs = model(sample_input)\n",
        "print(\"Output shape:\", outputs.last_hidden_state.shape)\n",
        "```\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading GPT‚ÄëOss‚Äë20B from Hugging Face\n",
        "\n",
        "In the previous step we peeked at the model‚Äôs blueprint. Now it‚Äôs time to bring the full 20‚Äëbillion‚Äëparameter engine to life. Think of this as assembling a giant Lego set: the instructions (config) are tiny, but the bricks (weights) are massive. We‚Äôll use Hugging Face‚Äôs `AutoModelForCausalLM` to pull the weights, but we‚Äôll also give the system a few hints so it doesn‚Äôt crash the GPU.\n",
        "\n",
        "### 3Ô∏è‚É£1Ô∏è‚É£ Why we use `device_map=\"auto\"`\n",
        "\n",
        "When you ask the library to load a model, it normally tries to put everything on the first GPU. For a 20‚ÄëB model that would require ~80‚ÄØGB of VRAM‚Äîmore than most single‚ÄëGPU setups provide. The `device_map=\"auto\"` flag tells the `accelerate` backend to split the model across available GPUs or, if only one GPU is present, to stream layers in and out of memory. It‚Äôs like having a warehouse that can move boxes in and out as needed, rather than trying to store the entire shipment in a single shelf.\n",
        "\n",
        "### 3Ô∏è‚É£2Ô∏è‚É£ Optional off‚Äëloading to disk\n",
        "\n",
        "If you‚Äôre on a machine with a single 24‚ÄØGB GPU, you can still run the model by off‚Äëloading the heaviest layers to disk. The `offload_folder` argument creates a temporary directory where those layers are stored when not in use. This trades a bit of CPU‚Äëdisk I/O for the ability to run the model without exceeding VRAM limits.\n",
        "\n",
        "### 3Ô∏è‚É£3Ô∏è‚É£ Trusting remote code\n",
        "\n",
        "GPT‚ÄëOss ships a custom model class that isn‚Äôt part of the standard Transformers distribution. Setting `trust_remote_code=True` allows the library to download and execute that custom class. Think of it as trusting a friend‚Äôs custom recipe that isn‚Äôt on the official cookbook.\n",
        "\n",
        "### 3Ô∏è‚É£4Ô∏è‚É£ Reproducibility and safety\n",
        "\n",
        "We‚Äôll set a deterministic seed for PyTorch before loading the model to ensure that any stochastic operations (e.g., dropout during evaluation) behave consistently across runs. We‚Äôll also catch common errors such as missing `HF_TOKEN` or insufficient GPU memory.\n",
        "\n",
        "### 3Ô∏è‚É£5Ô∏è‚É£ Key terms and trade‚Äëoffs\n",
        "\n",
        "- **`device_map`** ‚Äì a dictionary that tells Accelerate which GPU each layer should live on. `\"auto\"` automatically shards the model.\n",
        "- **`offload_folder`** ‚Äì a path on disk where layers are temporarily stored when not on GPU.\n",
        "- **`trust_remote_code`** ‚Äì allows the library to load custom model definitions from the Hugging Face Hub.\n",
        "- **`torch_dtype`** ‚Äì the data type used for weights (e.g., `torch.float16` for mixed‚Äëprecision). Using lower precision saves memory but can slightly degrade numerical stability.\n",
        "- **Trade‚Äëoffs** ‚Äì Sharding (`device_map`) reduces VRAM usage but increases inter‚ÄëGPU communication overhead. Off‚Äëloading saves VRAM but incurs disk I/O latency. Mixed‚Äëprecision (`torch_dtype`) speeds up inference but may introduce rounding errors.\n",
        "\n",
        "### 3Ô∏è‚É£6Ô∏è‚É£ Quick sanity check\n",
        "\n",
        "Below we load the full model with the recommended settings and print the device each parameter resides on. This gives you a quick visual confirmation that the model is correctly sharded or off‚Äëloaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Import required libraries\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "# 2Ô∏è‚É£ Set a deterministic seed for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 3Ô∏è‚É£ Verify HF_TOKEN is available\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if HF_TOKEN is None:\n",
        "    raise EnvironmentError(\n",
        "        \"HF_TOKEN not found. Set it with export HF_TOKEN=YOUR_TOKEN or os.environ['HF_TOKEN']=...\"\n",
        "    )\n",
        "\n",
        "# 4Ô∏è‚É£ Load the configuration first (tiny file, ~10‚ÄØKB)\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "print(\"Model config loaded: n_layer={}, n_embd={}, n_head={}\".format(\n",
        "    config.n_layer, config.n_embd, config.n_head\n",
        "))\n",
        "\n",
        "# 5Ô∏è‚É£ Load the full model with sharding and optional off‚Äëload\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # automatically shard across GPUs\n",
        "    offload_folder=\"/tmp/torch_offload\",  # optional: offload to disk if needed\n",
        "    torch_dtype=torch.float16,   # mixed‚Äëprecision to save memory\n",
        ")\n",
        "\n",
        "print(\"\\nModel loaded. Checking device placement:\")\n",
        "# Show a summary of where each layer lives\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name:60s} -> {param.device}\")\n",
        "\n",
        "# 6Ô∏è‚É£ Quick forward pass to confirm everything works\n",
        "sample_input = torch.tensor([[50256]])  # BOS token id\n",
        "with torch.no_grad():\n",
        "    outputs = model(sample_input)\n",
        "print(\"\\nForward pass successful. Output shape:\", outputs.logits.shape)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3Ô∏è‚É£7Ô∏è‚É£ Common pitfalls\n",
        "\n",
        "- **GPU memory exhausted** ‚Äì If you still hit OOM errors, try setting `device_map={\"transformer.h.0\": \"cpu\"}` to off‚Äëload the first few layers, or increase the `offload_folder` size.\n",
        "- **Slow disk I/O** ‚Äì Off‚Äëloading can be slow on HDDs. Use an SSD or a RAM disk for best performance.\n",
        "- **Mixed‚Äëprecision errors** ‚Äì Some models may produce NaNs when using `float16`. Switch to `torch.float32` if you encounter this.\n",
        "\n",
        "### 3Ô∏è‚É£8Ô∏è‚É£ Next step\n",
        "\n",
        "In Step‚ÄØ4 we‚Äôll dive deeper into GPU and memory management, showing how to fine‚Äëtune the model with LoRA adapters while keeping VRAM usage under control.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: GPU and Memory Management\n",
        "\n",
        "When you load a 20‚Äëbillion‚Äëparameter model, the GPU becomes the model‚Äôs *living room*. If you don‚Äôt manage the space, the room will overflow and the model will crash. Think of the GPU as a tiny apartment that can hold only a few heavy books (tensors). The trick is to keep the books organized, move some to a storage unit (disk), and sometimes use lighter versions of the books (mixed‚Äëprecision) so the apartment stays comfortable.\n",
        "\n",
        "### 1Ô∏è‚É£ Why GPU memory matters\n",
        "\n",
        "- **VRAM is limited** ‚Äì a single 24‚ÄØGB GPU can‚Äôt hold all 80‚ÄØGB of weights at once.\n",
        "- **Large tensors consume memory** ‚Äì each forward pass allocates new tensors that sit on the GPU until they‚Äôre freed.\n",
        "- **Memory fragmentation** ‚Äì repeated allocations can leave gaps that the GPU can‚Äôt use efficiently.\n",
        "\n",
        "### 2Ô∏è‚É£ Key tools for memory hygiene\n",
        "\n",
        "| Tool | What it does | Analogy |\n",
        "|------|--------------|---------|\n",
        "| `torch.cuda.memory_summary()` | Prints a concise report of allocated, reserved, and free memory. | A quick snapshot of how full your apartment is. |\n",
        "| `torch.cuda.set_per_process_memory_fraction()` | Caps the fraction of GPU memory a process can claim. | A budget that prevents overspending on furniture. |\n",
        "| `torch.cuda.empty_cache()` | Frees unused memory back to the GPU pool. | Clearing out the trash bin after a party. |\n",
        "| `torch.backends.cudnn.deterministic` | Forces deterministic convolution algorithms. | Choosing a single, predictable recipe. |\n",
        "| `torch.backends.cudnn.benchmark` | Enables auto‚Äëtuning for speed (may be nondeterministic). | Letting the kitchen auto‚Äëadjust heat for fastest cooking. |\n",
        "| `torch.backends.cuda.matmul.allow_tf32` | Enables TensorFloat‚Äë32 for faster matrix ops on Ampere+ GPUs. | Using a lighter, faster version of the same ingredient. |\n",
        "| `torch.autocast` | Mixed‚Äëprecision context manager (float16/float32). | Using lightweight paper instead of heavy cardboard for temporary storage. |\n",
        "| `torch.compile` | Compiles a model for speed (PyTorch 2.0+). | Pre‚Äëassembling furniture to save assembly time. |\n",
        "\n",
        "### 3Ô∏è‚É£ Trade‚Äëoffs to keep in mind\n",
        "\n",
        "- **Determinism vs. Speed** ‚Äì Setting `cudnn.deterministic=True` guarantees reproducible results but can slow down training.\n",
        "- **Mixed‚Äëprecision vs. Accuracy** ‚Äì `float16` saves memory and speeds up inference, but may introduce small numerical errors.\n",
        "- **Off‚Äëloading vs. I/O latency** ‚Äì Storing heavy layers on disk frees VRAM but can slow down training if the disk is slow.\n",
        "- **Memory cap vs. OOM risk** ‚Äì Limiting memory usage protects against crashes, but setting the cap too low may cause out‚Äëof‚Äëmemory errors during large batches.\n",
        "- **Compilation vs. Overhead** ‚Äì `torch.compile` can accelerate models, but the compilation step may temporarily increase memory usage.\n",
        "\n",
        "### 4Ô∏è‚É£ Practical checklist before you start training\n",
        "\n",
        "1. **Verify GPU availability** ‚Äì `torch.cuda.is_available()`.\n",
        "2. **Check VRAM** ‚Äì `torch.cuda.get_device_properties(0).total_memory`.\n",
        "3. **Set a memory cap** ‚Äì `torch.cuda.set_per_process_memory_fraction(0.9)`.\n",
        "4. **Enable mixed‚Äëprecision** ‚Äì `torch.autocast` or `torch.compile`.\n",
        "5. **Monitor memory** ‚Äì `torch.cuda.memory_summary()` before and after key operations.\n",
        "6. **Clean up** ‚Äì `torch.cuda.empty_cache()` after large tensors are no longer needed.\n",
        "\n",
        "By following these steps, you‚Äôll keep your GPU from bursting at the seams and ensure that fine‚Äëtuning GPT‚ÄëOss‚Äë20B runs smoothly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Import torch and set deterministic behavior for reproducibility\n",
        "import torch\n",
        "\n",
        "# Deterministic convolutions (slower but reproducible)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# Disable auto‚Äëtuning to keep results stable across runs\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 2Ô∏è‚É£ Enable TensorFloat‚Äë32 for faster matmul on Ampere+ GPUs (optional)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# 3Ô∏è‚É£ Limit per‚Äëprocess GPU memory to 90% of the device (adjust if you hit OOM)\n",
        "torch.cuda.set_per_process_memory_fraction(0.9, device=0)\n",
        "\n",
        "# 4Ô∏è‚É£ Show current memory status\n",
        "print(\"Initial memory summary:\")\n",
        "print(torch.cuda.memory_summary(device=0, abbreviated=True))\n",
        "\n",
        "# 5Ô∏è‚É£ Dummy model to illustrate memory usage\n",
        "class DummyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(1024, 1024).cuda()\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = DummyModel()\n",
        "\n",
        "# 6Ô∏è‚É£ Mixed‚Äëprecision inference with autocast\n",
        "x = torch.randn(8, 1024).cuda()\n",
        "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "    out = model(x)\n",
        "\n",
        "print(\"\\nAfter forward pass:\")\n",
        "print(torch.cuda.memory_summary(device=0, abbreviated=True))\n",
        "\n",
        "# 7Ô∏è‚É£ Optional: compile the model for speed (PyTorch 2.0+)\n",
        "if hasattr(torch, \"compile\"):\n",
        "    compiled = torch.compile(model)\n",
        "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "        out = compiled(x)\n",
        "    print(\"\\nCompiled model inference done.\")\n",
        "\n",
        "# 8Ô∏è‚É£ Clean up to free memory for the next cell\n",
        "torch.cuda.empty_cache()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Fine‚ÄëTuning with LoRA Adapters\n",
        "\n",
        "Fine‚Äëtuning a 20‚Äëbillion‚Äëparameter model on a single GPU is like trying to teach a giant library to write a new style of story. The library is too big to move around, so we add a *lightweight* set of bookmarks (the LoRA adapters) that only tweak a tiny fraction of the books. These bookmarks are small matrices that sit on top of the original weights and learn the new style while the rest of the library stays untouched.\n",
        "\n",
        "### Why LoRA?  A quick analogy\n",
        "\n",
        "Imagine you have a massive LEGO set (the base model). Building a new structure from scratch would mean buying a whole new set. LoRA is like giving you a handful of extra LEGO bricks that you can snap onto the existing set to change its shape. You don‚Äôt need to rebuild the entire set; you just add a few pieces.\n",
        "\n",
        "### Key terms and trade‚Äëoffs\n",
        "\n",
        "| Term | What it means | Trade‚Äëoff | Rationale |\n",
        "|------|----------------|-----------|-----------|\n",
        "| **LoRA (Low‚ÄëRank Adaptation)** | Adds two small trainable matrices (A and B) to each attention and MLP layer. | **Fewer trainable params** (‚âà0.1‚ÄØ% of the base model) | Saves GPU memory and speeds up training while preserving most of the original knowledge. |\n",
        "| **Adapter rank (r)** | Size of the hidden dimension in the LoRA matrices. | **Higher r ‚Üí more capacity** but **more memory** | Choose r based on dataset size and GPU limits. |\n",
        "| **Alpha (Œ±)** | Scaling factor applied to the LoRA update. | **Higher Œ± ‚Üí stronger updates** but can destabilize training | Helps balance learning speed and stability. |\n",
        "| **Gradient checkpointing** | Recomputes intermediate activations during back‚Äëprop to save memory. | **More compute** but **less VRAM** | Essential when training large models with limited GPU memory. |\n",
        "| **Mixed‚Äëprecision (float16/float32)** | Uses lower‚Äëprecision arithmetic for speed and memory. | **Potential numerical instability** | Combined with LoRA, it keeps memory low while maintaining accuracy. |\n",
        "\n",
        "### What we‚Äôll do in this section\n",
        "\n",
        "1. **Load the base GPT‚ÄëOss‚Äë20B model** and tokenizer.\n",
        "2. **Wrap the model with LoRA adapters** using the `peft` library.\n",
        "3. **Prepare a tiny synthetic dataset** (you can replace it with your own). \n",
        "4. **Set up a minimal training loop** that trains only the LoRA weights.\n",
        "5. **Show how to save and reload the fine‚Äëtuned adapters**.\n",
        "\n",
        "All code cells are kept under 30 lines and include comments for clarity. We also set a deterministic seed and pin library versions for reproducibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£  Imports and reproducibility\n",
        "import os, random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "\n",
        "# Pin versions for reproducibility\n",
        "assert torch.__version__.startswith(\"2.\")\n",
        "\n",
        "# Set a fixed random seed\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2Ô∏è‚É£  Load tokenizer and base model (weights are on GPU via device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "assert HF_TOKEN is not None, \"HF_TOKEN not set\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # shard across GPUs\n",
        "    torch_dtype=torch.float16,   # mixed‚Äëprecision\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£  Define LoRA configuration\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8,                # rank of the low‚Äërank matrices\n",
        "    lora_alpha=32,      # scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"fc1\", \"fc2\"],  # attention & MLP layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# 4Ô∏è‚É£  Wrap the model with LoRA adapters\n",
        "model = get_peft_model(base_model, lora_cfg)\n",
        "print(\"LoRA adapters added. Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "# 5Ô∏è‚É£  Create a tiny synthetic dataset (replace with real data)\n",
        "sample_texts = [\n",
        "    \"Hello world! This is a test sentence.\",\n",
        "    \"Fine‚Äëtuning GPT‚ÄëOss with LoRA is efficient.\",\n",
        "    \"We can add more data later.\",\n",
        "]\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": sample_texts})\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"]).with_format(\"torch\")\n",
        "\n",
        "# 6Ô∏è‚É£  Prepare DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# 7Ô∏è‚É£  Optimizer only for LoRA params\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# 8Ô∏è‚É£  Minimal training loop (2 epochs)\n",
        "model.train()\n",
        "for epoch in range(2):\n",
        "    for batch in loader:\n",
        "        inputs = {k: v.cuda() for k, v in batch.items() if k != \"idx\"}\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch+1} finished. Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 9Ô∏è‚É£  Save only the LoRA adapters\n",
        "model.save_pretrained(\"./gpt-oss-20b-lora\")\n",
        "print(\"LoRA adapters saved to ./gpt-oss-20b-lora\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the fine‚Äëtuned adapters later\n",
        "\n",
        "```python\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the base model again (weights stay on disk)\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Attach the saved LoRA weights\n",
        "model = PeftModel.from_pretrained(base, \"./gpt-oss-20b-lora\")\n",
        "\n",
        "# Now you can generate text or continue training\n",
        "```\n",
        "\n",
        "This lightweight approach lets you keep the massive 20‚ÄëB backbone on disk and only load the tiny LoRA weights into GPU memory when needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Preparing Dataset and Tokenizer\n",
        "\n",
        "Before we can teach GPT‚ÄëOss‚Äë20B a new style, we need to give it a *menu* of sentences to read. Think of the dataset as a cookbook: each recipe (text example) is a small paragraph that the model will learn from. The tokenizer is the chef‚Äôs knife that chops each recipe into bite‚Äësized *tokens* (words, sub‚Äëwords, or punctuation) so the model can understand them.\n",
        "\n",
        "### Why this step matters\n",
        "\n",
        "1. **Tokenization is the language of the model** ‚Äì GPT‚ÄëOss expects integer IDs, not raw text. If the tokenizer is wrong, the model will read gibberish.\n",
        "2. **Padding and truncation keep the batch shape consistent** ‚Äì GPUs love tensors that are all the same size. Padding adds a special *pad* token where needed, while truncation cuts off overly long sentences.\n",
        "3. **Dataset split gives us a validation set** ‚Äì we can monitor over‚Äëfitting and decide when to stop training.\n",
        "4. **Collation bundles the data into a format the model understands** ‚Äì the `DataCollatorForLanguageModeling` automatically creates the `labels` field that the loss function uses.\n",
        "\n",
        "### Key terms and trade‚Äëoffs\n",
        "\n",
        "| Term | What it means | Trade‚Äëoff | Rationale |\n",
        "|------|----------------|-----------|-----------|\n",
        "| **Dataset** | A collection of text examples (e.g., Wikipedia articles). | Larger datasets improve generalization but increase storage and preprocessing time. | Use a realistic corpus that matches your target domain. |\n",
        "| **Tokenizer** | Converts text to integer IDs. | Fast tokenizers (e.g., `GPT2TokenizerFast`) use a pre‚Äëcompiled C++ backend, but may have a larger vocabulary. | Choose a tokenizer that matches the model‚Äôs training tokenizer for best compatibility. |\n",
        "| **Padding** | Adds a special token to make all sequences the same length. | Padding increases memory usage but simplifies batching. | Use `padding='max_length'` for deterministic batch shapes. |\n",
        "| **Truncation** | Cuts sequences longer than `max_length`. | Truncation can lose context but keeps memory usage bounded. | Set `max_length` to a value that balances context length and GPU memory. |\n",
        "| **Batch size** | Number of examples processed together. | Larger batches improve GPU utilization but require more VRAM. | Start with a small batch and scale up if memory allows. |\n",
        "| **DataCollator** | Helper that prepares the final input tensors for the model. | Custom collators can add special handling (e.g., causal masking). | Use `DataCollatorForLanguageModeling` for standard causal LM training. |\n",
        "\n",
        "### Practical checklist\n",
        "\n",
        "1. **Set a deterministic seed** ‚Äì ensures reproducible shuffling and tokenization.\n",
        "2. **Verify the tokenizer matches the model** ‚Äì use `AutoTokenizer.from_pretrained(\"gpt-oss-20b\")`.\n",
        "3. **Choose a dataset** ‚Äì for demo we‚Äôll use the `wikitext-2-raw-v1` split; replace it with your own data later.\n",
        "4. **Tokenize with `batched=True`** ‚Äì speeds up preprocessing.\n",
        "5. **Split into train/validation** ‚Äì `train_test_split(test_size=0.1)`.\n",
        "6. **Create a DataCollator** ‚Äì handles padding and label creation.\n",
        "7. **Wrap in a DataLoader** ‚Äì set `shuffle=True` for training.\n",
        "\n",
        "With this foundation, the next step (Step‚ÄØ7) will show how to feed these batches into the LoRA‚Äëaugmented GPT‚ÄëOss‚Äë20B and monitor training progress.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£  Imports and reproducibility\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Pin library versions for reproducibility\n",
        "assert torch.__version__.startswith(\"2.\")\n",
        "\n",
        "# Set a fixed random seed for deterministic shuffling & tokenization\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2Ô∏è‚É£  Load the tokenizer that matches GPT‚ÄëOss‚Äë20B\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "assert HF_TOKEN is not None, \"HF_TOKEN not set\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "# Ensure the tokenizer has a pad token (GPT‚Äë2 style models use eos as pad)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 3Ô∏è‚É£  Load a small public dataset for demonstration\n",
        "# Replace this with your own data for real fine‚Äëtuning\n",
        "raw_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 4Ô∏è‚É£  Tokenize the dataset in batches\n",
        "max_length = 128  # keep sequences short to fit GPU memory\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "tokenized_ds = raw_ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])  # <30 lines\n",
        "\n",
        "# 5Ô∏è‚É£  Split into train/validation\n",
        "train_ds, val_ds = tokenized_ds.train_test_split(test_size=0.1, seed=SEED).values()\n",
        "\n",
        "# 6Ô∏è‚É£  Create a DataCollator that adds the labels field\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# 7Ô∏è‚É£  Prepare DataLoaders for training and validation\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=4,  # adjust based on GPU memory\n",
        "    shuffle=True,\n",
        "    collate_fn=collator,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=collator,\n",
        ")\n",
        "\n",
        "print(\"Dataset ready: \", len(train_ds), \"train examples,\", len(val_ds), \"validation examples\")\n",
        "print(\"Batch shape example:\")\n",
        "for batch in train_loader:\n",
        "    print(batch[\"input_ids\"].shape, batch[\"labels\"].shape)\n",
        "    break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 8Ô∏è‚É£  Quick sanity check: run a forward pass on a batch\n",
        "# (Assumes you have a LoRA‚Äëwrapped model from Step‚ÄØ5 loaded as `model`)\n",
        "# If you don‚Äôt have a model yet, this cell will just demonstrate the data pipeline.\n",
        "\n",
        "# Uncomment the following lines if you have a model ready:\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     for batch in train_loader:\n",
        "#         inputs = {k: v.cuda() for k, v in batch.items() if k != \"idx\"}\n",
        "#         outputs = model(**inputs)\n",
        "#         print(\"Loss:\", outputs.loss.item())\n",
        "#         break\n",
        "\n",
        "print(\"Data pipeline is functional. Ready for training in Step‚ÄØ7.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>‚ö†Ô∏è Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>‚úÖ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>‚ùå Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which library provides the LoRA implementation used in this notebook?\", [\"transformers\",\"peft\",\"datasets\",\"accelerate\"], 1, \"The peft library supplies lightweight adapters such as LoRA for efficient fine‚Äëtuning of large language models.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary benefit of using LoRA adapters?\", [\"Faster inference\",\"Lower memory usage\",\"Higher accuracy\",\"None of the above\"], 1, \"LoRA reduces the number of trainable parameters, thereby lowering memory consumption while maintaining performance.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:15:47.061Z",
      "title": "Deploying and Fine‚ÄëTuning GPT‚ÄëOss‚Äë20B in Jupyter: A Practitioner‚Äôs Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:15:47.070Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36‚Äì60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}