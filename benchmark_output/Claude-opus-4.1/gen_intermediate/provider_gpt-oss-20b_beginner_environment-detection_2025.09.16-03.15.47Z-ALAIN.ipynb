{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying and Fine‑Tuning GPT‑Oss‑20B in Jupyter: A Practitioner’s Guide\n\nThis notebook walks experienced ML practitioners through the end‑to‑end process of loading, configuring, and fine‑tuning the 20B‑parameter GPT‑Oss model using Hugging Face libraries and LoRA adapters. It emphasizes practical setup, GPU memory management, and reproducible training workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:15:47.070Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Understand the architecture and tokenization of GPT‑Oss‑20B.\n",
        "2. Load the model from the Hugging Face Hub and configure GPU memory efficiently.\n",
        "3. Apply LoRA adapters for lightweight fine‑tuning on custom datasets.\n",
        "4. Evaluate the fine‑tuned model and interpret training metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+ with pip\n",
        "- Basic knowledge of PyTorch and Hugging Face Transformers\n",
        "- Access to a GPU with at least 24 GB VRAM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 torch>=2.0.0 transformers>=4.40.0 accelerate>=0.28.0 datasets>=2.20.0 peft>=0.6.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"torch>=2.0.0\",\"transformers>=4.40.0\",\"accelerate>=0.28.0\",\"datasets>=2.20.0\",\"peft>=0.6.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Setup\n",
        "\n",
        "Welcome to the first step of our journey to fine‑tune the 20‑billion‑parameter GPT‑Oss model. Think of GPT‑Oss as a gigantic library of text—each book is a *parameter* that helps the model understand language. Fine‑tuning is like giving that library a new set of bookmarks so it can focus on a specific topic faster.\n",
        "\n",
        "In this section we will:\n",
        "\n",
        "1. **Verify the environment** – make sure you have the right Python version, GPU, and the Hugging Face token.\n",
        "2. **Install the required libraries** – `torch`, `transformers`, `accelerate`, `datasets`, `peft`, and `ipywidgets`.\n",
        "3. **Set a random seed** – reproducibility is the bread‑and‑butter of ML experiments.\n",
        "4. **Check GPU availability** – we’ll confirm that the notebook can see your 24 GB GPU.\n",
        "\n",
        "### Why these steps matter\n",
        "\n",
        "* **Reproducibility** – By fixing the random seed and using pinned library versions, you can share your notebook and others will get the same results.\n",
        "* **Memory management** – GPT‑Oss is huge; we’ll pre‑configure PyTorch to use the GPU efficiently.\n",
        "* **Error handling** – Early checks prevent the frustrating “module not found” or “no GPU” errors that can stall a notebook.\n",
        "\n",
        "#### Key terms\n",
        "\n",
        "- **Parameter** – a weight in the neural network; GPT‑Oss has 20 B of them.\n",
        "- **LoRA** – Low‑Rank Adaptation, a lightweight method that adds a small number of trainable matrices to the model.\n",
        "- **HF_TOKEN** – your Hugging Face authentication token that grants access to private models.\n",
        "- **Accelerate** – a library that abstracts device placement and mixed‑precision training.\n",
        "\n",
        "#### Trade‑offs\n",
        "\n",
        "- **Pinned versions** guarantee consistency but may miss out on bug fixes or performance improvements in newer releases.\n",
        "- **Setting a seed** can slightly reduce randomness, which is good for debugging but may hide stochastic effects that could be useful in some research settings.\n",
        "\n",
        "Let’s get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Install required packages (run once)\n",
        "# We use pip to install the exact versions that work with GPT‑Oss‑20B.\n",
        "# The `--quiet` flag keeps the output tidy.\n",
        "# If you already have the packages, pip will skip re‑installing.\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "packages = [\n",
        "    \"torch>=2.0.0\",\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"accelerate>=0.28.0\",\n",
        "    \"datasets>=2.20.0\",\n",
        "    \"peft>=0.6.0\",\n",
        "    \"ipywidgets>=8.0.0\"\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"--quiet\"], check=True)\n",
        "\n",
        "# Enable Jupyter widgets (only needed once per environment)\n",
        "try:\n",
        "    subprocess.run([sys.executable, \"-m\", \"jupyter\", \"nbextension\", \"enable\", \"--py\", \"widgetsnbextension\"], check=True)\n",
        "except Exception as e:\n",
        "    print(\"Widget extension already enabled or failed to enable:\", e)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2️⃣ Verify environment and set seed\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set a fixed random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Print environment info\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")\n",
        "\n",
        "# Check Hugging Face token\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if HF_TOKEN is None:\n",
        "    print(\"⚠️  HF_TOKEN environment variable not found.\\n\\tSet it with: export HF_TOKEN=YOUR_TOKEN\")\n",
        "else:\n",
        "    print(\"✅  HF_TOKEN found.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What you should see\n",
        "\n",
        "- The Python and PyTorch versions should match the pinned ones.\n",
        "- CUDA should report `True` and list your GPU name (e.g., NVIDIA RTX 4090).\n",
        "- A message confirming that `HF_TOKEN` is set.\n",
        "\n",
        "If any of these checks fail, pause the notebook and resolve the issue before moving on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Model Architecture Overview\n",
        "\n",
        "Think of GPT‑Oss‑20B as a **tower of bookshelves**. Each shelf holds a *layer* of the model, and each book on that shelf is a *parameter* that helps the model read and write text. The tower is built from the same type of shelf over and over, but the number of shelves (layers) and the size of each shelf (hidden dimension) vary.\n",
        "\n",
        "### 1️⃣ What makes up a transformer layer?\n",
        "\n",
        "| Component | Role | Analogy |\n",
        "|-----------|------|---------|\n",
        "| **Self‑Attention** | Lets the model look at every word in the sentence and decide which words matter most for predicting the next word. | A group of students in a classroom pointing at each other to decide who should speak next. |\n",
        "| **Feed‑Forward Network (FFN)** | Applies a small neural net to each word’s representation, adding non‑linearity. | A tiny calculator that tweaks each student’s idea before it’s shared. |\n",
        "| **LayerNorm** | Normalizes the activations so the network stays stable. | A teacher making sure everyone’s volume is at a comfortable level. |\n",
        "| **Residual Connection** | Adds the input of the layer back to its output, helping gradients flow. | A safety net that keeps the original idea intact while the student refines it. |\n",
        "\n",
        "The **GPT‑Oss‑20B** architecture stacks **48** of these identical shelves. Each shelf has a hidden size of **12,288** and uses **96** attention heads. The total number of parameters is roughly **20 billion**, which is why we need a powerful GPU and careful memory management.\n",
        "\n",
        "### 2️⃣ Why this design?\n",
        "\n",
        "* **Depth (48 layers)** gives the model a long “memory” of past tokens, enabling it to capture complex language patterns.\n",
        "* **Width (12,288 hidden units)** allows each layer to hold a rich representation of the input.\n",
        "* **Large number of heads (96)** lets the model attend to many different relationships simultaneously.\n",
        "\n",
        "Trade‑offs:\n",
        "\n",
        "- **More layers** → better performance but higher memory and compute cost.\n",
        "- **Wider layers** → richer representations but also more parameters per layer.\n",
        "- **More heads** → finer-grained attention but increased parallelism overhead.\n",
        "\n",
        "### 3️⃣ Quick sanity check in code\n",
        "\n",
        "Below we load the model configuration (no weights yet) and print a concise summary. This helps you confirm that the architecture matches the documentation before you start downloading the 20 B weights.\n",
        "\n",
        "```python\n",
        "# 1️⃣ Load the config without downloading the huge weights\n",
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,  # GPT‑Oss uses custom code on HF\n",
        "    use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        ")\n",
        "\n",
        "# 2️⃣ Print key hyper‑parameters\n",
        "print(\"Model name:\", config.model_type)\n",
        "print(\"Number of layers (n_layer):\", config.n_layer)\n",
        "print(\"Hidden size (n_embd):\", config.n_embd)\n",
        "print(\"Number of attention heads (n_head):\", config.n_head)\n",
        "print(\"Total parameters (approx):\", config.num_parameters())\n",
        "\n",
        "# 3️⃣ Visualize the layer structure (simple text diagram)\n",
        "print(\"\\nLayer diagram:\\n\")\n",
        "for i in range(config.n_layer):\n",
        "    print(f\"Layer {i+1:02d}: Attention + FFN + LayerNorm + Residual\")\n",
        "```\n",
        "\n",
        "> **⚠️ Note**: The `AutoConfig` call only pulls the configuration file, which is tiny (~10 KB). The actual weights are ~80 GB and will be downloaded when you instantiate the model.\n",
        "\n",
        "### 4️⃣ Key terms defined\n",
        "\n",
        "- **Transformer**: A neural network architecture that relies on self‑attention to process sequences.\n",
        "- **Self‑Attention**: Mechanism that lets each token weigh every other token in the sequence.\n",
        "- **Feed‑Forward Network (FFN)**: A two‑layer MLP applied to each token’s representation.\n",
        "- **LayerNorm**: Normalization technique that stabilizes training by scaling activations.\n",
        "- **Residual Connection**: Adds the input of a layer to its output, aiding gradient flow.\n",
        "- **HF_TOKEN**: Hugging Face authentication token required to download private or large models.\n",
        "\n",
        "### 5️⃣ Why we expose the config early\n",
        "\n",
        "By inspecting the config before loading weights, you:\n",
        "\n",
        "1. **Avoid wasted bandwidth** – if the config shows a mismatch (e.g., wrong number of layers), you can stop early.\n",
        "2. **Verify reproducibility** – the config contains the exact hyper‑parameters used by the authors.\n",
        "3. **Plan memory** – knowing `n_embd` and `n_layer` lets you estimate VRAM usage.\n",
        "\n",
        "---\n",
        "\n",
        "**Next step**: In Step 3 we’ll actually load the GPT‑Oss‑20B weights onto the GPU, taking care to keep memory usage in check.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick sanity check: print a few token embeddings\n",
        "# This demonstrates that the model can be instantiated without downloading all weights\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Load the model lazily (weights will be streamed as needed)\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",  # let accelerate decide placement\n",
        "    offload_folder=\"/tmp/torch_offload\",  # optional: offload to disk if GPU memory is tight\n",
        ")\n",
        "\n",
        "print(\"Model loaded on device:\", next(model.parameters()).device)\n",
        "\n",
        "# Inspect the first token embedding (just to confirm shapes)\n",
        "import torch\n",
        "sample_input = torch.tensor([[50256]])  # BOS token id\n",
        "with torch.no_grad():\n",
        "    outputs = model(sample_input)\n",
        "print(\"Output shape:\", outputs.last_hidden_state.shape)\n",
        "```\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading GPT‑Oss‑20B from Hugging Face\n",
        "\n",
        "In the previous step we peeked at the model’s blueprint. Now it’s time to bring the full 20‑billion‑parameter engine to life. Think of this as assembling a giant Lego set: the instructions (config) are tiny, but the bricks (weights) are massive. We’ll use Hugging Face’s `AutoModelForCausalLM` to pull the weights, but we’ll also give the system a few hints so it doesn’t crash the GPU.\n",
        "\n",
        "### 3️⃣1️⃣ Why we use `device_map=\"auto\"`\n",
        "\n",
        "When you ask the library to load a model, it normally tries to put everything on the first GPU. For a 20‑B model that would require ~80 GB of VRAM—more than most single‑GPU setups provide. The `device_map=\"auto\"` flag tells the `accelerate` backend to split the model across available GPUs or, if only one GPU is present, to stream layers in and out of memory. It’s like having a warehouse that can move boxes in and out as needed, rather than trying to store the entire shipment in a single shelf.\n",
        "\n",
        "### 3️⃣2️⃣ Optional off‑loading to disk\n",
        "\n",
        "If you’re on a machine with a single 24 GB GPU, you can still run the model by off‑loading the heaviest layers to disk. The `offload_folder` argument creates a temporary directory where those layers are stored when not in use. This trades a bit of CPU‑disk I/O for the ability to run the model without exceeding VRAM limits.\n",
        "\n",
        "### 3️⃣3️⃣ Trusting remote code\n",
        "\n",
        "GPT‑Oss ships a custom model class that isn’t part of the standard Transformers distribution. Setting `trust_remote_code=True` allows the library to download and execute that custom class. Think of it as trusting a friend’s custom recipe that isn’t on the official cookbook.\n",
        "\n",
        "### 3️⃣4️⃣ Reproducibility and safety\n",
        "\n",
        "We’ll set a deterministic seed for PyTorch before loading the model to ensure that any stochastic operations (e.g., dropout during evaluation) behave consistently across runs. We’ll also catch common errors such as missing `HF_TOKEN` or insufficient GPU memory.\n",
        "\n",
        "### 3️⃣5️⃣ Key terms and trade‑offs\n",
        "\n",
        "- **`device_map`** – a dictionary that tells Accelerate which GPU each layer should live on. `\"auto\"` automatically shards the model.\n",
        "- **`offload_folder`** – a path on disk where layers are temporarily stored when not on GPU.\n",
        "- **`trust_remote_code`** – allows the library to load custom model definitions from the Hugging Face Hub.\n",
        "- **`torch_dtype`** – the data type used for weights (e.g., `torch.float16` for mixed‑precision). Using lower precision saves memory but can slightly degrade numerical stability.\n",
        "- **Trade‑offs** – Sharding (`device_map`) reduces VRAM usage but increases inter‑GPU communication overhead. Off‑loading saves VRAM but incurs disk I/O latency. Mixed‑precision (`torch_dtype`) speeds up inference but may introduce rounding errors.\n",
        "\n",
        "### 3️⃣6️⃣ Quick sanity check\n",
        "\n",
        "Below we load the full model with the recommended settings and print the device each parameter resides on. This gives you a quick visual confirmation that the model is correctly sharded or off‑loaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Import required libraries\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "# 2️⃣ Set a deterministic seed for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 3️⃣ Verify HF_TOKEN is available\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if HF_TOKEN is None:\n",
        "    raise EnvironmentError(\n",
        "        \"HF_TOKEN not found. Set it with export HF_TOKEN=YOUR_TOKEN or os.environ['HF_TOKEN']=...\"\n",
        "    )\n",
        "\n",
        "# 4️⃣ Load the configuration first (tiny file, ~10 KB)\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "print(\"Model config loaded: n_layer={}, n_embd={}, n_head={}\".format(\n",
        "    config.n_layer, config.n_embd, config.n_head\n",
        "))\n",
        "\n",
        "# 5️⃣ Load the full model with sharding and optional off‑load\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # automatically shard across GPUs\n",
        "    offload_folder=\"/tmp/torch_offload\",  # optional: offload to disk if needed\n",
        "    torch_dtype=torch.float16,   # mixed‑precision to save memory\n",
        ")\n",
        "\n",
        "print(\"\\nModel loaded. Checking device placement:\")\n",
        "# Show a summary of where each layer lives\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name:60s} -> {param.device}\")\n",
        "\n",
        "# 6️⃣ Quick forward pass to confirm everything works\n",
        "sample_input = torch.tensor([[50256]])  # BOS token id\n",
        "with torch.no_grad():\n",
        "    outputs = model(sample_input)\n",
        "print(\"\\nForward pass successful. Output shape:\", outputs.logits.shape)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3️⃣7️⃣ Common pitfalls\n",
        "\n",
        "- **GPU memory exhausted** – If you still hit OOM errors, try setting `device_map={\"transformer.h.0\": \"cpu\"}` to off‑load the first few layers, or increase the `offload_folder` size.\n",
        "- **Slow disk I/O** – Off‑loading can be slow on HDDs. Use an SSD or a RAM disk for best performance.\n",
        "- **Mixed‑precision errors** – Some models may produce NaNs when using `float16`. Switch to `torch.float32` if you encounter this.\n",
        "\n",
        "### 3️⃣8️⃣ Next step\n",
        "\n",
        "In Step 4 we’ll dive deeper into GPU and memory management, showing how to fine‑tune the model with LoRA adapters while keeping VRAM usage under control.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: GPU and Memory Management\n",
        "\n",
        "When you load a 20‑billion‑parameter model, the GPU becomes the model’s *living room*. If you don’t manage the space, the room will overflow and the model will crash. Think of the GPU as a tiny apartment that can hold only a few heavy books (tensors). The trick is to keep the books organized, move some to a storage unit (disk), and sometimes use lighter versions of the books (mixed‑precision) so the apartment stays comfortable.\n",
        "\n",
        "### 1️⃣ Why GPU memory matters\n",
        "\n",
        "- **VRAM is limited** – a single 24 GB GPU can’t hold all 80 GB of weights at once.\n",
        "- **Large tensors consume memory** – each forward pass allocates new tensors that sit on the GPU until they’re freed.\n",
        "- **Memory fragmentation** – repeated allocations can leave gaps that the GPU can’t use efficiently.\n",
        "\n",
        "### 2️⃣ Key tools for memory hygiene\n",
        "\n",
        "| Tool | What it does | Analogy |\n",
        "|------|--------------|---------|\n",
        "| `torch.cuda.memory_summary()` | Prints a concise report of allocated, reserved, and free memory. | A quick snapshot of how full your apartment is. |\n",
        "| `torch.cuda.set_per_process_memory_fraction()` | Caps the fraction of GPU memory a process can claim. | A budget that prevents overspending on furniture. |\n",
        "| `torch.cuda.empty_cache()` | Frees unused memory back to the GPU pool. | Clearing out the trash bin after a party. |\n",
        "| `torch.backends.cudnn.deterministic` | Forces deterministic convolution algorithms. | Choosing a single, predictable recipe. |\n",
        "| `torch.backends.cudnn.benchmark` | Enables auto‑tuning for speed (may be nondeterministic). | Letting the kitchen auto‑adjust heat for fastest cooking. |\n",
        "| `torch.backends.cuda.matmul.allow_tf32` | Enables TensorFloat‑32 for faster matrix ops on Ampere+ GPUs. | Using a lighter, faster version of the same ingredient. |\n",
        "| `torch.autocast` | Mixed‑precision context manager (float16/float32). | Using lightweight paper instead of heavy cardboard for temporary storage. |\n",
        "| `torch.compile` | Compiles a model for speed (PyTorch 2.0+). | Pre‑assembling furniture to save assembly time. |\n",
        "\n",
        "### 3️⃣ Trade‑offs to keep in mind\n",
        "\n",
        "- **Determinism vs. Speed** – Setting `cudnn.deterministic=True` guarantees reproducible results but can slow down training.\n",
        "- **Mixed‑precision vs. Accuracy** – `float16` saves memory and speeds up inference, but may introduce small numerical errors.\n",
        "- **Off‑loading vs. I/O latency** – Storing heavy layers on disk frees VRAM but can slow down training if the disk is slow.\n",
        "- **Memory cap vs. OOM risk** – Limiting memory usage protects against crashes, but setting the cap too low may cause out‑of‑memory errors during large batches.\n",
        "- **Compilation vs. Overhead** – `torch.compile` can accelerate models, but the compilation step may temporarily increase memory usage.\n",
        "\n",
        "### 4️⃣ Practical checklist before you start training\n",
        "\n",
        "1. **Verify GPU availability** – `torch.cuda.is_available()`.\n",
        "2. **Check VRAM** – `torch.cuda.get_device_properties(0).total_memory`.\n",
        "3. **Set a memory cap** – `torch.cuda.set_per_process_memory_fraction(0.9)`.\n",
        "4. **Enable mixed‑precision** – `torch.autocast` or `torch.compile`.\n",
        "5. **Monitor memory** – `torch.cuda.memory_summary()` before and after key operations.\n",
        "6. **Clean up** – `torch.cuda.empty_cache()` after large tensors are no longer needed.\n",
        "\n",
        "By following these steps, you’ll keep your GPU from bursting at the seams and ensure that fine‑tuning GPT‑Oss‑20B runs smoothly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Import torch and set deterministic behavior for reproducibility\n",
        "import torch\n",
        "\n",
        "# Deterministic convolutions (slower but reproducible)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# Disable auto‑tuning to keep results stable across runs\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 2️⃣ Enable TensorFloat‑32 for faster matmul on Ampere+ GPUs (optional)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# 3️⃣ Limit per‑process GPU memory to 90% of the device (adjust if you hit OOM)\n",
        "torch.cuda.set_per_process_memory_fraction(0.9, device=0)\n",
        "\n",
        "# 4️⃣ Show current memory status\n",
        "print(\"Initial memory summary:\")\n",
        "print(torch.cuda.memory_summary(device=0, abbreviated=True))\n",
        "\n",
        "# 5️⃣ Dummy model to illustrate memory usage\n",
        "class DummyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(1024, 1024).cuda()\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = DummyModel()\n",
        "\n",
        "# 6️⃣ Mixed‑precision inference with autocast\n",
        "x = torch.randn(8, 1024).cuda()\n",
        "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "    out = model(x)\n",
        "\n",
        "print(\"\\nAfter forward pass:\")\n",
        "print(torch.cuda.memory_summary(device=0, abbreviated=True))\n",
        "\n",
        "# 7️⃣ Optional: compile the model for speed (PyTorch 2.0+)\n",
        "if hasattr(torch, \"compile\"):\n",
        "    compiled = torch.compile(model)\n",
        "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "        out = compiled(x)\n",
        "    print(\"\\nCompiled model inference done.\")\n",
        "\n",
        "# 8️⃣ Clean up to free memory for the next cell\n",
        "torch.cuda.empty_cache()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Fine‑Tuning with LoRA Adapters\n",
        "\n",
        "Fine‑tuning a 20‑billion‑parameter model on a single GPU is like trying to teach a giant library to write a new style of story. The library is too big to move around, so we add a *lightweight* set of bookmarks (the LoRA adapters) that only tweak a tiny fraction of the books. These bookmarks are small matrices that sit on top of the original weights and learn the new style while the rest of the library stays untouched.\n",
        "\n",
        "### Why LoRA?  A quick analogy\n",
        "\n",
        "Imagine you have a massive LEGO set (the base model). Building a new structure from scratch would mean buying a whole new set. LoRA is like giving you a handful of extra LEGO bricks that you can snap onto the existing set to change its shape. You don’t need to rebuild the entire set; you just add a few pieces.\n",
        "\n",
        "### Key terms and trade‑offs\n",
        "\n",
        "| Term | What it means | Trade‑off | Rationale |\n",
        "|------|----------------|-----------|-----------|\n",
        "| **LoRA (Low‑Rank Adaptation)** | Adds two small trainable matrices (A and B) to each attention and MLP layer. | **Fewer trainable params** (≈0.1 % of the base model) | Saves GPU memory and speeds up training while preserving most of the original knowledge. |\n",
        "| **Adapter rank (r)** | Size of the hidden dimension in the LoRA matrices. | **Higher r → more capacity** but **more memory** | Choose r based on dataset size and GPU limits. |\n",
        "| **Alpha (α)** | Scaling factor applied to the LoRA update. | **Higher α → stronger updates** but can destabilize training | Helps balance learning speed and stability. |\n",
        "| **Gradient checkpointing** | Recomputes intermediate activations during back‑prop to save memory. | **More compute** but **less VRAM** | Essential when training large models with limited GPU memory. |\n",
        "| **Mixed‑precision (float16/float32)** | Uses lower‑precision arithmetic for speed and memory. | **Potential numerical instability** | Combined with LoRA, it keeps memory low while maintaining accuracy. |\n",
        "\n",
        "### What we’ll do in this section\n",
        "\n",
        "1. **Load the base GPT‑Oss‑20B model** and tokenizer.\n",
        "2. **Wrap the model with LoRA adapters** using the `peft` library.\n",
        "3. **Prepare a tiny synthetic dataset** (you can replace it with your own). \n",
        "4. **Set up a minimal training loop** that trains only the LoRA weights.\n",
        "5. **Show how to save and reload the fine‑tuned adapters**.\n",
        "\n",
        "All code cells are kept under 30 lines and include comments for clarity. We also set a deterministic seed and pin library versions for reproducibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣  Imports and reproducibility\n",
        "import os, random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "\n",
        "# Pin versions for reproducibility\n",
        "assert torch.__version__.startswith(\"2.\")\n",
        "\n",
        "# Set a fixed random seed\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2️⃣  Load tokenizer and base model (weights are on GPU via device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "assert HF_TOKEN is not None, \"HF_TOKEN not set\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # shard across GPUs\n",
        "    torch_dtype=torch.float16,   # mixed‑precision\n",
        ")\n",
        "\n",
        "# 3️⃣  Define LoRA configuration\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8,                # rank of the low‑rank matrices\n",
        "    lora_alpha=32,      # scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"fc1\", \"fc2\"],  # attention & MLP layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# 4️⃣  Wrap the model with LoRA adapters\n",
        "model = get_peft_model(base_model, lora_cfg)\n",
        "print(\"LoRA adapters added. Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "# 5️⃣  Create a tiny synthetic dataset (replace with real data)\n",
        "sample_texts = [\n",
        "    \"Hello world! This is a test sentence.\",\n",
        "    \"Fine‑tuning GPT‑Oss with LoRA is efficient.\",\n",
        "    \"We can add more data later.\",\n",
        "]\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": sample_texts})\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"]).with_format(\"torch\")\n",
        "\n",
        "# 6️⃣  Prepare DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# 7️⃣  Optimizer only for LoRA params\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# 8️⃣  Minimal training loop (2 epochs)\n",
        "model.train()\n",
        "for epoch in range(2):\n",
        "    for batch in loader:\n",
        "        inputs = {k: v.cuda() for k, v in batch.items() if k != \"idx\"}\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch+1} finished. Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 9️⃣  Save only the LoRA adapters\n",
        "model.save_pretrained(\"./gpt-oss-20b-lora\")\n",
        "print(\"LoRA adapters saved to ./gpt-oss-20b-lora\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the fine‑tuned adapters later\n",
        "\n",
        "```python\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the base model again (weights stay on disk)\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Attach the saved LoRA weights\n",
        "model = PeftModel.from_pretrained(base, \"./gpt-oss-20b-lora\")\n",
        "\n",
        "# Now you can generate text or continue training\n",
        "```\n",
        "\n",
        "This lightweight approach lets you keep the massive 20‑B backbone on disk and only load the tiny LoRA weights into GPU memory when needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Preparing Dataset and Tokenizer\n",
        "\n",
        "Before we can teach GPT‑Oss‑20B a new style, we need to give it a *menu* of sentences to read. Think of the dataset as a cookbook: each recipe (text example) is a small paragraph that the model will learn from. The tokenizer is the chef’s knife that chops each recipe into bite‑sized *tokens* (words, sub‑words, or punctuation) so the model can understand them.\n",
        "\n",
        "### Why this step matters\n",
        "\n",
        "1. **Tokenization is the language of the model** – GPT‑Oss expects integer IDs, not raw text. If the tokenizer is wrong, the model will read gibberish.\n",
        "2. **Padding and truncation keep the batch shape consistent** – GPUs love tensors that are all the same size. Padding adds a special *pad* token where needed, while truncation cuts off overly long sentences.\n",
        "3. **Dataset split gives us a validation set** – we can monitor over‑fitting and decide when to stop training.\n",
        "4. **Collation bundles the data into a format the model understands** – the `DataCollatorForLanguageModeling` automatically creates the `labels` field that the loss function uses.\n",
        "\n",
        "### Key terms and trade‑offs\n",
        "\n",
        "| Term | What it means | Trade‑off | Rationale |\n",
        "|------|----------------|-----------|-----------|\n",
        "| **Dataset** | A collection of text examples (e.g., Wikipedia articles). | Larger datasets improve generalization but increase storage and preprocessing time. | Use a realistic corpus that matches your target domain. |\n",
        "| **Tokenizer** | Converts text to integer IDs. | Fast tokenizers (e.g., `GPT2TokenizerFast`) use a pre‑compiled C++ backend, but may have a larger vocabulary. | Choose a tokenizer that matches the model’s training tokenizer for best compatibility. |\n",
        "| **Padding** | Adds a special token to make all sequences the same length. | Padding increases memory usage but simplifies batching. | Use `padding='max_length'` for deterministic batch shapes. |\n",
        "| **Truncation** | Cuts sequences longer than `max_length`. | Truncation can lose context but keeps memory usage bounded. | Set `max_length` to a value that balances context length and GPU memory. |\n",
        "| **Batch size** | Number of examples processed together. | Larger batches improve GPU utilization but require more VRAM. | Start with a small batch and scale up if memory allows. |\n",
        "| **DataCollator** | Helper that prepares the final input tensors for the model. | Custom collators can add special handling (e.g., causal masking). | Use `DataCollatorForLanguageModeling` for standard causal LM training. |\n",
        "\n",
        "### Practical checklist\n",
        "\n",
        "1. **Set a deterministic seed** – ensures reproducible shuffling and tokenization.\n",
        "2. **Verify the tokenizer matches the model** – use `AutoTokenizer.from_pretrained(\"gpt-oss-20b\")`.\n",
        "3. **Choose a dataset** – for demo we’ll use the `wikitext-2-raw-v1` split; replace it with your own data later.\n",
        "4. **Tokenize with `batched=True`** – speeds up preprocessing.\n",
        "5. **Split into train/validation** – `train_test_split(test_size=0.1)`.\n",
        "6. **Create a DataCollator** – handles padding and label creation.\n",
        "7. **Wrap in a DataLoader** – set `shuffle=True` for training.\n",
        "\n",
        "With this foundation, the next step (Step 7) will show how to feed these batches into the LoRA‑augmented GPT‑Oss‑20B and monitor training progress.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣  Imports and reproducibility\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Pin library versions for reproducibility\n",
        "assert torch.__version__.startswith(\"2.\")\n",
        "\n",
        "# Set a fixed random seed for deterministic shuffling & tokenization\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2️⃣  Load the tokenizer that matches GPT‑Oss‑20B\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "assert HF_TOKEN is not None, \"HF_TOKEN not set\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "# Ensure the tokenizer has a pad token (GPT‑2 style models use eos as pad)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 3️⃣  Load a small public dataset for demonstration\n",
        "# Replace this with your own data for real fine‑tuning\n",
        "raw_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 4️⃣  Tokenize the dataset in batches\n",
        "max_length = 128  # keep sequences short to fit GPU memory\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "tokenized_ds = raw_ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])  # <30 lines\n",
        "\n",
        "# 5️⃣  Split into train/validation\n",
        "train_ds, val_ds = tokenized_ds.train_test_split(test_size=0.1, seed=SEED).values()\n",
        "\n",
        "# 6️⃣  Create a DataCollator that adds the labels field\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# 7️⃣  Prepare DataLoaders for training and validation\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=4,  # adjust based on GPU memory\n",
        "    shuffle=True,\n",
        "    collate_fn=collator,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=collator,\n",
        ")\n",
        "\n",
        "print(\"Dataset ready: \", len(train_ds), \"train examples,\", len(val_ds), \"validation examples\")\n",
        "print(\"Batch shape example:\")\n",
        "for batch in train_loader:\n",
        "    print(batch[\"input_ids\"].shape, batch[\"labels\"].shape)\n",
        "    break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 8️⃣  Quick sanity check: run a forward pass on a batch\n",
        "# (Assumes you have a LoRA‑wrapped model from Step 5 loaded as `model`)\n",
        "# If you don’t have a model yet, this cell will just demonstrate the data pipeline.\n",
        "\n",
        "# Uncomment the following lines if you have a model ready:\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     for batch in train_loader:\n",
        "#         inputs = {k: v.cuda() for k, v in batch.items() if k != \"idx\"}\n",
        "#         outputs = model(**inputs)\n",
        "#         print(\"Loss:\", outputs.loss.item())\n",
        "#         break\n",
        "\n",
        "print(\"Data pipeline is functional. Ready for training in Step 7.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which library provides the LoRA implementation used in this notebook?\", [\"transformers\",\"peft\",\"datasets\",\"accelerate\"], 1, \"The peft library supplies lightweight adapters such as LoRA for efficient fine‑tuning of large language models.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary benefit of using LoRA adapters?\", [\"Faster inference\",\"Lower memory usage\",\"Higher accuracy\",\"None of the above\"], 1, \"LoRA reduces the number of trainable parameters, thereby lowering memory consumption while maintaining performance.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:15:47.061Z",
      "title": "Deploying and Fine‑Tuning GPT‑Oss‑20B in Jupyter: A Practitioner’s Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:15:47.070Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}