{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Running GPT‑OSS 20B on Your Laptop: A Beginner’s Guide\n\nLearn how to set up, load, and play with the 20‑billion‑parameter GPT‑OSS model right from a Jupyter notebook. This lesson uses simple analogies and step‑by‑step instructions so even non‑developers can get hands‑on experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:29:19.579Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what GPT‑OSS 20B is and why it matters.\n",
        "2. Show how to install the required libraries and the model weights.\n",
        "3. Demonstrate how to generate text with the model in a notebook.\n",
        "4. Identify common pitfalls and how to avoid them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic familiarity with Python and Jupyter notebooks.\n",
        "- A laptop with at least 8 GB of RAM (GPU recommended for speed).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.40.0 torch>=2.2.0 accelerate>=0.25.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.40.0\",\"torch>=2.2.0\",\"accelerate>=0.25.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1\n",
        "\n",
        "Thinking...\n",
        ">We need to output JSON with structure. Section 1: Step 1: Introduction and Setup. Must target 800-1000 tokens. Provide markdown and code cells. Include callouts. Provide estimated_tokens field. Provide prerequisites_check. Next_section_hint. Use beginner-friendly ELI5 language. Include extra explanatory paragraph defining key terms and rationale/trade-offs. Provide reproducibility seeds/versions. Provide code cells <=30 lines each. Provide callouts. Provide content array with cells....\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: What is GPT‑OSS 20B? (The Big Brain Analogy)\n",
        "\n",
        "Imagine a gigantic library that contains every book you can think of, plus a few that you haven’t even imagined yet. GPT‑OSS 20B is like that library, but instead of books it stores *patterns* in language. Each pattern is a tiny piece of knowledge that the model can pull out when you ask a question or give it a prompt.\n",
        "\n",
        "### The Big Brain Analogy\n",
        "- **Neurons → Parameters**: In a human brain, neurons fire to transmit information. In GPT‑OSS, each *parameter* is a tiny weight that helps the model decide how to transform input text into output text. With 20 billion parameters, the model has a mind‑boggling number of “neurons” to play with.\n",
        "- **Synapses → Attention Heads**: Attention heads are like the connections between neurons. They let the model look at different parts of the input simultaneously, which is crucial for understanding context.\n",
        "- **Learning → Training**: Just as a child learns by reading and listening, GPT‑OSS learns by being exposed to massive amounts of text. During training, the model adjusts its parameters to reduce the difference between its predictions and the real next word.\n",
        "\n",
        "### Why 20 B Matters\n",
        "- **Scale vs. Quality**: Larger models tend to generate more coherent and context‑aware text. 20 B sits at the sweet spot where you get noticeable quality improvements without needing a super‑high‑end GPU.\n",
        "- **Open‑Source**: Unlike some proprietary models, GPT‑OSS is freely available under a permissive license, so you can experiment, tweak, and even fine‑tune it on your own data.\n",
        "\n",
        "### Key Terms & Trade‑offs\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|----------------|----------------|-----------|\n",
        "| **Parameters** | Learnable weights in the neural network. | More parameters → richer representations. | More memory & compute needed. |\n",
        "| **Attention Heads** | Sub‑components that focus on different parts of the input. | Enables parallel context understanding. | More heads → higher memory usage. |\n",
        "| **Precision (FP32 vs FP16)** | Number of bits used to represent each weight. | FP32 gives higher numerical stability. | FP16 reduces memory and speeds up inference. |\n",
        "| **Batch Size** | Number of prompts processed together. | Larger batch → better GPU utilization. | Larger batch → higher VRAM consumption. |\n",
        "\n",
        "**Rationale**: The 20 B size was chosen to balance performance and accessibility. It’s large enough to produce high‑quality text but small enough that a laptop with an 8 GB GPU can run it in inference mode (especially if you use FP16). If you need even faster inference, you can enable *gradient checkpointing* or *model parallelism*, but those techniques add complexity.\n",
        "\n",
        "### Quick Memory Footprint Calculator\n",
        "Below is a tiny helper that estimates how much VRAM a 20 B model would need when loaded in FP32 or FP16. It’s not perfect, but it gives you a ballpark figure.\n",
        "\n",
        "> **Tip**: Run this cell *before* loading the full model to decide if you need to switch to FP16.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick VRAM estimate for GPT‑OSS 20B\n",
        "# 20B parameters * 4 bytes (FP32) ≈ 80 GB\n",
        "# 20B parameters * 2 bytes (FP16) ≈ 40 GB\n",
        "# We also add a small overhead for activations (~10%)\n",
        "\n",
        "import math\n",
        "\n",
        "PARAMS = 20_000_000_000\n",
        "BYTES_FP32 = 4\n",
        "BYTES_FP16 = 2\n",
        "OVERHEAD = 0.1  # 10% extra for activations, buffers, etc.\n",
        "\n",
        "vram_fp32 = PARAMS * BYTES_FP32 * (1 + OVERHEAD) / (1024 ** 3)\n",
        "vram_fp16 = PARAMS * BYTES_FP16 * (1 + OVERHEAD) / (1024 ** 3)\n",
        "\n",
        "print(f\"Estimated VRAM (FP32): {vram_fp32:.2f} GB\")\n",
        "print(f\"Estimated VRAM (FP16): {vram_fp16:.2f} GB\")\n",
        "\n",
        "# If you have a GPU with 8 GB, FP32 is out of reach.\n",
        "# FP16 still requires 40 GB, so you’ll need to use CPU or a cloud GPU.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Downloading the Model Weights\n",
        "\n",
        "Imagine you’re building a giant LEGO set. The instructions (the *model architecture*) tell you how the pieces fit together, but you still need to get all the bricks (the *weights*) from the store. In the world of GPT‑OSS, the bricks are huge – 20 billion tiny numbers that the model uses to decide what word comes next. This step shows you how to fetch those bricks from Hugging Face’s model hub and store them locally so you can load them later.\n",
        "\n",
        "### Why do we need to download the weights?\n",
        "- **Separation of concerns**: The architecture (how the model is built) is lightweight and can be shipped with the library, while the weights are massive and are best stored separately.\n",
        "- **Version control**: By downloading a specific commit or tag, you guarantee that the weights match the code you’re running.\n",
        "- **Offline use**: Once the weights are on disk, you can run the model without an internet connection.\n",
        "\n",
        "### Key terms and trade‑offs\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|----------------|----------------|-----------|\n",
        "| **Cache directory** | Folder where Hugging Face stores downloaded files. | Re‑uses files across projects, saving bandwidth. | Takes up disk space (≈40 GB for FP16). |\n",
        "| **HF_TOKEN** | Personal access token for private or rate‑limited repos. | Allows access to protected models. | Requires you to keep the token secret. |\n",
        "| **torch_dtype** | Data type used to load weights (e.g., `torch.float16`). | FP16 reduces memory by half but can introduce numerical noise. | FP32 is more stable but needs twice the VRAM. |\n",
        "| **Accelerate** | Library that abstracts device placement and parallelism. | Lets you run on CPU or GPU without writing device‑specific code. | Adds a small runtime overhead. |\n",
        "\n",
        "**Rationale**: We choose FP16 (`torch.float16`) for the download because it halves the storage requirement and speeds up loading on GPUs that support it. If you’re running on CPU or a GPU without FP16 support, you can switch to FP32, but be prepared for a larger disk footprint and slower inference.\n",
        "\n",
        "### What you’ll do in this cell\n",
        "1. **Set up a cache directory** so that the weights are stored in a predictable place.\n",
        "2. **Download** the `gpt-oss-20b` checkpoint using `huggingface_hub.snapshot_download`.\n",
        "3. **Handle errors** gracefully – if the download fails, we’ll print a helpful message.\n",
        "4. **Verify** that the files are present and report the total size.\n",
        "\n",
        "Feel free to tweak the `cache_dir` path if you want to keep the weights in a different location.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 1️⃣  Download the GPT‑OSS 20B weights from Hugging Face\n",
        "# ---------------------------------------------------------------\n",
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Hugging Face hub utilities\n",
        "try:\n",
        "    from huggingface_hub import snapshot_download\n",
        "except ImportError as e:\n",
        "    print(\"huggingface_hub not installed. Installing now...\")\n",
        "    !pip install -U \"huggingface_hub>=0.23.0\"\n",
        "    from huggingface_hub import snapshot_download\n",
        "\n",
        "# 2️⃣  Define where to cache the weights\n",
        "#    You can change this to any writable directory\n",
        "cache_dir = Path(\"./hf_cache\")\n",
        "cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 3️⃣  Model identifier on Hugging Face\n",
        "model_id = \"gpt-oss-20b\"\n",
        "\n",
        "# 4️⃣  Optional: set HF_TOKEN if the repo is private\n",
        "#    os.environ[\"HF_TOKEN\"] = \"<YOUR_TOKEN>\"\n",
        "\n",
        "# 5️⃣  Download with error handling\n",
        "try:\n",
        "    print(f\"Downloading {model_id} into {cache_dir} ...\")\n",
        "    snapshot_path = snapshot_download(\n",
        "        repo_id=model_id,\n",
        "        cache_dir=str(cache_dir),\n",
        "        local_files_only=False,  # allow network download\n",
        "        force_download=False,   # reuse cached files if present\n",
        "        resume_download=True,   # resume interrupted downloads\n",
        "    )\n",
        "    print(\"✅ Download completed.\")\n",
        "except Exception as exc:\n",
        "    print(\"❌ Failed to download model weights.\")\n",
        "    print(f\"Error: {exc}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# 6️⃣  Quick sanity check: list top-level files\n",
        "print(\"\\nTop-level files in the snapshot:\")\n",
        "for item in Path(snapshot_path).iterdir():\n",
        "    print(f\"- {item.name}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verifying the download\n",
        "\n",
        "After the download finishes, you should see a folder structure that looks something like this:\n",
        "\n",
        "```\n",
        "./hf_cache/gpt-oss-20b/\n",
        "├── config.json\n",
        "├── generation_config.json\n",
        "├── pytorch_model.bin\n",
        "├── tokenizer.json\n",
        "└── tokenizer_config.json\n",
        "```\n",
        "\n",
        "The `pytorch_model.bin` file is the heavy‑weight part – it contains all 20 billion parameters. If you’re on a machine with limited disk space, you might want to delete any old checkpoints that you no longer need.\n",
        "\n",
        "**Tip**: If you plan to run the model on a GPU that supports FP16, you can skip the `pytorch_model.bin` file after you’ve loaded the model once and saved it in FP16 format. This can save you a few gigabytes of storage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2️⃣  Verify the size of the downloaded checkpoint\n",
        "# ---------------------------------------------------------------\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "snapshot_path = Path(\"./hf_cache/gpt-oss-20b\")\n",
        "model_file = snapshot_path / \"pytorch_model.bin\"\n",
        "\n",
        "if model_file.exists():\n",
        "    size_bytes = model_file.stat().st_size\n",
        "    size_gb = size_bytes / (1024 ** 3)\n",
        "    print(f\"\\n✅ {model_file.name} size: {size_gb:.2f} GB\")\n",
        "else:\n",
        "    print(\"❌ pytorch_model.bin not found. Check the download path.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Loading the Model in Python\n",
        "\n",
        "Imagine you have a huge library of books (the model weights) that you just downloaded in the previous step. Now you want to read a specific book, but you don’t want to load the entire library into your mind at once – that would be exhausting and slow. Instead, you open the book you need, read a few pages, and then close it. In the same way, we’ll load only the parts of the GPT‑OSS 20B model that we need for inference, and we’ll do it in a way that fits comfortably on your laptop.\n",
        "\n",
        "### Why do we need a special loading routine?\n",
        "- **Memory is limited**: 20 B parameters are huge – even in FP16 they need ~40 GB of RAM if you load everything at once.\n",
        "- **Speed matters**: Loading the entire checkpoint into memory can take minutes, especially on a CPU.\n",
        "- **Flexibility**: We want the same code to run on a laptop with a small GPU, a laptop with no GPU, or a cloud instance with many GPUs.\n",
        "\n",
        "### The loading workflow\n",
        "1. **Choose a data type** (`torch_dtype`). FP16 halves the memory footprint but can introduce a tiny loss in numerical precision. FP32 is safer but doubles the memory.\n",
        "2. **Decide where to run** (`device_map`). The `accelerate` library can automatically split the model across available devices (CPU, GPU, or multiple GPUs) using a strategy called *auto*.\n",
        "3. **Optional speed‑up**: `torch.compile` (available in PyTorch 2.2+) can re‑write the model graph for faster execution, especially on CPUs.\n",
        "4. **Load the tokenizer** so that we can convert text to token IDs and back.\n",
        "\n",
        "### Extra explanatory paragraph – key terms & trade‑offs\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|----------------|----------------|-----------|\n",
        "| **Model weights** | The 20 B numbers that encode language knowledge. | They are the core of the model’s ability to generate text. | Huge – 40 GB FP16, 80 GB FP32. |\n",
        "| **Tokenizer** | Converts words/characters into integer IDs the model understands. | Needed for any text input or output. | Small (~10 MB). |\n",
        "| **torch_dtype** | The precision of the tensors (`torch.float16` or `torch.float32`). | Controls memory usage and numerical stability. | FP16 saves memory but can be slightly less accurate. |\n",
        "| **device_map** | Where each part of the model lives (CPU, GPU, or multiple GPUs). | Allows you to run large models on limited hardware. | More devices → more complexity; *auto* may fall back to CPU if GPU memory is insufficient. |\n",
        "| **accelerate** | A helper library that abstracts device placement and parallelism. | Lets you write one line of code that works on any hardware. | Adds a small runtime overhead. |\n",
        "| **torch.compile** | JIT‑compiles PyTorch code for speed. | Can give 2–3× speedup on CPUs. | Requires PyTorch 2.2+ and may not always improve GPU speed. |\n",
        "\n",
        "**Rationale**: By combining FP16, `accelerate`’s *auto* device map, and optional `torch.compile`, we can run GPT‑OSS 20B on a laptop with an 8 GB GPU or even on a CPU, albeit with a longer inference time. If you have a more powerful GPU (≥12 GB VRAM) or a multi‑GPU setup, you can switch to FP32 for maximum stability.\n",
        "\n",
        "### Quick sanity check\n",
        "Below we’ll load the model and tokenizer, set a random seed for reproducibility, and print out a few details so you can confirm everything is wired up correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 1️⃣  Load GPT‑OSS 20B with accelerate and optional torch.compile\n",
        "# ---------------------------------------------------------------\n",
        "# Import libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "\n",
        "# 2️⃣  Reproducibility: set a fixed seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 3️⃣  Define model path (the folder created in Step 3)\n",
        "MODEL_DIR = \"./hf_cache/gpt-oss-20b\"\n",
        "\n",
        "# 4️⃣  Choose precision – FP16 is usually enough for inference\n",
        "TORCH_DTYPE = torch.float16\n",
        "\n",
        "# 5️⃣  Load the tokenizer (small, fast)\n",
        "print(\"Loading tokenizer…\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "except Exception as exc:\n",
        "    raise RuntimeError(f\"Failed to load tokenizer: {exc}\")\n",
        "\n",
        "# 6️⃣  Load the model with accelerate’s dispatch\n",
        "print(\"Loading model… (this may take a few minutes)\\n\")\n",
        "try:\n",
        "    # Use accelerate to automatically place layers on available devices\n",
        "    model = load_checkpoint_and_dispatch(\n",
        "        AutoModelForCausalLM,\n",
        "        checkpoint=MODEL_DIR,\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # auto‑split across CPU/GPU\n",
        "        dtype=TORCH_DTYPE,           # FP16 for lower memory\n",
        "        no_split_module_classes=[\"GPTNeoXAttention\", \"GPTNeoXMLP\"],  # avoid splitting large modules\n",
        "    )\n",
        "except Exception as exc:\n",
        "    raise RuntimeError(f\"Failed to load model: {exc}\")\n",
        "\n",
        "# 7️⃣  Optional: compile the model for faster CPU inference\n",
        "if torch.backends.cuda.is_available():\n",
        "    # GPU path – no compile needed, PyTorch already optimised\n",
        "    print(\"Using GPU – no torch.compile needed.\")\n",
        "else:\n",
        "    try:\n",
        "        model = torch.compile(model)\n",
        "        print(\"torch.compile applied – CPU inference will be faster.\")\n",
        "    except Exception as exc:\n",
        "        print(f\"torch.compile failed: {exc}\\nContinuing without compilation.\")\n",
        "\n",
        "# 8️⃣  Quick sanity check: run a tiny forward pass\n",
        "prompt = \"Once upon a time\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(input_ids, max_new_tokens=5)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generating Your First Text\n",
        "\n",
        "Now that we have the model and tokenizer ready, it’s time to ask the model a question and see what it writes. Think of the model as a very eager student who has read a huge textbook (the 20 billion parameters). When you give it a prompt, the student tries to finish the sentence in the most plausible way, based on everything it has learned.\n",
        "\n",
        "### How the model decides what to write\n",
        "When you call `model.generate()`, the transformer runs a *forward pass* through all its layers for each token you want to produce. At each step it looks at the probability distribution over the entire vocabulary and picks the next token. The way it picks that token is controlled by a handful of knobs:\n",
        "\n",
        "| Knob | What it does | Typical values | Why it matters |\n",
        "|------|--------------|----------------|----------------|\n",
        "| **temperature** | Scales the logits before softmax. Lower values make the distribution sharper (more deterministic). | 0.1 – 1.0 | Controls *creativity* vs. *certainty*. |\n",
        "| **top_p** (nucleus sampling) | Keeps only the smallest set of tokens whose cumulative probability exceeds `p`. | 0.8 – 0.95 | Avoids picking very unlikely words while still allowing variety. |\n",
        "| **repetition_penalty** | Penalises tokens that have already appeared. | 1.0 – 1.2 | Reduces repetitive loops. |\n",
        "| **max_new_tokens** | How many new tokens to generate after the prompt. | 20 – 200 | Limits output length and memory usage. |\n",
        "\n",
        "**Extra explanatory paragraph – key terms & trade‑offs**\n",
        "\n",
        "- **Logits** are raw, unnormalised scores that the model assigns to each word. They are transformed into probabilities by the softmax function.\n",
        "- **Softmax** turns logits into a probability distribution that sums to 1. A higher temperature flattens this distribution, giving rarer words a better chance to be chosen.\n",
        "- **Sampling vs. Greedy**: Greedy decoding picks the highest‑probability token every time (temperature=0). Sampling (temperature>0) introduces randomness, which can produce more interesting text but also more errors.\n",
        "- **Nucleus sampling (top_p)** is a compromise: it keeps the most likely tokens that together make up a certain probability mass, discarding the rest. This prevents the model from choosing very unlikely words while still allowing diversity.\n",
        "- **Repetition penalty** is useful when the model starts looping (e.g., \"...and then...and then...\"), but too high a penalty can make the text feel unnatural.\n",
        "\n",
        "**Trade‑offs**: Lower temperature + higher top_p → more deterministic, safer text but potentially dull. Higher temperature + lower top_p → more creative but risk of nonsensical output. The right balance depends on your use‑case.\n",
        "\n",
        "### Quick sanity check\n",
        "Below we’ll generate a short paragraph about a robot learning to cook. We’ll set a fixed random seed so that you can reproduce the same output every time you run the cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 1️⃣  Generate a short paragraph with default settings\n",
        "# ---------------------------------------------------------------\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Reproducibility: set a fixed seed for torch and numpy\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Load tokenizer and model (assumes Step 4 already loaded them)\n",
        "# If you ran Step 4 in the same notebook, you can reuse the objects:\n",
        "# tokenizer = <existing tokenizer>\n",
        "# model = <existing model>\n",
        "# For safety, we reload them from the checkpoint directory.\n",
        "MODEL_DIR = \"./hf_cache/gpt-oss-20b\"\n",
        "\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "print(\"Loading model…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    torch_dtype=torch.float16,  # keep FP16 for speed/memory\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # auto‑split across CPU/GPU\n",
        ")\n",
        "\n",
        "# Prompt and generation parameters\n",
        "prompt = \"The robot decided to try cooking a new dish.\"\n",
        "max_new_tokens = 60\n",
        "temperature = 0.7\n",
        "top_p = 0.9\n",
        "repetition_penalty = 1.1\n",
        "\n",
        "# Encode prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate text\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        do_sample=True,  # enable sampling\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated paragraph:\\n\")\n",
        "print(generated_text)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2️⃣  Experiment with different generation settings\n",
        "# ---------------------------------------------------------------\n",
        "# Feel free to tweak the following values and re‑run the cell.\n",
        "# The output will change because we are sampling from a probability distribution.\n",
        "\n",
        "# New settings\n",
        "temperature = 0.3   # more deterministic\n",
        "top_p = 0.95        # keep a larger pool of tokens\n",
        "max_new_tokens = 40\n",
        "\n",
        "# Re‑run generation\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "print(\"\\nGenerated paragraph with new settings:\\n\")\n",
        "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6\n",
        "\n",
        "Thinking...\n",
        ">We need to output JSON structure for section 6. Must follow guidelines: 800-1000 tokens, markdown + code cells, callouts, estimated_tokens, prerequisites_check, next_section_hint. Provide beginner-friendly ELI5 language, analogies, precise terms, extra explanatory paragraph defining key terms and rationale/trade-offs. Code cells <=30 lines each. Provide reproducibility seeds/versions. Provide callouts. Provide content array with cells. The content_type is markdown only for step 6? I...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following is NOT a recommended way to reduce GPU memory usage when running GPT‑OSS 20B?\", [\"Use a smaller batch size\",\"Enable gradient checkpointing\",\"Increase the number of attention heads\",\"Run inference on CPU instead of GPU\"], 2, \"Increasing the number of attention heads would actually increase memory usage. The other options help reduce memory or shift the load to CPU.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Quick check 2: Basic understanding\", [\"A\",\"B\",\"C\",\"D\"], 0, \"Review the outline section to find the correct answer.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:29:19.572Z",
      "title": "Running GPT‑OSS 20B on Your Laptop: A Beginner’s Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:29:19.579Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}