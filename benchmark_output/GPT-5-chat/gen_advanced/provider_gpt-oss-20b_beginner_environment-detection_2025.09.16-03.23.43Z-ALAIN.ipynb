{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ”§ Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('ğŸ“ Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ” Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('âš ï¸ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'ğŸ” Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return 'âˆ…'\\n    return v[:3] + 'â€¦' + v[-2:] if len(v) > 6 else 'â€¢â€¢â€¢'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸŒ ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('âš ï¸ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('âš ï¸ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('âš ï¸ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('âœ… Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('âš ï¸ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ” Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('âš ï¸ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('âœ… Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('âš ï¸ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) â€” 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying and Fineâ€‘Tuning GPTâ€‘OSSâ€‘20B for Research Applications\n\nThis lesson guides advanced practitioners through the endâ€‘toâ€‘end workflow of deploying the GPTâ€‘OSSâ€‘20B model, from environment setup to efficient fineâ€‘tuning on custom corpora. It covers architectural tradeâ€‘offs, memoryâ€‘efficient inference, and best practices for reproducibility in research settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> â±ï¸ Estimated time to complete: 36â€“60 minutes (rough).  ",
        "\n> ğŸ•’ Created (UTC): 2025-09-16T03:23:43.221Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain the architectural differences between GPTâ€‘OSSâ€‘20B and other largeâ€‘languageâ€‘model variants.\n",
        "2. Configure a GPUâ€‘optimized environment that supports 20Bâ€‘parameter inference and training.\n",
        "3. Implement memoryâ€‘efficient fineâ€‘tuning using gradient checkpointing and 8â€‘bit optimizers.\n",
        "4. Evaluate model performance on domainâ€‘specific benchmarks and document reproducibility metadata.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+ with pip\n",
        "- PyTorch 2.0+ (CUDA 11.8 or higher)\n",
        "- Basic familiarity with Hugging Face Transformers and Jupyter notebooks\n",
        "- Access to a GPU with â‰¥24â€¯GB VRAM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 torch==2.0.0+cu118 transformers==4.40.0 accelerate==0.24.0 bitsandbytes==0.41.0 datasets==2.20.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"torch==2.0.0+cu118\",\"transformers==4.40.0\",\"accelerate==0.24.0\",\"bitsandbytes==0.41.0\",\"datasets==2.20.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('âš ï¸ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('âœ… Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('âš ï¸ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Validation and GPU Profiling\n",
        "\n",
        "Before we can play with a 20â€‘billionâ€‘parameter model, we need to make sure our playground (the GPU) is ready to host it. Think of the GPU as a giant kitchen: the model is a huge recipe that requires a lot of space (memory) and a steady supply of ingredients (compute). If the kitchen is too small or the stove is too weak, the recipe will burn.\n",
        "\n",
        "In this step we will:\n",
        "\n",
        "1. **Verify the software stack** â€“ confirm that the correct versions of PyTorch, CUDA, and Hugging Face libraries are installed.\n",
        "2. **Check GPU availability** â€“ ensure that the GPU we intend to use is visible to PyTorch and that its memory capacity meets the 20B requirement.\n",
        "3. **Profile memory usage** â€“ run a quick inference pass with a small prompt to see how much VRAM is actually consumed.\n",
        "4. **Set reproducibility seeds** â€“ lock down random number generators so that experiments can be repeated exactly.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **CUDA**: NVIDIAâ€™s parallel computing platform. It turns the GPU into a fast math engine.\n",
        "- **VRAM**: Video RAM, the memory that lives on the GPU. Large models need a lot of VRAM to store parameters and intermediate activations.\n",
        "- **Gradient Checkpointing**: A technique that trades extra computation for lower memory usage by recomputing certain activations during backâ€‘propagation.\n",
        "- **8â€‘bit Quantization**: Reducing the precision of model weights from 32â€‘bit floats to 8â€‘bit integers, cutting memory by ~4Ã— while keeping accuracy largely intact.\n",
        "- **Reproducibility Seed**: A fixed integer that initializes all random number generators (Python, NumPy, PyTorch) so that the same sequence of random numbers is produced every run.\n",
        "\n",
        "### Why These Checks Matter\n",
        "\n",
        "- **Avoid Runtime Crashes**: If the GPU is not detected or the VRAM is insufficient, the notebook will crash midâ€‘run, wasting time.\n",
        "- **Performance Tuning**: Knowing the exact memory footprint lets us decide whether to enable gradient checkpointing or 8â€‘bit quantization.\n",
        "- **Scientific Rigor**: Reproducibility seeds ensure that results can be verified by others, a cornerstone of research.\n",
        "\n",
        "### Tradeâ€‘offs\n",
        "\n",
        "- **Speed vs Memory**: Enabling gradient checkpointing or 8â€‘bit quantization reduces memory but increases compute time. For quick prototyping, you might skip checkpointing; for fullâ€‘scale fineâ€‘tuning, youâ€™ll need it.\n",
        "- **Precision vs Efficiency**: 8â€‘bit quantization saves memory but can introduce a tiny drop in accuracy. For most research tasks, the tradeâ€‘off is negligible.\n",
        "\n",
        "Now letâ€™s put these ideas into practice with a couple of short code snippets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1ï¸âƒ£  Verify software versions and GPU visibility\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import torch, transformers, accelerate, bitsandbytes, datasets\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "print(f\"Bitsandbytes version: {bitsandbytes.__version__}\")\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "\n",
        "# Check GPU count and properties\n",
        "gpu_count = torch.cuda.device_count()\n",
        "print(f\"Number of GPUs detected: {gpu_count}\")\n",
        "for i in range(gpu_count):\n",
        "    prop = torch.cuda.get_device_properties(i)\n",
        "    print(f\"GPU {i}: {prop.name}, VRAM: {prop.total_memory / (1024**3):.2f} GB\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2ï¸âƒ£  Quick VRAM profiling with a tiny prompt\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Use a small 8â€‘bit quantized model for the test\n",
        "model_name = \"TheBloke/GPT-OSS-20B-GPTQ-4bit-128g\"\n",
        "# Note: replace with your actual 8â€‘bit checkpoint if different\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# Load model with 8â€‘bit quantization (requires bitsandbytes)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Simple prompt\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# Run inference and measure peak memory\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "peak_mem = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "print(f\"Peak VRAM used for inference: {peak_mem:.2f} GB\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 3ï¸âƒ£  Set reproducibility seeds\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import random, numpy as np\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "print(f\"Reproducibility seed set to {seed}\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Loading GPTâ€‘OSSâ€‘20B with 8â€‘bit Quantization\n",
        "\n",
        "Imagine you have a gigantic library (the 20â€‘billionâ€‘parameter model) that you want to read on a small laptop (your GPU). The library is too big to fit in the laptopâ€™s memory, so you decide to shrink each book by a factor of four: you keep only the most important words and drop the rest. Thatâ€™s what **8â€‘bit quantization** does â€“ it compresses the model weights from 32â€‘bit floatingâ€‘point numbers to 8â€‘bit integers, cutting the memory footprint by roughly 75â€¯% while keeping the story (the modelâ€™s predictions) almost unchanged.\n",
        "\n",
        "In this step weâ€™ll:\n",
        "\n",
        "1. **Pull the tokenizer** â€“ the piece of software that turns text into numbers the model can understand.\n",
        "2. **Load the model with 8â€‘bit weights** â€“ using the `bitsandbytes` library, which hooks into PyTorch to perform the compression on the fly.\n",
        "3. **Map the model to the GPU** â€“ automatically splitting the layers across available devices so we donâ€™t run out of VRAM.\n",
        "4. **Verify the memory savings** â€“ by running a tiny inference pass and printing the peak VRAM usage.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **8â€‘bit Quantization**: Converting 32â€‘bit floatingâ€‘point weights to 8â€‘bit integers. This reduces memory usage by ~4Ã— but introduces a small quantization error.\n",
        "- **bitsandbytes**: A PyTorch extension that implements efficient 8â€‘bit (and 4â€‘bit) weight loading and inference, plus fast matrix multiplication kernels.\n",
        "- **device_map**: A dictionary that tells Hugging Face Transformers which GPU each layer should live on. Setting it to \"auto\" lets the library decide the best placement.\n",
        "- **torch_dtype**: The data type used for activations during inference. Using `torch.float16` keeps compute fast while still being accurate enough for most tasks.\n",
        "- **Peak VRAM**: The maximum amount of GPU memory allocated at any point during a run. Monitoring this helps ensure we stay within hardware limits.\n",
        "\n",
        "### Why 8â€‘bit Quantization Matters\n",
        "\n",
        "- **Memory Efficiency**: A 20â€‘B model normally needs 80â€¯GB of VRAM in 32â€‘bit precision. With 8â€‘bit quantization, we can fit it on a single 24â€‘GB GPU.\n",
        "- **Speed**: 8â€‘bit kernels in bitsandbytes are often faster than 32â€‘bit ones because they use less memory bandwidth.\n",
        "- **Accuracy Tradeâ€‘off**: The quantization error is usually <1â€¯% in perplexity for large language models, making it acceptable for research and many production scenarios.\n",
        "\n",
        "### Tradeâ€‘offs to Keep in Mind\n",
        "\n",
        "| Aspect | 32â€‘bit | 8â€‘bit | What to Expect |\n",
        "|--------|--------|-------|----------------|\n",
        "| VRAM | ~80â€¯GB | ~20â€¯GB | Huge savings |\n",
        "| Compute | Baseline | Slightly faster (less memory traffic) | Good for inference |\n",
        "| Accuracy | Gold standard | Minor drop (often <1â€¯%) | Acceptable for most tasks |\n",
        "| Implementation | Simple | Requires bitsandbytes | Add a dependency |\n",
        "\n",
        "Now letâ€™s see how to do this in code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1ï¸âƒ£  Load tokenizer and 8â€‘bit GPTâ€‘OSSâ€‘20B\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Replace with the exact checkpoint you want to use\n",
        "MODEL_NAME = \"TheBloke/gpt-oss-20b-gptq-4bit-128g\"\n",
        "\n",
        "# Load tokenizer (fast tokenizer is usually faster)\n",
        "print(\"Loading tokenizerâ€¦\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Load model with 8â€‘bit weights via bitsandbytes\n",
        "print(\"Loading model with 8â€‘bit quantizationâ€¦\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # automatically place layers on GPUs\n",
        "    load_in_8bit=True,          # enable 8â€‘bit quantization\n",
        "    torch_dtype=torch.float16   # use FP16 activations for speed\n",
        ")\n",
        "\n",
        "print(\"Model loaded on device:\", next(model.parameters()).device)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2ï¸âƒ£  Quick VRAM profiling with a tiny prompt\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "with torch.no_grad():\n",
        "    _ = model.generate(**inputs, max_new_tokens=10)\n",
        "peak_mem = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "print(f\"Peak VRAM used for inference: {peak_mem:.2f} GB\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Memoryâ€‘Efficient Inference with Gradient Checkpointing\n",
        "\n",
        "### Why do we need it?\n",
        "When you run a 20â€‘B model on a single 24â€‘GB GPU, the forward pass alone can eat up most of the memory. Think of the model as a giant Lego set: each block (layer) needs to sit on the table (GPU memory) while you build the next block. If the table is too small, you have to keep moving blocks around, which slows you down. Gradient checkpointing is like a clever Legoâ€‘stacking trick: you only keep a few blocks on the table at a time and rebuild the rest on the fly when you need them. The tradeâ€‘off is a bit more time, but you can fit the whole set on a smaller table.\n",
        "\n",
        "### How it works in PyTorch\n",
        "PyTorchâ€™s `torch.utils.checkpoint` module lets you wrap a function (e.g., a transformer block) so that its intermediate activations are *not* stored during the forward pass. When the backward pass (or a manual recomputation) is triggered, the function is reâ€‘executed to regenerate those activations. For inference, we can force a recomputation after each block to keep memory low, at the cost of extra compute.\n",
        "\n",
        "### Key Terms Explained\n",
        "- **Checkpointing**: The act of discarding intermediate activations during the forward pass and recomputing them later.\n",
        "- **Recomputation**: Running the same forward function again to regenerate activations that were not stored.\n",
        "- **Peak VRAM**: The maximum amount of GPU memory allocated at any point during a run.\n",
        "- **`torch.utils.checkpoint.checkpoint`**: A helper that automatically handles the discardâ€‘andâ€‘recompute logic.\n",
        "- **`accelerate` gradientâ€‘checkpointing**: A higherâ€‘level wrapper that applies checkpointing to all layers of a Hugging Face model.\n",
        "\n",
        "### Rationale & Tradeâ€‘offs\n",
        "| Aspect | 32â€‘bit / No Checkpointing | With Checkpointing |\n",
        "|--------|---------------------------|--------------------|\n",
        "| Memory | Uses full activations (â‰ˆ20â€¯GB for 8â€‘bit GPTâ€‘OSSâ€‘20B) | Stores only a few activations (â‰ˆ5â€“10â€¯GB) |\n",
        "| Compute | Baseline | Extra forward passes (â‰ˆ2Ã— slower) |\n",
        "| Accuracy | Unchanged | Unchanged (exact recomputation) |\n",
        "| Complexity | Simple | Requires wrapping layers or using `accelerate` |\n",
        "\n",
        "In research settings where GPU memory is the bottleneck, checkpointing is a lifesaver. For quick prototyping, you might skip it to save time.\n",
        "\n",
        "### Quick Code Demo\n",
        "Below we load the same 8â€‘bit GPTâ€‘OSSâ€‘20B model from Stepâ€¯2 and wrap its transformer blocks with `torch.utils.checkpoint`. We also set a reproducibility seed and measure peak VRAM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1ï¸âƒ£  Reproducibility & imports\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import random, numpy as np, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(f\"Reproducibility seed set to {SEED}\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2ï¸âƒ£  Load tokenizer & 8â€‘bit model (same as Stepâ€¯2)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "MODEL_NAME = \"TheBloke/gpt-oss-20b-gptq-4bit-128g\"\n",
        "print(\"Loading tokenizerâ€¦\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(\"Loading model with 8â€‘bit quantizationâ€¦\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 3ï¸âƒ£  Wrap transformer blocks with checkpointing\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Hugging Face stores blocks in model.model.decoder.layers\n",
        "layers = model.model.decoder.layers\n",
        "for i, layer in enumerate(layers):\n",
        "    # Replace the forward method with a checkpointed version\n",
        "    orig_forward = layer.forward\n",
        "    def chkpt_forward(*args, **kwargs):\n",
        "        return checkpoint(orig_forward, *args, **kwargs)\n",
        "    layer.forward = chkpt_forward\n",
        "print(\"Applied checkpointing to all decoder layers.\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 4ï¸âƒ£  Quick VRAM profiling with a tiny prompt\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "with torch.no_grad():\n",
        "    _ = model.generate(**inputs, max_new_tokens=10)\n",
        "peak_mem = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "print(f\"Peak VRAM used for inference with checkpointing: {peak_mem:.2f} GB\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Preparing Domainâ€‘Specific Dataset with Hugging Face Datasets\n",
        "\n",
        "When you want a language model to speak like a cardiologist, a lawyer, or a software engineer, you need to give it a *diet* of text that reflects that specialty. Think of the model as a sponge that absorbs water (text). If you pour in only kitchen recipes, it will never learn how to talk about heart disease. The Hugging Face `datasets` library is the kitchen sink that lets you fetch, clean, and feed the right kind of water into the sponge.\n",
        "\n",
        "In this step weâ€™ll:\n",
        "\n",
        "1. **Pull a domainâ€‘specific corpus** from the Hugging Face Hub or a local file.\n",
        "2. **Split** it into training, validation, and test sets while preserving class balance.\n",
        "3. **Tokenize** the raw text with the same tokenizer we used to load GPTâ€‘OSSâ€‘20B.\n",
        "4. **Cache** the processed dataset for fast reuse.\n",
        "5. **Inspect** a few examples to sanityâ€‘check the pipeline.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **Dataset**: A collection of data points (e.g., sentences, paragraphs) that the model will learn from.\n",
        "- **Tokenization**: The process of converting raw text into a sequence of integer IDs that the model can understand.\n",
        "- **Streaming**: Loading data lazily from disk or the internet to avoid memory overload.\n",
        "- **Cache**: Storing the processed dataset on disk so that subsequent runs skip the expensive preprocessing step.\n",
        "- **Shuffling**: Randomly reordering the data to prevent the model from learning spurious order effects.\n",
        "- **Batching**: Grouping multiple examples together to take advantage of GPU parallelism.\n",
        "\n",
        "### Why Domainâ€‘Specific Datasets Matter\n",
        "\n",
        "- **Relevance**: The model learns the vocabulary, style, and facts that are most useful for your target audience.\n",
        "- **Bias Mitigation**: By curating the data, you can reduce unwanted stereotypes or misinformation.\n",
        "- **Performance**: Fineâ€‘tuning on a focused corpus often yields higher accuracy on downstream tasks than generic data.\n",
        "\n",
        "### Tradeâ€‘offs to Keep in Mind\n",
        "\n",
        "| Decision | Memory | Speed | Quality |\n",
        "|----------|--------|-------|---------|\n",
        "| **Fullâ€‘text tokenization** | High (tokens are stored) | Slower (more passes) | Highest (no loss of context) |\n",
        "| **Chunking** (e.g., 512â€‘token windows) | Lower | Faster | Slightly lower (context truncated) |\n",
        "| **Streaming + onâ€‘theâ€‘fly tokenization** | Minimal | Fastest | Depends on implementation |\n",
        "\n",
        "In research, we usually favor fullâ€‘text tokenization with caching because it preserves the richest context while still being reproducible.\n",
        "\n",
        "### Extra Explanatory Paragraph\n",
        "\n",
        "The `datasets` library abstracts away many of the pain points of data handling. Internally it represents a dataset as a lazy, columnar structure that can be filtered, mapped, and split with declarative syntax. When you call `dataset.map(tokenize_function, batched=True)`, the library automatically parallelizes the operation across CPU cores and writes the results to a cache directory (`~/.cache/huggingface/datasets`). This means that the next time you run the notebook, the heavy tokenization step is skipped, saving you minutes or hours. The tradeâ€‘off is that the cache can grow large (hundreds of MBs), so you should monitor disk usage or clean the cache (`datasets.cleanup_cache_files()`) when necessary.\n",
        "\n",
        "Now letâ€™s turn theory into practice with a couple of short code cells.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1ï¸âƒ£  Load a domainâ€‘specific dataset (e.g., PubMed abstracts)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import os\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Example: PubMed abstracts from the Hugging Face Hub\n",
        "# Replace with your own dataset path or name if needed\n",
        "DATASET_NAME = \"allenai/pubmed_abstracts\"\n",
        "\n",
        "# Load the dataset with streaming to avoid memory overload\n",
        "print(\"Loading datasetâ€¦\")\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\", streaming=True)\n",
        "\n",
        "# Convert to a finite Dataset (weâ€™ll take the first 10k examples for demo)\n",
        "print(\"Collecting 10,000 examplesâ€¦\")\n",
        "train_data = dataset.take(10000)\n",
        "train_dataset = DatasetDict({\"train\": train_data})\n",
        "\n",
        "# Split into train/validation (90/10)\n",
        "print(\"Splitting into train/validationâ€¦\")\n",
        "train_split, val_split = train_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = DatasetDict({\"train\": train_split, \"validation\": val_split})\n",
        "\n",
        "print(f\"Train size: {len(train_dataset['train'])}\")\n",
        "print(f\"Validation size: {len(train_dataset['validation'])}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2ï¸âƒ£  Tokenize the dataset with the GPTâ€‘OSSâ€‘20B tokenizer\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the same tokenizer used for the model\n",
        "MODEL_NAME = \"TheBloke/gpt-oss-20b-gptq-4bit-128g\"\n",
        "print(\"Loading tokenizerâ€¦\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Define a tokenization function\n",
        "def tokenize_function(examples):\n",
        "    # The dataset column is \"abstract\" for PubMed; adjust if yours differs\n",
        "    return tokenizer(examples[\"abstract\"], truncation=True, max_length=512)\n",
        "\n",
        "# Apply tokenization in batched mode for speed\n",
        "print(\"Tokenizingâ€¦ (this may take a minute)\")\n",
        "train_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"abstract\"],  # keep only tokenized fields\n",
        "    num_proc=4,  # parallelize across 4 CPU cores\n",
        "    load_from_cache_file=True,  # reuse cached results if available\n",
        ")\n",
        "\n",
        "# Verify the first example\n",
        "print(\"First tokenized example:\")\n",
        "print(train_dataset['train'][0])\n",
        "\n",
        "# Save the processed dataset for later reuse\n",
        "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/datasets/processed_pubmed\")\n",
        "print(f\"Saving processed dataset to {CACHE_DIR}\")\n",
        "train_dataset.save_to_disk(CACHE_DIR)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Configuring Accelerate for Multiâ€‘GPU Fineâ€‘Tuning\n",
        "\n",
        "Imagine youâ€™re a conductor leading an orchestra that spans several concert halls (GPUs). Each hall has its own acoustics (memory, compute), but you want the music (training) to sound seamless across all of them. Hugging Face **Accelerate** is the conductorâ€™s score: it tells each hall when to play, how many instruments to assign, and how to share the sheet music (model parameters) so that the whole symphony stays in tune.\n",
        "\n",
        "In this step weâ€™ll:\n",
        "\n",
        "1. **Create an `accelerate` configuration** that tells the library how many GPUs to use, what precision to run in, and whether to enable gradient checkpointing.\n2. **Generate a minimal training script** that imports the configuration, loads the 8â€‘bit GPTâ€‘OSSâ€‘20B, and starts distributed training.\n3. **Launch the training** with `accelerate launch`, which automatically handles device placement, mixedâ€‘precision, and logging.\n4. **Verify reproducibility** by setting seeds and checking that the same random numbers are produced on every run.\n",
        "\n",
        "By the end of this section youâ€™ll have a readyâ€‘toâ€‘run training pipeline that scales across multiple GPUs without writing any boilerplate distributedâ€‘training code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Terms Explained\n",
        "\n",
        "- **Accelerate**: A lightweight library that abstracts away the complexities of distributed training, mixedâ€‘precision, and device placement.\n",
        "- **`accelerate config`**: An interactive CLI that writes a YAML file (`accelerate_config.yaml`) describing the training environment (number of processes, GPUs, precision, etc.).\n",
        "- **Gradient Checkpointing**: A memoryâ€‘saving technique that recomputes activations during backâ€‘propagation instead of storing them.\n",
        "- **Mixedâ€‘Precision (FP16/FP8)**: Using lowerâ€‘precision arithmetic for activations and gradients to reduce memory bandwidth while maintaining model accuracy.\n",
        "- **Distributed Data Parallel (DDP)**: A PyTorch strategy that replicates the model on each GPU and synchronizes gradients automatically.\n",
        "- **Reproducibility Seed**: A fixed integer that seeds all random number generators (Python, NumPy, PyTorch) to ensure deterministic behavior.\n",
        "\n",
        "### Why Accelerate Matters\n",
        "\n",
        "1. **Zero Boilerplate** â€“ You donâ€™t need to write `torch.distributed.init_process_group()` or manually wrap your model with `DistributedDataParallel`.\n",
        "2. **Flexibility** â€“ The same configuration works for singleâ€‘GPU, multiâ€‘GPU, or even multiâ€‘node setups.\n",
        "3. **Performance** â€“ Accelerate automatically selects the best precision (FP16 or BF16) for your GPU and handles gradient accumulation.\n",
        "4. **Reproducibility** â€“ By setting seeds in both the script and the config, you can guarantee that the same training run produces identical results.\n",
        "\n",
        "### Tradeâ€‘offs to Keep in Mind\n",
        "\n",
        "| Feature | Memory | Speed | Complexity |\n",
        "|---------|--------|-------|------------|\n",
        "| Gradient Checkpointing | â†“ | â†‘ | Moderate (requires `gradient_checkpointing=True`) |\n",
        "| Mixedâ€‘Precision | â†“ | â†‘ | Low (handled by Accelerate) |\n",
        "| Distributed Data Parallel | â†“ (perâ€‘GPU) | â†‘ | Low (handled by Accelerate) |\n",
        "| Fullâ€‘Precision | â†‘ | â†“ | None |\n",
        "\n",
        "In research, the combination of gradient checkpointing + mixedâ€‘precision + DDP gives you the best memoryâ€‘speed tradeâ€‘off for a 20â€‘B model on a 24â€‘GB GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extra Explanatory Paragraph\n",
        "\n",
        "The `accelerate` configuration file is essentially a recipe that tells the training script how to orchestrate the orchestra. Internally, Accelerate parses the YAML, sets up the `accelerate.Accelerator` object, and injects the correct device map, precision, and distributed backend. When you run `accelerate launch train.py`, the CLI spawns one process per GPU, each process loads the same script, but the `Accelerator` ensures that each process only sees its own GPU and that gradients are synchronized across all processes. This design keeps the code simple while still leveraging the full power of multiâ€‘GPU training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1ï¸âƒ£  Create an accelerate config file programmatically\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import os\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Define the config dictionary\n",
        "config = {\n",
        "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
        "    \"deepspeed_config\": None,\n",
        "    \"distributed_type\": \"MULTI_GPU\",\n",
        "    \"fp16\": True,  # use mixedâ€‘precision\n",
        "    \"bf16\": False,\n",
        "    \"zero_stage\": 0,  # no ZeRO optimization\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"log_with\": \"tensorboard\",\n",
        "    \"logging_dir\": \"./logs\",\n",
        "    \"mixed_precision\": \"fp16\",\n",
        "    \"num_processes\": 2,  # adjust to your GPU count\n",
        "    \"process_index\": 0,\n",
        "    \"use_cpu\": False,\n",
        "    \"use_mps\": False,\n",
        "    \"use_cuda\": True,\n",
        "    \"use_torch_distributed\": True\n",
        "}\n",
        "\n",
        "# Write to accelerate_config.yaml\n",
        "config_path = \"accelerate_config.yaml\"\n",
        "with open(config_path, \"w\") as f:\n",
        "    import yaml\n",
        "    yaml.safe_dump(config, f)\n",
        "\n",
        "print(f\"Accelerate config written to {config_path}\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2ï¸âƒ£  Minimal training script (train.py)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Save this as train.py in the same directory as the config file.\n",
        "\n",
        "# Note: This is a minimal example; for full training youâ€™ll need a data loader, optimizer, etc.\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Reproducibility seed\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Initialize accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Load tokenizer and 8â€‘bit model\n",
        "MODEL_NAME = \"TheBloke/gpt-oss-20b-gptq-4bit-128g\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Dummy dataset: replace with your processed dataset\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __len__(self): return 1000\n",
        "    def __getitem__(self, idx):\n",
        "        text = \"Once upon a time\"\n",
        "        enc = tokenizer(text, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        return {\"input_ids\": enc[\"input_ids\"].squeeze(), \"labels\": enc[\"input_ids\"].squeeze()}\n",
        "\n",
        "train_dataset = DummyDataset()\n",
        "\n",
        "# Prepare training arguments (Accelerator will override some settings)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=0,\n",
        ")\n",
        "\n",
        "# Wrap everything with accelerator\n",
        "model, train_dataset, training_args = accelerator.prepare(\n",
        "    model, train_dataset, training_args\n",
        ")\n",
        "\n",
        "# Trainer (you can also use a custom training loop)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the fineâ€‘tuned model\n",
        "trainer.save_model(\"./fine_tuned_gpt_oss_20b\")\n",
        "\n",
        "print(\"Training complete and model saved.\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6\n",
        "\n",
        "Thinking...\n",
        ">We need to produce JSON for section 6. Must follow structure: section_number 6, title \"Step 6: Fineâ€‘Tuning with 8â€‘bit AdamW and Mixed Precision\". Content: array of cells: markdown and code. Must be 800-1000 tokens. Provide callouts array. estimated_tokens 1000. prerequisites_check array. next_section_hint.\n",
        ">\n",
        ">We need to produce content for fine-tuning with 8-bit AdamW and mixed precision. Use beginner-friendly ELI5 language with analogies, but precise technical terms. Add one extra ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>âš ï¸ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>âœ… Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>âŒ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which optimizer is most suitable for 8â€‘bit fineâ€‘tuning on a 20B model?\", [\"AdamW\",\"SGD\",\"RMSprop\",\"Adagrad\"], 0, \"AdamW with 8â€‘bit precision balances memory efficiency and convergence speed for largeâ€‘parameter models.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary benefit of gradient checkpointing?\", [\"Reduces GPU memory usage\",\"Speeds up inference\",\"Increases model accuracy\",\"Simplifies data preprocessing\"], 0, \"Gradient checkpointing trades compute for memory, enabling training of very large models on limited GPU resources.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime â†’ Change runtime type â†’ GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:23:43.213Z",
      "title": "Deploying and Fineâ€‘Tuning GPTâ€‘OSSâ€‘20B for Research Applications",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:23:43.221Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36â€“60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}