{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying and Fine‑Tuning GPT‑OSS‑20B for Research Applications\n\nThis lesson guides advanced practitioners through the end‑to‑end workflow of deploying the GPT‑OSS‑20B model, from environment setup to efficient fine‑tuning on custom corpora. It covers architectural trade‑offs, memory‑efficient inference, and best practices for reproducibility in research settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:23:43.221Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain the architectural differences between GPT‑OSS‑20B and other large‑language‑model variants.\n",
        "2. Configure a GPU‑optimized environment that supports 20B‑parameter inference and training.\n",
        "3. Implement memory‑efficient fine‑tuning using gradient checkpointing and 8‑bit optimizers.\n",
        "4. Evaluate model performance on domain‑specific benchmarks and document reproducibility metadata.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+ with pip\n",
        "- PyTorch 2.0+ (CUDA 11.8 or higher)\n",
        "- Basic familiarity with Hugging Face Transformers and Jupyter notebooks\n",
        "- Access to a GPU with ≥24 GB VRAM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 torch==2.0.0+cu118 transformers==4.40.0 accelerate==0.24.0 bitsandbytes==0.41.0 datasets==2.20.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"torch==2.0.0+cu118\",\"transformers==4.40.0\",\"accelerate==0.24.0\",\"bitsandbytes==0.41.0\",\"datasets==2.20.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Validation and GPU Profiling\n",
        "\n",
        "Before we can play with a 20‑billion‑parameter model, we need to make sure our playground (the GPU) is ready to host it. Think of the GPU as a giant kitchen: the model is a huge recipe that requires a lot of space (memory) and a steady supply of ingredients (compute). If the kitchen is too small or the stove is too weak, the recipe will burn.\n",
        "\n",
        "In this step we will:\n",
        "\n",
        "1. **Verify the software stack** – confirm that the correct versions of PyTorch, CUDA, and Hugging Face libraries are installed.\n",
        "2. **Check GPU availability** – ensure that the GPU we intend to use is visible to PyTorch and that its memory capacity meets the 20B requirement.\n",
        "3. **Profile memory usage** – run a quick inference pass with a small prompt to see how much VRAM is actually consumed.\n",
        "4. **Set reproducibility seeds** – lock down random number generators so that experiments can be repeated exactly.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **CUDA**: NVIDIA’s parallel computing platform. It turns the GPU into a fast math engine.\n",
        "- **VRAM**: Video RAM, the memory that lives on the GPU. Large models need a lot of VRAM to store parameters and intermediate activations.\n",
        "- **Gradient Checkpointing**: A technique that trades extra computation for lower memory usage by recomputing certain activations during back‑propagation.\n",
        "- **8‑bit Quantization**: Reducing the precision of model weights from 32‑bit floats to 8‑bit integers, cutting memory by ~4× while keeping accuracy largely intact.\n",
        "- **Reproducibility Seed**: A fixed integer that initializes all random number generators (Python, NumPy, PyTorch) so that the same sequence of random numbers is produced every run.\n",
        "\n",
        "### Why These Checks Matter\n",
        "\n",
        "- **Avoid Runtime Crashes**: If the GPU is not detected or the VRAM is insufficient, the notebook will crash mid‑run, wasting time.\n",
        "- **Performance Tuning**: Knowing the exact memory footprint lets us decide whether to enable gradient checkpointing or 8‑bit quantization.\n",
        "- **Scientific Rigor**: Reproducibility seeds ensure that results can be verified by others, a cornerstone of research.\n",
        "\n",
        "### Trade‑offs\n",
        "\n",
        "- **Speed vs Memory**: Enabling gradient checkpointing or 8‑bit quantization reduces memory but increases compute time. For quick prototyping, you might skip checkpointing; for full‑scale fine‑tuning, you’ll need it.\n",
        "- **Precision vs Efficiency**: 8‑bit quantization saves memory but can introduce a tiny drop in accuracy. For most research tasks, the trade‑off is negligible.\n",
        "\n",
        "Now let’s put these ideas into practice with a couple of short code snippets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 1️⃣  Verify software versions and GPU visibility\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "import torch, transformers, accelerate, bitsandbytes, datasets\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "print(f\"Bitsandbytes version: {bitsandbytes.__version__}\")\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "\n",
        "# Check GPU count and properties\n",
        "gpu_count = torch.cuda.device_count()\n",
        "print(f\"Number of GPUs detected: {gpu_count}\")\n",
        "for i in range(gpu_count):\n",
        "    prop = torch.cuda.get_device_properties(i)\n",
        "    print(f\"GPU {i}: {prop.name}, VRAM: {prop.total_memory / (1024**3):.2f} GB\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 2️⃣  Quick VRAM profiling with a tiny prompt\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Use a small 8‑bit quantized model for the test\n",
        "model_name = \"TheBloke/GPT-OSS-20B-GPTQ-4bit-128g\"\n",
        "# Note: replace with your actual 8‑bit checkpoint if different\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# Load model with 8‑bit quantization (requires bitsandbytes)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Simple prompt\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# Run inference and measure peak memory\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "peak_mem = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "print(f\"Peak VRAM used for inference: {peak_mem:.2f} GB\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 3️⃣  Set reproducibility seeds\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "import random, numpy as np\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "print(f\"Reproducibility seed set to {seed}\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Loading GPT‑OSS‑20B with 8‑bit Quantization\n",
        "\n",
        "Imagine you have a gigantic library (the 20‑billion‑parameter model) that you want to read on a small laptop (your GPU). The library is too big to fit in the laptop’s memory, so you decide to shrink each book by a factor of four: you keep only the most important words and drop the rest. That’s what **8‑bit quantization** does – it compresses the model weights from 32‑bit floating‑point numbers to 8‑bit integers, cutting the memory footprint by roughly 75 % while keeping the story (the model’s predictions) almost unchanged.\n",
        "\n",
        "In this step we’ll:\n",
        "\n",
        "1. **Pull the tokenizer** – the piece of software that turns text into numbers the model can understand.\n",
        "2. **Load the model with 8‑bit weights** – using the `bitsandbytes` library, which hooks into PyTorch to perform the compression on the fly.\n",
        "3. **Map the model to the GPU** – automatically splitting the layers across available devices so we don’t run out of VRAM.\n",
        "4. **Verify the memory savings** – by running a tiny inference pass and printing the peak VRAM usage.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **8‑bit Quantization**: Converting 32‑bit floating‑point weights to 8‑bit integers. This reduces memory usage by ~4× but introduces a small quantization error.\n",
        "- **bitsandbytes**: A PyTorch extension that implements efficient 8‑bit (and 4‑bit) weight loading and inference, plus fast matrix multiplication kernels.\n",
        "- **device_map**: A dictionary that tells Hugging Face Transformers which GPU each layer should live on. Setting it to \"auto\" lets the library decide the best placement.\n",
        "- **torch_dtype**: The data type used for activations during inference. Using `torch.float16` keeps compute fast while still being accurate enough for most tasks.\n",
        "- **Peak VRAM**: The maximum amount of GPU memory allocated at any point during a run. Monitoring this helps ensure we stay within hardware limits.\n",
        "\n",
        "### Why 8‑bit Quantization Matters\n",
        "\n",
        "- **Memory Efficiency**: A 20‑B model normally needs 80 GB of VRAM in 32‑bit precision. With 8‑bit quantization, we can fit it on a single 24‑GB GPU.\n",
        "- **Speed**: 8‑bit kernels in bitsandbytes are often faster than 32‑bit ones because they use less memory bandwidth.\n",
        "- **Accuracy Trade‑off**: The quantization error is usually <1 % in perplexity for large language models, making it acceptable for research and many production scenarios.\n",
        "\n",
        "### Trade‑offs to Keep in Mind\n",
        "\n",
        "| Aspect | 32‑bit | 8‑bit | What to Expect |\n",
        "|--------|--------|-------|----------------|\n",
        "| VRAM | ~80 GB | ~20 GB | Huge savings |\n",
        "| Compute | Baseline | Slightly faster (less memory traffic) | Good for inference |\n",
        "| Accuracy | Gold standard | Minor drop (often <1 %) | Acceptable for most tasks |\n",
        "| Implementation | Simple | Requires bitsandbytes | Add a dependency |\n",
        "\n",
        "Now let’s see how to do this in code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 1️⃣  Load tokenizer and 8‑bit GPT‑OSS‑20B\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Replace with the exact checkpoint you want to use\n",
        "MODEL_NAME = \"TheBloke/gpt-oss-20b-gptq-4bit-128g\"\n",
        "\n",
        "# Load tokenizer (fast tokenizer is usually faster)\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Load model with 8‑bit weights via bitsandbytes\n",
        "print(\"Loading model with 8‑bit quantization…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # automatically place layers on GPUs\n",
        "    load_in_8bit=True,          # enable 8‑bit quantization\n",
        "    torch_dtype=torch.float16   # use FP16 activations for speed\n",
        ")\n",
        "\n",
        "print(\"Model loaded on device:\", next(model.parameters()).device)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 2️⃣  Quick VRAM profiling with a tiny prompt\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "with torch.no_grad():\n",
        "    _ = model.generate(**inputs, max_new_tokens=10)\n",
        "peak_mem = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "print(f\"Peak VRAM used for inference: {peak_mem:.2f} GB\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Memory‑Efficient Inference with Gradient Checkpointing\n",
        "\n",
        "### Why do we need it?\n",
        "When you run a 20‑B model on a single 24‑GB GPU, the forward pass alone can eat up most of the memory. Think of the model as a giant Lego set: each block (layer) needs to sit on the table (GPU memory) while you build the next block. If the table is too small, you have to keep moving blocks around, which slows you down. Gradient checkpointing is like a clever Lego‑stacking trick: you only keep a few blocks on the table at a time and rebuild the rest on the fly when you need them. The trade‑off is a bit more time, but you can fit the whole set on a smaller table.\n",
        "\n",
        "### How it works in PyTorch\n",
        "PyTorch’s `torch.utils.checkpoint` module lets you wrap a function (e.g., a transformer block) so that its intermediate activations are *not* stored during the forward pass. When the backward pass (or a manual recomputation) is triggered, the function is re‑executed to regenerate those activations. For inference, we can force a recomputation after each block to keep memory low, at the cost of extra compute.\n",
        "\n",
        "### Key Terms Explained\n",
        "- **Checkpointing**: The act of discarding intermediate activations during the forward pass and recomputing them later.\n",
        "- **Recomputation**: Running the same forward function again to regenerate activations that were not stored.\n",
        "- **Peak VRAM**: The maximum amount of GPU memory allocated at any point during a run.\n",
        "- **`torch.utils.checkpoint.checkpoint`**: A helper that automatically handles the discard‑and‑recompute logic.\n",
        "- **`accelerate` gradient‑checkpointing**: A higher‑level wrapper that applies checkpointing to all layers of a Hugging Face model.\n",
        "\n",
        "### Rationale & Trade‑offs\n",
        "| Aspect | 32‑bit / No Checkpointing | With Checkpointing |\n",
        "|--------|---------------------------|--------------------|\n",
        "| Memory | Uses full activations (≈20 GB for 8‑bit GPT‑OSS‑20B) | Stores only a few activations (≈5–10 GB) |\n",
        "| Compute | Baseline | Extra forward passes (≈2× slower) |\n",
        "| Accuracy | Unchanged | Unchanged (exact recomputation) |\n",
        "| Complexity | Simple | Requires wrapping layers or using `accelerate` |\n",
        "\n",
        "In research settings where GPU memory is the bottleneck, checkpointing is a lifesaver. For quick prototyping, you might skip it to save time.\n",
        "\n",
        "### Quick Code Demo\n",
        "Below we load the same 8‑bit GPT‑OSS‑20B model from Step 2 and wrap its transformer blocks with `torch.utils.checkpoint`. We also set a reproducibility seed and measure peak VRAM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 1️⃣  Reproducibility & imports\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "import random, numpy as np, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(f\"Reproducibility seed set to {SEED}\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 2️⃣  Load tokenizer & 8‑bit model (same as Step 2)\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "MODEL_NAME = \"TheBloke/gpt-oss-20b-gptq-4bit-128g\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(\"Loading model with 8‑bit quantization…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 3️⃣  Wrap transformer blocks with checkpointing\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# Hugging Face stores blocks in model.model.decoder.layers\n",
        "layers = model.model.decoder.layers\n",
        "for i, layer in enumerate(layers):\n",
        "    # Replace the forward method with a checkpointed version\n",
        "    orig_forward = layer.forward\n",
        "    def chkpt_forward(*args, **kwargs):\n",
        "        return checkpoint(orig_forward, *args, **kwargs)\n",
        "    layer.forward = chkpt_forward\n",
        "print(\"Applied checkpointing to all decoder layers.\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 4️⃣  Quick VRAM profiling with a tiny prompt\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "with torch.no_grad():\n",
        "    _ = model.generate(**inputs, max_new_tokens=10)\n",
        "peak_mem = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "print(f\"Peak VRAM used for inference with checkpointing: {peak_mem:.2f} GB\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Preparing Domain‑Specific Dataset with Hugging Face Datasets\n",
        "\n",
        "When you want a language model to speak like a cardiologist, a lawyer, or a software engineer, you need to give it a *diet* of text that reflects that specialty. Think of the model as a sponge that absorbs water (text). If you pour in only kitchen recipes, it will never learn how to talk about heart disease. The Hugging Face `datasets` library is the kitchen sink that lets you fetch, clean, and feed the right kind of water into the sponge.\n",
        "\n",
        "In this step we’ll:\n",
        "\n",
        "1. **Pull a domain‑specific corpus** from the Hugging Face Hub or a local file.\n",
        "2. **Split** it into training, validation, and test sets while preserving class balance.\n",
        "3. **Tokenize** the raw text with the same tokenizer we used to load GPT‑OSS‑20B.\n",
        "4. **Cache** the processed dataset for fast reuse.\n",
        "5. **Inspect** a few examples to sanity‑check the pipeline.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **Dataset**: A collection of data points (e.g., sentences, paragraphs) that the model will learn from.\n",
        "- **Tokenization**: The process of converting raw text into a sequence of integer IDs that the model can understand.\n",
        "- **Streaming**: Loading data lazily from disk or the internet to avoid memory overload.\n",
        "- **Cache**: Storing the processed dataset on disk so that subsequent runs skip the expensive preprocessing step.\n",
        "- **Shuffling**: Randomly reordering the data to prevent the model from learning spurious order effects.\n",
        "- **Batching**: Grouping multiple examples together to take advantage of GPU parallelism.\n",
        "\n",
        "### Why Domain‑Specific Datasets Matter\n",
        "\n",
        "- **Relevance**: The model learns the vocabulary, style, and facts that are most useful for your target audience.\n",
        "- **Bias Mitigation**: By curating the data, you can reduce unwanted stereotypes or misinformation.\n",
        "- **Performance**: Fine‑tuning on a focused corpus often yields higher accuracy on downstream tasks than generic data.\n",
        "\n",
        "### Trade‑offs to Keep in Mind\n",
        "\n",
        "| Decision | Memory | Speed | Quality |\n",
        "|----------|--------|-------|---------|\n",
        "| **Full‑text tokenization** | High (tokens are stored) | Slower (more passes) | Highest (no loss of context) |\n",
        "| **Chunking** (e.g., 512‑token windows) | Lower | Faster | Slightly lower (context truncated) |\n",
        "| **Streaming + on‑the‑fly tokenization** | Minimal | Fastest | Depends on implementation |\n",
        "\n",
        "In research, we usually favor full‑text tokenization with caching because it preserves the richest context while still being reproducible.\n",
        "\n",
        "### Extra Explanatory Paragraph\n",
        "\n",
        "The `datasets` library abstracts away many of the pain points of data handling. Internally it represents a dataset as a lazy, columnar structure that can be filtered, mapped, and split with declarative syntax. When you call `dataset.map(tokenize_function, batched=True)`, the library automatically parallelizes the operation across CPU cores and writes the results to a cache directory (`~/.cache/huggingface/datasets`). This means that the next time you run the notebook, the heavy tokenization step is skipped, saving you minutes or hours. The trade‑off is that the cache can grow large (hundreds of MBs), so you should monitor disk usage or clean the cache (`datasets.cleanup_cache_files()`) when necessary.\n",
        "\n",
        "Now let’s turn theory into practice with a couple of short code cells.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 1️⃣  Load a domain‑specific dataset (e.g., PubMed abstracts)\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "import os\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Example: PubMed abstracts from the Hugging Face Hub\n",
        "# Replace with your own dataset path or name if needed\n",
        "DATASET_NAME = \"allenai/pubmed_abstracts\"\n",
        "\n",
        "# Load the dataset with streaming to avoid memory overload\n",
        "print(\"Loading dataset…\")\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\", streaming=True)\n",
        "\n",
        "# Convert to a finite Dataset (we’ll take the first 10k examples for demo)\n",
        "print(\"Collecting 10,000 examples…\")\n",
        "train_data = dataset.take(10000)\n",
        "train_dataset = DatasetDict({\"train\": train_data})\n",
        "\n",
        "# Split into train/validation (90/10)\n",
        "print(\"Splitting into train/validation…\")\n",
        "train_split, val_split = train_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = DatasetDict({\"train\": train_split, \"validation\": val_split})\n",
        "\n",
        "print(f\"Train size: {len(train_dataset['train'])}\")\n",
        "print(f\"Validation size: {len(train_dataset['validation'])}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 2️⃣  Tokenize the dataset with the GPT‑OSS‑20B tokenizer\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the same tokenizer used for the model\n",
        "MODEL_NAME = \"TheBloke/gpt-oss-20b-gptq-4bit-128g\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Define a tokenization function\n",
        "def tokenize_function(examples):\n",
        "    # The dataset column is \"abstract\" for PubMed; adjust if yours differs\n",
        "    return tokenizer(examples[\"abstract\"], truncation=True, max_length=512)\n",
        "\n",
        "# Apply tokenization in batched mode for speed\n",
        "print(\"Tokenizing… (this may take a minute)\")\n",
        "train_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"abstract\"],  # keep only tokenized fields\n",
        "    num_proc=4,  # parallelize across 4 CPU cores\n",
        "    load_from_cache_file=True,  # reuse cached results if available\n",
        ")\n",
        "\n",
        "# Verify the first example\n",
        "print(\"First tokenized example:\")\n",
        "print(train_dataset['train'][0])\n",
        "\n",
        "# Save the processed dataset for later reuse\n",
        "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/datasets/processed_pubmed\")\n",
        "print(f\"Saving processed dataset to {CACHE_DIR}\")\n",
        "train_dataset.save_to_disk(CACHE_DIR)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Configuring Accelerate for Multi‑GPU Fine‑Tuning\n",
        "\n",
        "Imagine you’re a conductor leading an orchestra that spans several concert halls (GPUs). Each hall has its own acoustics (memory, compute), but you want the music (training) to sound seamless across all of them. Hugging Face **Accelerate** is the conductor’s score: it tells each hall when to play, how many instruments to assign, and how to share the sheet music (model parameters) so that the whole symphony stays in tune.\n",
        "\n",
        "In this step we’ll:\n",
        "\n",
        "1. **Create an `accelerate` configuration** that tells the library how many GPUs to use, what precision to run in, and whether to enable gradient checkpointing.\n2. **Generate a minimal training script** that imports the configuration, loads the 8‑bit GPT‑OSS‑20B, and starts distributed training.\n3. **Launch the training** with `accelerate launch`, which automatically handles device placement, mixed‑precision, and logging.\n4. **Verify reproducibility** by setting seeds and checking that the same random numbers are produced on every run.\n",
        "\n",
        "By the end of this section you’ll have a ready‑to‑run training pipeline that scales across multiple GPUs without writing any boilerplate distributed‑training code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Terms Explained\n",
        "\n",
        "- **Accelerate**: A lightweight library that abstracts away the complexities of distributed training, mixed‑precision, and device placement.\n",
        "- **`accelerate config`**: An interactive CLI that writes a YAML file (`accelerate_config.yaml`) describing the training environment (number of processes, GPUs, precision, etc.).\n",
        "- **Gradient Checkpointing**: A memory‑saving technique that recomputes activations during back‑propagation instead of storing them.\n",
        "- **Mixed‑Precision (FP16/FP8)**: Using lower‑precision arithmetic for activations and gradients to reduce memory bandwidth while maintaining model accuracy.\n",
        "- **Distributed Data Parallel (DDP)**: A PyTorch strategy that replicates the model on each GPU and synchronizes gradients automatically.\n",
        "- **Reproducibility Seed**: A fixed integer that seeds all random number generators (Python, NumPy, PyTorch) to ensure deterministic behavior.\n",
        "\n",
        "### Why Accelerate Matters\n",
        "\n",
        "1. **Zero Boilerplate** – You don’t need to write `torch.distributed.init_process_group()` or manually wrap your model with `DistributedDataParallel`.\n",
        "2. **Flexibility** – The same configuration works for single‑GPU, multi‑GPU, or even multi‑node setups.\n",
        "3. **Performance** – Accelerate automatically selects the best precision (FP16 or BF16) for your GPU and handles gradient accumulation.\n",
        "4. **Reproducibility** – By setting seeds in both the script and the config, you can guarantee that the same training run produces identical results.\n",
        "\n",
        "### Trade‑offs to Keep in Mind\n",
        "\n",
        "| Feature | Memory | Speed | Complexity |\n",
        "|---------|--------|-------|------------|\n",
        "| Gradient Checkpointing | ↓ | ↑ | Moderate (requires `gradient_checkpointing=True`) |\n",
        "| Mixed‑Precision | ↓ | ↑ | Low (handled by Accelerate) |\n",
        "| Distributed Data Parallel | ↓ (per‑GPU) | ↑ | Low (handled by Accelerate) |\n",
        "| Full‑Precision | ↑ | ↓ | None |\n",
        "\n",
        "In research, the combination of gradient checkpointing + mixed‑precision + DDP gives you the best memory‑speed trade‑off for a 20‑B model on a 24‑GB GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extra Explanatory Paragraph\n",
        "\n",
        "The `accelerate` configuration file is essentially a recipe that tells the training script how to orchestrate the orchestra. Internally, Accelerate parses the YAML, sets up the `accelerate.Accelerator` object, and injects the correct device map, precision, and distributed backend. When you run `accelerate launch train.py`, the CLI spawns one process per GPU, each process loads the same script, but the `Accelerator` ensures that each process only sees its own GPU and that gradients are synchronized across all processes. This design keeps the code simple while still leveraging the full power of multi‑GPU training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 1️⃣  Create an accelerate config file programmatically\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "import os\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Define the config dictionary\n",
        "config = {\n",
        "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
        "    \"deepspeed_config\": None,\n",
        "    \"distributed_type\": \"MULTI_GPU\",\n",
        "    \"fp16\": True,  # use mixed‑precision\n",
        "    \"bf16\": False,\n",
        "    \"zero_stage\": 0,  # no ZeRO optimization\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"log_with\": \"tensorboard\",\n",
        "    \"logging_dir\": \"./logs\",\n",
        "    \"mixed_precision\": \"fp16\",\n",
        "    \"num_processes\": 2,  # adjust to your GPU count\n",
        "    \"process_index\": 0,\n",
        "    \"use_cpu\": False,\n",
        "    \"use_mps\": False,\n",
        "    \"use_cuda\": True,\n",
        "    \"use_torch_distributed\": True\n",
        "}\n",
        "\n",
        "# Write to accelerate_config.yaml\n",
        "config_path = \"accelerate_config.yaml\"\n",
        "with open(config_path, \"w\") as f:\n",
        "    import yaml\n",
        "    yaml.safe_dump(config, f)\n",
        "\n",
        "print(f\"Accelerate config written to {config_path}\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 2️⃣  Minimal training script (train.py)\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# Save this as train.py in the same directory as the config file.\n",
        "\n",
        "# Note: This is a minimal example; for full training you’ll need a data loader, optimizer, etc.\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Reproducibility seed\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Initialize accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Load tokenizer and 8‑bit model\n",
        "MODEL_NAME = \"TheBloke/gpt-oss-20b-gptq-4bit-128g\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Dummy dataset: replace with your processed dataset\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __len__(self): return 1000\n",
        "    def __getitem__(self, idx):\n",
        "        text = \"Once upon a time\"\n",
        "        enc = tokenizer(text, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        return {\"input_ids\": enc[\"input_ids\"].squeeze(), \"labels\": enc[\"input_ids\"].squeeze()}\n",
        "\n",
        "train_dataset = DummyDataset()\n",
        "\n",
        "# Prepare training arguments (Accelerator will override some settings)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=0,\n",
        ")\n",
        "\n",
        "# Wrap everything with accelerator\n",
        "model, train_dataset, training_args = accelerator.prepare(\n",
        "    model, train_dataset, training_args\n",
        ")\n",
        "\n",
        "# Trainer (you can also use a custom training loop)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine‑tuned model\n",
        "trainer.save_model(\"./fine_tuned_gpt_oss_20b\")\n",
        "\n",
        "print(\"Training complete and model saved.\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6\n",
        "\n",
        "Thinking...\n",
        ">We need to produce JSON for section 6. Must follow structure: section_number 6, title \"Step 6: Fine‑Tuning with 8‑bit AdamW and Mixed Precision\". Content: array of cells: markdown and code. Must be 800-1000 tokens. Provide callouts array. estimated_tokens 1000. prerequisites_check array. next_section_hint.\n",
        ">\n",
        ">We need to produce content for fine-tuning with 8-bit AdamW and mixed precision. Use beginner-friendly ELI5 language with analogies, but precise technical terms. Add one extra ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which optimizer is most suitable for 8‑bit fine‑tuning on a 20B model?\", [\"AdamW\",\"SGD\",\"RMSprop\",\"Adagrad\"], 0, \"AdamW with 8‑bit precision balances memory efficiency and convergence speed for large‑parameter models.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary benefit of gradient checkpointing?\", [\"Reduces GPU memory usage\",\"Speeds up inference\",\"Increases model accuracy\",\"Simplifies data preprocessing\"], 0, \"Gradient checkpointing trades compute for memory, enabling training of very large models on limited GPU resources.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:23:43.213Z",
      "title": "Deploying and Fine‑Tuning GPT‑OSS‑20B for Research Applications",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:23:43.221Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}