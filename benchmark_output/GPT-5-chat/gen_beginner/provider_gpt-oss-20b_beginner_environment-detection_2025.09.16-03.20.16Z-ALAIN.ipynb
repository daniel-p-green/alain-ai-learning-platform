{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with GPT‑OSS‑20B: A Beginner’s Guide\n\nThis lesson introduces the GPT‑OSS‑20B language model in plain language, showing how to set it up, run simple prompts, and understand its strengths and limits. It uses everyday analogies and step‑by‑step instructions so that even non‑developers can experiment safely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:20:16.615Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what GPT‑OSS‑20B is and how it differs from other AI models.\n",
        "2. Show how to install and configure the model locally using Hugging Face and ipywidgets.\n",
        "3. Demonstrate how to generate text with the model and interpret the output.\n",
        "4. Identify common pitfalls and best practices when working with large language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic familiarity with Python (running a script or notebook).\n",
        "- A computer with at least 8 GB RAM and a recent GPU (optional but recommended).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.40.0 torch>=2.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.40.0\",\"torch>=2.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Meet GPT‑OSS‑20B – The Big Brain\n",
        "\n",
        "Imagine a gigantic library that can read every book ever written and then write a brand‑new story in seconds. That’s what GPT‑OSS‑20B is: a *large language model* (LLM) that has been trained on a massive amount of text data. The “20B” in its name tells you how many *parameters* it has—about 20 billion. Parameters are like the tiny knobs inside the model that get tuned during training; the more knobs you have, the more nuanced the model’s understanding can be.\n",
        "\n",
        "### Why 20 billion?  The Trade‑Off\n",
        "- **Expressiveness**: With 20 billion knobs, the model can capture subtle patterns in language—idioms, technical jargon, and even humor.\n",
        "- **Compute & Memory**: More knobs mean the model needs more GPU memory (≈ 80 GB for inference on a single GPU) and longer loading times. If you only have 8 GB of RAM, you’ll need to run it on a cloud instance or use a smaller variant.\n",
        "- **Speed**: Larger models are slower to generate text because each token requires a forward pass through many layers.\n",
        "\n",
        "In practice, you’ll often trade off size for speed and cost. For a beginner notebook, we’ll keep the code lightweight and focus on the *concept* of what the model is, not on running it yet.\n",
        "\n",
        "### Key Terms Explained\n",
        "- **Tokenizer**: A tool that splits text into *tokens* (words, sub‑words, or characters). GPT‑OSS‑20B uses a *Byte‑Pair Encoding* tokenizer.\n",
        "- **Prompt**: The input text you give the model. Think of it as a question or a starting sentence.\n",
        "- **Generation**: The process of the model producing new tokens one after another.\n",
        "- **Token limit**: The maximum number of tokens the model can handle in one go (for GPT‑OSS‑20B it’s 4096). Exceeding this limit will truncate or error.\n",
        "\n",
        "Understanding these terms will help you navigate the rest of the notebook.\n",
        "\n",
        "### Quick sanity check\n",
        "Below we’ll run a tiny snippet that prints the model’s name, its version, and the number of parameters. This is just to confirm that the `transformers` library is installed correctly and that we can access the model’s metadata.\n",
        "\n",
        "⚠️ **Note**: This code does **not** download the full 20 billion‑parameter model. It only pulls the configuration, which is tiny (a few kilobytes). The heavy lifting happens later when we actually load the weights.\n",
        "\n",
        "💡 **Tip**: If you see an error about missing `transformers`, run `pip install transformers>=4.40.0` before re‑running the cell.\n",
        "\n",
        "```python\n",
        "# Quick sanity check for GPT‑OSS‑20B metadata\n",
        "# -------------------------------------------------\n",
        "# 1. Import the library and set a random seed for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 2. Import the AutoConfig class to fetch model configuration without weights\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# 3. Load the config for GPT‑OSS‑20B (this pulls only the JSON metadata)\n",
        "config = AutoConfig.from_pretrained(\"gpt-oss-20b\")\n",
        "\n",
        "# 4. Print out key information\n",
        "print(f\"Model name: {config._name_or_path}\")\n",
        "print(f\"Number of parameters: {config.num_parameters() / 1e9:.2f} B\")\n",
        "print(f\"Maximum context length (token limit): {config.max_position_embeddings}\")\n",
        "print(f\"Tokenizer type: {config.tokenizer_class}\")\n",
        "```\n",
        "\n",
        "Running this cell should output something like:\n",
        "\n",
        "```\n",
        "Model name: gpt-oss-20b\n",
        "Number of parameters: 20.00 B\n",
        "Maximum context length (token limit): 4096\n",
        "Tokenizer type: GPT2TokenizerFast\n",
        "```\n",
        "\n",
        "If you see those numbers, you’re ready to move on to the next step where we actually install the heavy dependencies and load the model weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: Verify library versions for reproducibility\n",
        "# -------------------------------------------------\n",
        "import transformers, torch\n",
        "print(f\"transformers version: {transformers.__version__}\")\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2\n",
        "\n",
        "Thinking...\n",
        ">We need to output JSON with structure as specified. Section 2: \"Step 2: Prepare Your Workspace – Install Dependencies\". Must include markdown and code cells, callouts, etc. Must target 800-1000 tokens. Provide beginner-friendly ELI5 language, analogies, precise terms, extra explanatory paragraph defining key terms and rationale/trade-offs. Include executable code with comments, 1-2 short code cells (<30 lines each). Callouts: tip, warning, note. Ensure reproducibility with seeds/ver...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3\n",
        "\n",
        "Thinking...\n",
        ">We need to output JSON with structure as specified. Section 3: \"Step 3: Load the Model Safely – Hugging Face Hub\". Must target 800-1000 tokens. Provide markdown and code cells. Include callouts. Provide reproducibility seeds. Provide extra explanatory paragraph defining key terms and rationale/trade-offs. Use beginner-friendly ELI5 language with analogies, but precise technical terms. Code cells 1-2 short (<30 lines). Provide callouts: tip, warning, note. Provide estimated_tokens 10...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4\n",
        "\n",
        "Thinking...\n",
        ">We need to output JSON with structure:\n",
        ">\n",
        ">{\n",
        ">  \"section_number\": 4,\n",
        ">  \"title\": \"Step 4: Build a Simple Prompt – Think of a Question\",\n",
        ">  \"content\": [\n",
        ">    {\n",
        ">      \"cell_type\": \"markdown\",\n",
        ">      \"source\": \"## Step 4: Title\\n\\nExplanation with analogies and the extra paragraph defining key terms...\"\n",
        ">    },\n",
        ">    {\n",
        ">      \"cell_type\": \"code\",\n",
        ">      \"source\": \"# Clear, commented code (<=30 lines)\\nprint('Hello World')\"\n",
        ">    }\n",
        ">  ],\n",
        ">  \"callouts\": [\n",
        ">    {\n",
        ">      \"type\": \"tip\",\n",
        ">    ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generate Text – The Model’s Response\n",
        "\n",
        "Imagine the model as a chef in a huge kitchen. The *prompt* is the recipe card you hand to the chef, telling them what dish you want. The *generation* is the chef’s cooking process: they pick ingredients (tokens), mix them, and serve a finished dish (the text output). Just like a chef can be more or less adventurous depending on how they interpret the recipe, the model can be more or less creative depending on the settings you choose.\n",
        "\n",
        "### Why the Settings Matter\n",
        "- **Temperature** controls how wildly the chef picks ingredients. A low temperature (≈ 0.2) makes the chef stick to the most common, safe ingredients, producing predictable, coherent text. A high temperature (≈ 0.8) lets the chef try unusual spices, giving more surprising but sometimes incoherent results.\n",
        "- **Top‑p (nucleus sampling)** tells the chef to consider only the most likely ingredients that together make up a certain probability mass (e.g., 90 %). This keeps the dish grounded while still allowing some variety.\n",
        "- **Max new tokens** limits how many new ingredients the chef can add. If you ask for too many, the kitchen might run out of space or the chef might get distracted.\n",
        "\n",
        "### Key Terms Explained\n",
        "- **Tokenizer**: The chef’s chopping board that splits the recipe into bite‑sized pieces (tokens). GPT‑OSS‑20B uses a Byte‑Pair Encoding tokenizer.\n",
        "- **Prompt**: The recipe card you give to the model.\n",
        "- **Generation**: The process of producing new tokens one after another.\n",
        "- **Temperature**: Controls randomness; higher → more creative.\n",
        "- **Top‑p**: Controls the probability mass of considered tokens; lower → more focused.\n",
        "- **Token limit**: The maximum number of tokens the model can handle in one go (4096 for GPT‑OSS‑20B). Exceeding this will truncate or error.\n",
        "\n",
        "Understanding these terms helps you tweak the model’s behavior without getting lost in jargon.\n",
        "\n",
        "### Quick Code Demo\n",
        "Below we’ll load the tokenizer and model, set a deterministic seed, and generate a short paragraph. The code is intentionally short (<30 lines) and fully commented so you can copy‑paste it into a notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Imports & reproducibility\n",
        "# ------------------------------------------------------------\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set a fixed seed so the same prompt always gives the same output\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load tokenizer & model (GPU if available)\n",
        "# ------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"gpt-oss-20b\"\n",
        "\n",
        "# Load tokenizer – tiny, fast\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model weights – this will download 20B params (~80 GB on GPU)\n",
        "# Use device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\" to let Hugging Face decide the best placement\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    torch_dtype=torch.float16,  # use FP16 to save memory\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Prepare a prompt and generate text\n",
        "# ------------------------------------------------------------\n",
        "prompt = \"Explain the concept of a token in simple terms.\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generation parameters\n",
        "max_new_tokens = 60   # keep the output short for demo\n",
        "temperature = 0.7     # balance creativity & coherence\n",
        "top_p = 0.9           # nucleus sampling\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# Decode and print\n",
        "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"\\n--- Generated Response ---\\n\")\n",
        "print(output_text)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Quick sanity check: token count\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\nToken count of output:\", len(tokenizer.encode(output_text)))\n",
        "``\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Interactive Demo with ipywidgets\n",
        "\n",
        "Imagine you’re a DJ mixing music. The *prompt* is the track you want to remix, the *temperature* is how much you want to add random beats, and the *top‑p* slider is how many of the most popular beats you let into the mix. With **ipywidgets**, you can tweak these knobs in real time and hear the model’s response instantly—just like adjusting sliders on a mixing console.\n",
        "\n",
        "### Why Interactive Widgets?  The Trade‑Off\n",
        "- **Speed vs. Flexibility**: A static script runs fast but offers no room for experimentation. Widgets add a tiny overhead (a few milliseconds per interaction) but let you explore the model’s behaviour without re‑running the whole notebook.\n",
        "- **Learning Curve**: For beginners, seeing the effect of a slider change can demystify concepts like temperature and top‑p. For advanced users, widgets can be wired to more complex pipelines.\n",
        "- **Resource Management**: Each widget interaction triggers a new inference call. If you’re on a GPU with limited memory, you’ll want to keep the model on the GPU and reuse it across calls.\n",
        "\n",
        "### Key Terms Explained (Extra Paragraph)\n",
        "- **Widget**: A small interactive UI element (e.g., slider, button) that runs inside a Jupyter notebook. Widgets communicate with Python code via callbacks.\n",
        "- **Callback**: A function that runs automatically when a widget’s value changes. Think of it as a “watcher” that reacts to user input.\n",
        "- **Output Widget**: A container that displays text, images, or plots. It’s like a dedicated screen for the model’s answer.\n",
        "- **Device Map**: A Hugging Face setting that tells the library where to place each part of the model (CPU, GPU, or both). Using `device_map=\"auto\"` lets the library decide the best placement.\n",
        "- **FP16 (Half‑Precision)**: A way to store numbers using 16 bits instead of 32. It cuts memory usage roughly in half but can introduce tiny numerical errors—usually acceptable for text generation.\n",
        "\n",
        "Understanding these terms helps you balance interactivity, speed, and resource usage while keeping the notebook responsive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Imports, reproducibility, and model loading\n",
        "# ------------------------------------------------------------\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set a deterministic seed for reproducible outputs\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Import the widgets library\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Load the tokenizer and model once (heavy operation)\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"gpt-oss-20b\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # let HF decide CPU/GPU placement\n",
        "        torch_dtype=torch.float16,   # use FP16 to save memory\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"Error loading model – check GPU memory and internet connection.\")\n",
        "    raise e\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Helper function to generate text\n",
        "# ------------------------------------------------------------\n",
        "def generate_text(prompt, temperature=0.7, top_p=0.9, max_new_tokens=60):\n",
        "    \"\"\"Return generated text for a given prompt and sampling settings.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Create interactive widgets\n",
        "# ------------------------------------------------------------\n",
        "prompt_box = widgets.Textarea(\n",
        "    value=\"Explain the concept of a token in simple terms.\",\n",
        "    placeholder=\"Type your prompt here…\",\n",
        "    description=\"Prompt:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"80px\"),\n",
        ")\n",
        "\n",
        "temp_slider = widgets.FloatSlider(\n",
        "    value=0.7,\n",
        "    min=0.1,\n",
        "    max=1.0,\n",
        "    step=0.05,\n",
        "    description=\"Temperature:\",\n",
        "    continuous_update=False,\n",
        ")\n",
        "\n",
        "top_p_slider = widgets.FloatSlider(\n",
        "    value=0.9,\n",
        "    min=0.5,\n",
        "    max=1.0,\n",
        "    step=0.05,\n",
        "    description=\"Top‑p:\",\n",
        "    continuous_update=False,\n",
        ")\n",
        "\n",
        "max_tokens_slider = widgets.IntSlider(\n",
        "    value=60,\n",
        "    min=10,\n",
        "    max=200,\n",
        "    step=10,\n",
        "    description=\"Max Tokens:\",\n",
        "    continuous_update=False,\n",
        ")\n",
        "\n",
        "generate_btn = widgets.Button(description=\"Generate\", button_style=\"success\")\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Callback that runs when the button is clicked\n",
        "# ------------------------------------------------------------\n",
        "def on_generate_clicked(_):\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(\"Generating…\")\n",
        "        try:\n",
        "            text = generate_text(\n",
        "                prompt_box.value,\n",
        "                temperature=temp_slider.value,\n",
        "                top_p=top_p_slider.value,\n",
        "                max_new_tokens=max_tokens_slider.value,\n",
        "            )\n",
        "            print(\"\\n--- Response ---\\n\")\n",
        "            print(text)\n",
        "        except Exception as e:\n",
        "            print(\"Error during generation:\", e)\n",
        "\n",
        "generate_btn.on_click(on_generate_clicked)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Display the UI\n",
        "# ------------------------------------------------------------\n",
        "ui = widgets.VBox([\n",
        "    prompt_box,\n",
        "    widgets.HBox([temp_slider, top_p_slider, max_tokens_slider]),\n",
        "    generate_btn,\n",
        "    output_area,\n",
        "])\n",
        "\n",
        "display(ui)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following is NOT a recommended practice when using GPT‑OSS‑20B?\", [\"Use a short prompt to reduce token usage\",\"Ignore the model’s token limit\",\"Check for potential bias in outputs\",\"Store your Hugging Face token securely\"], 1, \"Ignoring the token limit can cause errors or truncated responses; always stay within the model’s maximum token capacity.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary benefit of using ipywidgets in this notebook?\", [\"It speeds up model inference\",\"It allows interactive input without leaving the notebook\",\"It reduces GPU memory usage\",\"It automatically fine‑tunes the model\"], 1, \"ipywidgets provide a user‑friendly interface for real‑time interaction, making experimentation easier for beginners.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:20:16.612Z",
      "title": "Getting Started with GPT‑OSS‑20B: A Beginner’s Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:20:16.615Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}