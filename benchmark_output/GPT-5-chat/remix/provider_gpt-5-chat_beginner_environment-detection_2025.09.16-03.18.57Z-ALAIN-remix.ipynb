{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Remix Note**: This ELI5 version was created with the Applied Learning AI Notebooks (ALAIN) Project on 09.14.2025.\\n",
        "Created by [Daniel Green](https://www.linkedin.com/in/danielpgreen).\\n",
        "---\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-5 Prompting Guide — ELI5 Remix\n\nThis beginner-friendly guide explains how to talk to GPT-5 so it understands you clearly and gives better answers. We’ll use simple analogies and avoid technical jargon, showing you how to adjust GPT-5’s behavior like tuning the knobs on a friendly robot helper. You’ll learn how to make GPT-5 more proactive or more cautious, how to give it clear instructions, and how to use its built-in tools effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:18:57.516Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Understand what prompting means and why it matters for GPT-5\n",
        "2. Learn how to control GPT-5’s eagerness in completing tasks\n",
        "3. Use tool preambles to improve clarity and collaboration with GPT-5\n",
        "4. Apply simple strategies to get consistent, high-quality responses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic familiarity with using chatbots\n",
        "- Access to GPT-5 via the OpenAI interface or API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Setup\n",
        "\n",
        "Imagine GPT-5 as a super-helpful robot friend who can talk, think, and write — but only if you tell it what you want in a way it understands. Just like giving directions to a friend who’s driving you somewhere, the clearer and more specific you are, the better the journey will be. This guide will show you how to \"drive\" GPT-5 by giving it the right prompts so it can give you great answers.\n",
        "\n",
        "Before we jump in, we need to set up our workspace, like making sure our robot has fresh batteries and a clear space to work. That means installing the right tools, having your API key ready, and confirming that your Python environment is ready to talk to GPT-5.\n",
        "\n",
        "**Key terms:**\n",
        "- **Prompt**: The instructions or question you give to GPT-5.\n",
        "- **API**: A way for your computer code to talk to GPT-5 over the internet.\n",
        "- **Environment Variable**: A secret storage spot for information (like your API key) that your code can use.\n",
        "- **Dependency**: Extra software your program needs to run (like `ipywidgets` for interactive features).\n",
        "\n",
        "**Rationale & trade-offs:** We’re using the OpenAI API instead of only the web app because it gives us more control and lets us automate tasks. The trade-off is that we must handle setup and security ourselves — for example, keeping our API key private. Installing `ipywidgets` will help us create interactive examples later, but if you don’t need interactivity, you could skip it. Still, we’ll include it here for completeness.\n",
        "\n",
        "📝 **Note:** These setup steps will only need to be done once per environment. If you’re using something like Google Colab, you’ll repeat them each new session.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Install dependencies safely\n",
        "# We wrap in a try/except so the notebook doesn't crash if already installed\n",
        "\n",
        "try:\n",
        "    import ipywidgets\n",
        "    import openai\n",
        "    print(\"✅ Dependencies already installed.\")\n",
        "except ImportError:\n",
        "    print(\"Installing required packages... This may take a minute.\")\n",
        "    !pip install ipywidgets>=8.0.0 openai --quiet\n",
        "    import ipywidgets\n",
        "    import openai\n",
        "    print(\"✅ Installation complete.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 2: Set and check your API key\n",
        "# 💡 Tip: Never hardcode your API key directly in shared notebooks.\n",
        "# Instead, store it in an environment variable for security.\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if the API key is already set in your environment\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    # Prompt the user securely (works best locally, not on public notebooks)\n",
        "    os.environ[\"OPENAI_API_KEY\"] = input(\"Enter your OpenAI API key: \").strip()\n",
        "\n",
        "# Confirm the key is set without printing it\n",
        "if os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    print(\"🔐 API key is set and ready to use.\")\n",
        "else:\n",
        "    raise EnvironmentError(\"❌ API key not set. Please set OPENAI_API_KEY before continuing.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: What is Prompting? (ELI5 Analogy)\n",
        "\n",
        "Imagine you have a magical talking chef in your kitchen — but this chef can cook *anything* in the world. The catch? They can’t read your mind. You have to tell them exactly what you want. If you just say, “Make food,” they might surprise you with something you didn’t expect (like a pineapple pizza with chocolate sauce!). But if you say, “Make spaghetti with tomato sauce, no cheese, and serve it in a deep bowl,” you’re *prompting* them with clear, detailed instructions.\n",
        "\n",
        "**Prompting** in GPT-5 works the same way: your words are the “recipe request” you give to this intelligent system. The better your request, the better the result.\n",
        "\n",
        "Another way to think about it is like using a search engine, but instead of getting a list of links, GPT-5 tries to *directly* create the answer for you. The way you phrase your request hugely influences what you get back.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Prompting Matters\n",
        "If you give a vague prompt, GPT-5 might guess what you meant — and sometimes it guesses wrong. If you give a precise prompt, GPT-5 has a clear target and is more likely to hit the bullseye. Prompting is basically the art of **telling GPT-5 what you want, how you want it, and sometimes even why you want it**.\n",
        "\n",
        "💡 Think of prompting like programming — but in plain English (or any language you choose). You’re writing instructions, and GPT-5 is your flexible but literal-minded assistant.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Terms and Rationale\n",
        "- **Prompt**: The text (question, instruction, or context) you give GPT-5 to generate a response.\n",
        "- **Prompt Engineering**: The skill of designing and refining prompts to reliably get the kind of responses you want.\n",
        "- **Context**: Extra information you include so GPT-5 understands the situation better.\n",
        "- **Trade-offs**: Short prompts are fast to write but can lead to vague answers. Long, detailed prompts give more control but take more effort and may limit GPT-5’s creativity.\n",
        "\n",
        "**Rationale:** We focus on prompting because it’s the *steering wheel* of your interaction with GPT-5. You can’t control the internal workings of the model, but you can control the inputs it sees. Just like a GPS needs the exact address, GPT-5 needs a clear prompt to take you where you want to go.\n",
        "\n",
        "---\n",
        "\n",
        "### A Quick Demo\n",
        "Let’s compare two prompts and see how GPT-5 might respond differently.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demo: Comparing vague vs. specific prompts with GPT-5\n",
        "# This example assumes you have set OPENAI_API_KEY in your environment.\n",
        "\n",
        "import os\n",
        "import openai\n",
        "\n",
        "# Ensure reproducibility (though large models can still vary slightly)\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Define two prompts\n",
        "vague_prompt = \"Tell me about space.\"\n",
        "specific_prompt = \"Explain in simple terms how black holes are formed, using a balloon analogy.\"\n",
        "\n",
        "# Function to get GPT-5 response\n",
        "def get_gpt5_response(prompt):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-5-chat\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7  # moderate creativity\n",
        "        )\n",
        "        return response.choices[0].message[\"content\"]\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "print(\"Vague prompt result:\\n\", get_gpt5_response(vague_prompt), \"\\n\")\n",
        "print(\"Specific prompt result:\\n\", get_gpt5_response(specific_prompt))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Understanding GPT-5’s Personality Knobs\n",
        "\n",
        "Think of GPT-5 like a friendly robot actor who can play many roles. Sometimes you might want it to be a serious teacher, other times a silly storyteller. These \"personality knobs\" are settings you can adjust — a bit like turning the volume, bass, and treble on a music player — to change *how* GPT-5 answers, not just *what* it says.\n",
        "\n",
        "The most common knobs you can control via the API are:\n",
        "- **Temperature**: Controls creativity vs. precision.\n",
        "- **Top-p**: Controls the diversity of word choices.\n",
        "- **System messages**: Set the role or style of GPT-5's responses.\n",
        "- **Max tokens**: Limits how long GPT-5's answer can be.\n",
        "\n",
        "Imagine you’re baking cookies: temperature is like oven heat — higher heat might give you crispier (more creative) cookies, lower heat gives you softer (more predictable) ones. Top-p is like deciding whether to use only your favorite ingredients or to try new ones from the pantry.\n",
        "\n",
        "---\n",
        "\n",
        "### The Main Knobs in Practice\n",
        "1. **Temperature** (`0.0` to `2.0`): Lower = more focused and deterministic answers. Higher = more creative and varied answers. For example, `0.2` might always give the same factual reply, while `1.0` might give different, imaginative takes each time.\n",
        "2. **Top-p** (`0.0` to `1.0`): Also called *nucleus sampling*. Instead of randomly picking from all words, GPT-5 picks from the top group whose probabilities add up to `p`. Lower values = safer, more common words. Higher = more variety.\n",
        "3. **System Message**: A special instruction at the start of the conversation telling GPT-5 “who it is” or “how to behave.” Example: _\"You are a patient math tutor who explains in simple steps.\"_\n",
        "4. **Max tokens**: The upper limit on response length. Shorter limits force concise answers, longer limits allow detailed ones.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Terms:**\n",
        "- **Sampling**: The method GPT-5 uses to choose the next word.\n",
        "- **Deterministic**: Always producing the same result from the same input.\n",
        "- **Stochastic**: Having randomness — can produce different results from the same input.\n",
        "\n",
        "**Rationale & Trade-offs:**\n",
        "Adjusting these knobs is about balancing creativity, accuracy, and control. Lower temperature and top-p values make GPT-5 more predictable — good for technical answers but possibly dull. Higher values encourage variety and creativity — great for brainstorming but risk going off-topic. The system message is your “director’s note” to GPT-5, guiding tone and perspective. Max tokens prevents overly long outputs but can cut off answers if too low.\n",
        "\n",
        "📝 **Note:** Temperature and top-p are usually adjusted together — but you can keep one at the default (e.g., top-p = 1.0) and only tweak the other for simplicity.\n",
        "\n",
        "---\n",
        "\n",
        "Below, we'll try the same prompt with different settings so you can “hear” the personality change.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demo: Tuning GPT-5's personality knobs\n",
        "# Assumes OPENAI_API_KEY is set in your environment\n",
        "\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "prompt = \"Describe a sunset over the ocean in one paragraph.\"\n",
        "\n",
        "# Helper function to get GPT-5's response\n",
        "def get_response(temp, top_p):\n",
        "    try:\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model=\"gpt-5-chat\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a poetic narrator.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=temp,\n",
        "            top_p=top_p,\n",
        "            max_tokens=80\n",
        "        )\n",
        "        return resp.choices[0].message[\"content\"]\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Compare low vs. high creativity\n",
        "print(\"\\nLow temperature (0.2), top_p=0.5:\\n\", get_response(0.2, 0.5))\n",
        "print(\"\\nHigh temperature (1.0), top_p=1.0:\\n\", get_response(1.0, 1.0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Controlling Agentic Eagerness — Calming or Energizing the Model\n",
        "\n",
        "Imagine GPT-5 as an enthusiastic helper in a workshop. Sometimes you want them to take initiative — grabbing tools, suggesting ideas, even starting tasks without being asked. Other times, you just want them to wait quietly until you give exact instructions. This \"agentic eagerness\" is like a dial you can turn up or down to match your needs.\n",
        "\n",
        "**Agentic eagerness** describes how proactive GPT-5 is in interpreting your request, filling in gaps, and proposing next steps. High eagerness can feel like working with a brainstorming partner who keeps tossing in suggestions. Low eagerness feels more like a careful clerk who follows your order to the letter without adding extras.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Adjust Eagerness?\n",
        "- **When to calm it down**: You need precise, factual, or compliance-heavy outputs — like legal summaries or data extraction.\n",
        "- **When to energize it**: You want creative suggestions, exploratory options, or help generating next steps without micromanaging.\n",
        "\n",
        "Think of eagerness like a dog on a leash: a short leash keeps it right by your side (low eagerness), while a long leash lets it roam and sniff around (high eagerness). Neither is \"better\" — it depends on the walk you want to take.\n",
        "\n",
        "---\n",
        "\n",
        "### How to Control It\n",
        "You can influence eagerness through:\n",
        "1. **System message tone** — e.g., _\"Only respond to direct questions\"_ vs. _\"Feel free to suggest ideas and ask clarifying questions.\"_\n",
        "2. **Temperature and top-p** — Lower values = calmer, more literal; higher values = more exploratory.\n",
        "3. **Explicit behavioral rules** — e.g., _\"Do not make assumptions beyond the provided data.\"_ or _\"Propose at least three alternative approaches.\"_\n",
        "\n",
        "---\n",
        "\n",
        "**Key Terms:**\n",
        "- **Agentic**: Acting with initiative or self-direction.\n",
        "- **Initiative**: Willingness to start tasks or make suggestions without being told.\n",
        "- **Proactivity**: Anticipating needs and acting before being prompted.\n",
        "\n",
        "**Rationale & Trade-offs:**\n",
        "High eagerness can save you time by surfacing options you didn’t think of, but it risks going off-topic or adding unwanted detail. Low eagerness keeps outputs tightly on-spec, but you might miss useful side ideas. The trick is to choose the right level for the task at hand — and you can adjust it mid-conversation if the situation changes.\n",
        "\n",
        "📝 **Note:** Eagerness is not a single API parameter — it’s an emergent behavior you shape through a mix of prompt wording, system messages, and sampling settings.\n",
        "\n",
        "---\n",
        "\n",
        "Below, we’ll try the same question with low vs. high eagerness settings so you can see the difference.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demo: Comparing low vs. high agentic eagerness\n",
        "# Assumes OPENAI_API_KEY is set in your environment\n",
        "\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "prompt = \"I need help planning a small birthday party for a friend.\"\n",
        "\n",
        "# Helper function to get GPT-5 response with eagerness style\n",
        "def get_eagerness_response(style):\n",
        "    if style == \"low\":\n",
        "        system_msg = \"You are a reserved assistant. Only answer the specific question asked, without adding extra ideas.\"\n",
        "        temp, top_p = 0.2, 0.5\n",
        "    else:  # high\n",
        "        system_msg = \"You are an enthusiastic event planner. Suggest creative ideas, ask clarifying questions, and propose next steps.\"\n",
        "        temp, top_p = 1.0, 1.0\n",
        "    try:\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model=\"gpt-5-chat\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_msg},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=temp,\n",
        "            top_p=top_p,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        return resp.choices[0].message[\"content\"]\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "print(\"--- Low Eagerness ---\\n\", get_eagerness_response(\"low\"))\n",
        "print(\"\\n--- High Eagerness ---\\n\", get_eagerness_response(\"high\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Prompting for Less Eagerness (When You Want Quick, Focused Answers)\n",
        "\n",
        "Imagine you’re ordering at a drive-thru. If you say, “I’ll have a burger,” and the cashier asks five follow-up questions about sauce, toppings, sides, and drinks, it slows things down. Sometimes, you just want them to take your exact order and hand it over — no extras. That’s what prompting for **less eagerness** does with GPT-5: it keeps the conversation short, precise, and on target.\n",
        "\n",
        "When GPT-5 is *less eager*, it:\n",
        "- Waits for your exact instructions before adding detail.\n",
        "- Avoids making assumptions or expanding beyond the facts you give.\n",
        "- Responds in a concise style without unnecessary elaboration.\n",
        "\n",
        "This is useful for:\n",
        "- Quick fact-checking.\n",
        "- Data extraction.\n",
        "- Step-by-step instructions where extra info could cause errors.\n",
        "- Compliance-heavy contexts where every word matters.\n",
        "\n",
        "---\n",
        "\n",
        "### How to Prompt for Less Eagerness\n",
        "To dial GPT-5 down, we combine:\n",
        "1. **System message constraints** — e.g., _\"Answer only the question asked, with no extra commentary.\"_\n",
        "2. **Low temperature & top-p** — e.g., temperature=0.2, top_p=0.5 for focused, consistent answers.\n",
        "3. **Explicit brevity instructions** — e.g., _\"Answer in one sentence\"_ or _\"Limit to 3 bullet points.\"_\n",
        "4. **Tight max_tokens** — to prevent long, wandering answers.\n",
        "\n",
        "Think of these like giving your robot assistant a checklist: *\"Stick to the list, no surprises.\"*\n",
        "\n",
        "---\n",
        "\n",
        "**Key Terms:**\n",
        "- **Constraint**: A rule that limits what GPT-5 can say.\n",
        "- **Brevity**: Keeping responses short and to the point.\n",
        "- **Max tokens**: A hard limit on the length of GPT-5’s output.\n",
        "\n",
        "**Rationale & Trade-offs:**\n",
        "A low-eagerness setup speeds up interactions and reduces irrelevant content, but can also make GPT-5 less helpful if you *do* need extra context. It’s like telling a chef to follow the recipe exactly — you get consistency, but no creative garnish. The trade-off is between accuracy/speed and richness of information.\n",
        "\n",
        "📝 **Note:** You can always start low-eagerness to get the core answer, then follow up with a higher-eagerness prompt for elaboration.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demo: Prompting GPT-5 for less eagerness\n",
        "# Assumes OPENAI_API_KEY is set in your environment\n",
        "\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Example prompt where we want a quick factual answer\n",
        "user_question = \"What is the capital of France?\"\n",
        "\n",
        "# System message to enforce low eagerness\n",
        "system_message = (\n",
        "    \"You are a precise assistant. Answer only the question asked, \"\n",
        "    \"in the fewest words possible, with no extra commentary.\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-5-chat\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_question}\n",
        "        ],\n",
        "        temperature=0.2,   # low creativity\n",
        "        top_p=0.5,         # limited diversity\n",
        "        max_tokens=10      # short answer cap\n",
        "    )\n",
        "    print(\"Low-eagerness answer:\", response.choices[0].message[\"content\"].strip())\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Prompting for More Eagerness (When You Want GPT-5 to Take the Lead)\n",
        "\n",
        "Imagine you’re on a road trip with a super-energetic friend who loves exploring. You might say, “Pick somewhere fun for lunch,” and they’ll not only choose a place, but also suggest a scenic route, a playlist, and a dessert stop. That’s **more eagerness** in action — GPT-5 taking initiative, filling gaps, and proposing next steps without you having to ask.\n",
        "\n",
        "When GPT-5 is *more eager*, it:\n",
        "- Proactively offers multiple ideas or solutions.\n",
        "- Asks clarifying questions to shape the goal.\n",
        "- Suggests related opportunities or next steps.\n",
        "- Adds creative flourishes or extra detail.\n",
        "\n",
        "This is useful for:\n",
        "- Brainstorming new projects.\n",
        "- Creative writing and storytelling.\n",
        "- Strategic planning with alternative options.\n",
        "- Learning explorations where you want tangents and insights.\n",
        "\n",
        "---\n",
        "\n",
        "### How to Prompt for More Eagerness\n",
        "To turn GPT-5 into your proactive co-pilot, combine:\n",
        "1. **Encouraging system messages** — e.g., _\"You are a highly proactive expert. Offer extra related ideas and suggestions without waiting for me to ask.\"_\n",
        "2. **Higher temperature & top-p** — e.g., temperature=1.0, top_p=1.0 for more diverse, creative outputs.\n",
        "3. **Explicit permission for initiative** — e.g., _\"If you see gaps, fill them with reasonable assumptions.\"_\n",
        "4. **Open-ended user prompts** — questions that invite exploration rather than a single fact.\n",
        "\n",
        "Think of it like giving your robot assistant a treasure map and saying, *\"Find the gold — and if you see any silver or gems along the way, grab those too.\"*\n",
        "\n",
        "---\n",
        "\n",
        "**Key Terms:**\n",
        "- **Open-ended prompt**: A question or instruction that allows for multiple possible directions.\n",
        "- **Initiative clause**: A sentence in your prompt explicitly telling GPT-5 it can act on its own judgment.\n",
        "- **Exploratory output**: A response that goes beyond the immediate question, offering related or novel ideas.\n",
        "\n",
        "**Rationale & Trade-offs:**\n",
        "High eagerness can spark creativity and uncover ideas you didn't think to ask for, but it also increases the risk of drifting off-topic or adding speculative content. This is like asking a chef for a \"surprise meal\" — you may discover amazing flavors, but there’s a chance you’ll get something unexpected. The trade-off is between discovery and control. You can always rein it in with follow-up prompts.\n",
        "\n",
        "📝 **Note:** Eagerness is shaped by context. If you start with a narrow, fact-based question, even a high-eagerness GPT-5 won’t go too far afield. To unlock its full proactivity, give it room to roam with your wording.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demo: Prompting GPT-5 for more eagerness\n",
        "# Assumes OPENAI_API_KEY is set in your environment\n",
        "\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Example open-ended request\n",
        "user_request = \"I want to start a weekend hobby. Suggest ideas and help me choose.\"\n",
        "\n",
        "# System message to encourage high eagerness\n",
        "system_message = (\n",
        "    \"You are an enthusiastic lifestyle coach. Offer multiple creative suggestions, \"\n",
        "    \"ask clarifying questions, and propose next steps without waiting for more input.\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-5-chat\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_request}\n",
        "        ],\n",
        "        temperature=1.0,   # high creativity\n",
        "        top_p=1.0,         # full variety\n",
        "        max_tokens=200     # allow space for richer ideas\n",
        "    )\n",
        "    print(\"High-eagerness answer:\\n\")\n",
        "    print(response.choices[0].message[\"content\"].strip())\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following best describes a 'tool preamble'?\", [\"A friendly introduction that explains what GPT-5 will do before it starts working\",\"A hidden code that unlocks GPT-5’s advanced features\",\"A short delay before GPT-5 processes your request\",\"A list of all GPT-5's available tools\"], 0, \"A tool preamble is an upfront explanation or plan from GPT-5 outlining how it will approach your request before using any tools.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Quick check 2: Basic understanding\", [\"A\",\"B\",\"C\",\"D\"], 0, \"Review the outline section to find the correct answer.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:18:57.509Z",
      "title": "GPT-5 Prompting Guide — ELI5 Remix",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "remix": true,
    "remix_source": "gpt-5_prompting_guide.ipynb",
    "remix_date": "2025-09-14",
    "remix_by": "Daniel Green",
    "remix_by_link": "https://www.linkedin.com/in/danielpgreen",
    "created_utc": "2025-09-16T03:18:57.516Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}