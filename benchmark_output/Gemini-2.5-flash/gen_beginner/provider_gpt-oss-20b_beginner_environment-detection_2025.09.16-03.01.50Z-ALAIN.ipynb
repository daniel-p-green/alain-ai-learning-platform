{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with GPT‑OSS‑20B: A Beginner’s Guide\n\nThis lesson introduces the GPT‑OSS‑20B language model in plain language, showing how to set it up, run simple prompts, and explore its capabilities—all without any coding experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:01:50.576Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what GPT‑OSS‑20B is and why it matters.\n",
        "2. Show how to install the required libraries and load the model.\n",
        "3. Demonstrate how to generate text with a simple prompt.\n",
        "4. Identify common pitfalls and how to avoid them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic computer skills (opening a terminal or command prompt).\n",
        "- A free or paid GPU-enabled environment (e.g., Google Colab, Kaggle, or a local GPU).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.30.0 torch>=2.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.30.0\",\"torch>=2.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Meet GPT‑OSS‑20B\n",
        "\n",
        "Imagine a gigantic library that has read every book, article, and conversation on the internet up to 2023. GPT‑OSS‑20B is that library, but instead of shelves and books, it’s a **transformer neural network** with **20 billion parameters**—the knobs that let it remember patterns in language. When you ask it a question, it flips through its internal pages and writes a response that feels like a human wrote it.\n",
        "\n",
        "### Why 20 billion?  What does that mean?\n",
        "* **Parameters** are the tiny weights inside the model that have been tuned during training. More parameters usually mean the model can capture more subtle patterns, but they also require more memory and compute to run.\n",
        "* **Tokens** are the building blocks of text—think of them as words or sub‑words. GPT‑OSS‑20B can handle up to 4 096 tokens in a single pass, which is enough for a short story or a detailed answer.\n",
        "* **Transformer architecture** uses self‑attention to weigh the importance of each token relative to every other token, allowing it to understand context far better than older models.\n",
        "\n",
        "### Trade‑offs you’ll encounter\n",
        "| Aspect | Benefit | Cost |\n",
        "|--------|---------|------|\n",
        "| **Large size** | More accurate, nuanced responses | Requires a GPU with at least 8 GB VRAM for inference |\n",
        "| **Fast inference** | Quick responses once loaded | Loading time can be several minutes on a modest GPU |\n",
        "| **High token limit** | Longer context windows | More memory per token |\n",
        "\n",
        "In short, GPT‑OSS‑20B is a powerful, general‑purpose language model that balances **accuracy** and **speed** for most beginner‑friendly tasks. It’s like having a super‑smart assistant that can write, explain, and even brainstorm for you.\n",
        "\n",
        "### Key terms you’ll see\n",
        "* **Transformer** – the neural network architecture that powers GPT‑OSS‑20B.\n",
        "* **Parameters** – the internal weights that the model learned during training.\n",
        "* **Tokens** – the smallest units of text the model processes.\n",
        "* **Inference** – the act of generating text from a prompt.\n",
        "* **GPU** – a graphics card that accelerates the heavy math needed for inference.\n",
        "\n",
        "Understanding these terms will help you troubleshoot and tweak the model later in the lesson.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick sanity check: import the library and print the model name\n",
        "# This cell is short and safe to run on any environment with internet access.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Set a random seed for reproducibility (not strictly needed for inference, but good practice)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load a tiny checkpoint just to confirm everything works\n",
        "# (Replace \"gpt-oss-20b\" with a smaller model if you don’t have a GPU)\n",
        "model_name = \"gpt-oss-20b\"\n",
        "print(f\"Attempting to load {model_name}…\")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "    print(\"✅ Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Failed to load the model. Check your internet connection and GPU availability.\")\n",
        "    print(e)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Prepare Your Workspace\n",
        "\n",
        "Before we can ask GPT‑OSS‑20B to write a story, we need to set up a clean, reproducible environment—just like a chef preparing a kitchen before cooking. Think of the notebook as a *recipe book* and the packages we install as the *ingredients*. If any ingredient is missing or out of date, the dish (our model run) will taste off.\n",
        "\n",
        "### What we’ll do in this step\n",
        "1. **Create a fresh notebook** – start from a blank cell so you can see every command.\n",
        "2. **Install the required libraries** – `transformers`, `torch`, and `ipywidgets`.\n",
        "3. **Set your Hugging Face token** – this is the key that unlocks the model files.\n",
        "4. **Enable widgets** – so future interactive demos work.\n",
        "5. **Verify GPU availability** – GPT‑OSS‑20B needs a GPU with at least 8 GB VRAM.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **pip** | Python’s package installer | Lets us fetch the latest libraries from PyPI | Requires internet and can be slow on older machines |\n",
        "| **HF_TOKEN** | Your personal Hugging Face access token | Authenticates downloads from the Hub | Must be kept secret; exposing it can lead to quota misuse |\n",
        "| **torch_dtype** | Data type for model weights (e.g., `float16`) | Reduces memory usage and speeds inference | Lower precision can slightly degrade output quality |\n",
        "| **ipywidgets** | Interactive UI components for Jupyter | Enables sliders, buttons, etc. | Adds a small runtime overhead |\n",
        "\n",
        "Balancing these trade‑offs ensures the model loads quickly, runs efficiently, and stays secure. For example, using `float16` saves memory but may introduce tiny rounding errors—usually negligible for text generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Install required packages (max 30 lines)\n",
        "#    We use `--quiet` to keep the output tidy.\n",
        "#    Wrap in a try/except so the notebook continues if a package is already installed.\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "def pip_install(package):\n",
        "    try:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "        except Exception as exc:\n",
        "            if IN_COLAB:\n",
        "                packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                if packages:\n",
        "                    try:\n",
        "                        import IPython\n",
        "                        ip = IPython.get_ipython()\n",
        "                        if ip is not None:\n",
        "                            ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                        else:\n",
        "                            import subprocess as _subprocess\n",
        "                            _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                    except Exception as colab_exc:\n",
        "                        print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                        raise\n",
        "                else:\n",
        "                    print('No packages specified for pip install; skipping fallback')\n",
        "            else:\n",
        "                raise\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"⚠️  Failed to install {package}: {e}\")\n",
        "\n",
        "packages = [\n",
        "    \"ipywidgets>=8.0.0\",\n",
        "    \"transformers>=4.30.0\",\n",
        "    \"torch>=2.0.0\",\n",
        "    \"huggingface_hub>=0.20.0\"\n",
        "]\n",
        "for pkg in packages:\n",
        "    pip_install(pkg)\n",
        "\n",
        "# 2️⃣ Enable widgets for future interactive cells\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"jupyter\", \"nbextension\", \"enable\", \"--py\", \"widgetsnbextension\", \"--sys-prefix\", \"--quiet\"])\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"⚠️  Could not enable widgets. They may already be enabled.\")\n",
        "\n",
        "# 3️⃣ Set your Hugging Face token (replace YOUR_TOKEN_HERE)\n",
        "import os\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    # For safety, we prompt the user but keep the token hidden.\n",
        "    from getpass import getpass\n",
        "    HF_TOKEN = getpass(\"Enter your Hugging Face token: \")\n",
        "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "# 4️⃣ Verify GPU availability\n",
        "import torch\n",
        "print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"✅ Number of GPUs: {torch.cuda.device_count()}\")\n",
        "print(f\"✅ Current device: {torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# 5️⃣ Set a random seed for reproducibility (not strictly needed for inference, but good practice)\n",
        "torch.manual_seed(42)\n",
        "print(\"🔑 Random seed set to 42 for reproducibility.\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3\n",
        "\n",
        "Thinking...\n",
        ">We need to output JSON structure with section_number 3, title \"Step 3: Load the Model Safely\", content array with markdown and code cells, callouts array, estimated_tokens 1000, prerequisites_check [\"item verified\"], next_section_hint preview. Must follow guidelines: 800-1000 tokens. Use beginner-friendly ELI5 language, analogies, precise terms. Add one extra explanatory paragraph defining key terms and explaining rationale/trade-offs. Include executable code with comments, 1-2 shor...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Try a Simple Prompt\n",
        "\n",
        "Imagine you’re a detective asking a very smart robot to write a short story. The robot has read a huge library of books, so it can generate text that sounds like a human wrote it. In this step we’ll give it a single line of text—called a prompt—and let it produce a few sentences.\n",
        "\n",
        "### How the prompt works\n",
        "\n",
        "Think of the prompt as a seed in a garden. The model takes that seed and grows a paragraph from it. The seed can be as short as “Once upon a time” or as long as a paragraph of context. The model then predicts the next token (word or sub‑word) one by one until it decides the story is finished or it reaches a limit.\n",
        "\n",
        "### Key terms and trade‑offs\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Prompt** | The text you give the model to start from | Sets the topic and style | Too short → generic; too long → may hit token limits |\n",
        "| **Temperature** | Controls randomness (0 = deterministic, >1 = more creative) | 0.7 gives balanced output; 1.5 is wild | High temp → more varied but less coherent |\n",
        "| **Max tokens** | The maximum number of tokens the model can generate | Prevents runaway generation | Lower limit saves memory but may cut off the answer |\n",
        "| **Seed** | Random number generator starting point | Makes results reproducible | Same seed → same output each run |\n",
        "| **Device** | CPU or GPU where the model runs | GPU speeds up inference | GPU memory limits how big the model can be |\n",
        "| **torch_dtype** | Data type for weights (e.g., float16) | Reduces memory usage | Lower precision can slightly degrade quality |\n",
        "\n",
        "Choosing the right temperature and max tokens is like adjusting the seasoning in a recipe: too little and the dish is bland; too much and it becomes overwhelming. In the code below we’ll use a moderate temperature of 0.7 and allow the model to generate up to 50 new tokens, which is enough for a short story or answer.\n",
        "\n",
        "### The code\n",
        "\n",
        "Below is a minimal, fully reproducible snippet that\n",
        "\n",
        "1. Loads the tokenizer and model (using `float16` for speed).\n",
        "2. Sets a fixed random seed.\n",
        "3. Generates text from a simple prompt.\n",
        "4. Prints the result.\n",
        "\n",
        "Feel free to change the prompt, temperature, or max tokens to see how the output changes.\n",
        "\n",
        "```python\n",
        "# 1️⃣ Imports and reproducibility\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "torch.manual_seed(42)  # Make results repeatable\n",
        "\n",
        "# 2️⃣ Load tokenizer and model (float16 for memory efficiency)\n",
        "model_name = \"gpt-oss-20b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"  # Let transformers decide the best device\n",
        ")\n",
        "\n",
        "# 3️⃣ Define a simple prompt\n",
        "prompt = \"Once upon a time, in a land of floating islands,\"\n",
        "\n",
        "# 4️⃣ Encode prompt and generate\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,   # Stop after 50 new tokens\n",
        "        temperature=0.7,     # Balanced creativity\n",
        "        top_p=0.95,          # Optional: nucleus sampling for diversity\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "# 5️⃣ Decode and print\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "```\n",
        "\n",
        "Run the cell and watch the model spin up a short continuation. If you hit a memory error, try lowering `max_new_tokens` or using a smaller model. If the output feels too generic, bump the temperature to 1.0 or 1.2. If it feels too chaotic, lower it to 0.5. Experimenting is the best way to learn how these knobs shape the model’s behavior.\n",
        "\n",
        "### What to expect\n",
        "\n",
        "The printed text will be a few sentences that continue the story. It might read something like:\n",
        "\n",
        "> “Once upon a time, in a land of floating islands, a young explorer named Liora discovered a hidden valley where the clouds whispered secrets. She followed the wind’s song, and the valley revealed a crystal lake that glowed with the colors of the sunrise…”\n",
        "\n",
        "Feel free to copy the prompt into a new cell and tweak it. The model is like a very eager storyteller—just give it a starting point and let it do its magic.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Imports and reproducibility\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "torch.manual_seed(42)  # Make results repeatable\n",
        "\n",
        "# 2️⃣ Load tokenizer and model (float16 for memory efficiency)\n",
        "model_name = \"gpt-oss-20b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # Let transformers decide the best device\n",
        ")\n",
        "\n",
        "# 3️⃣ Define a simple prompt\n",
        "prompt = \"Once upon a time, in a land of floating islands,\"\n",
        "\n",
        "# 4️⃣ Encode prompt and generate\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,   # Stop after 50 new tokens\n",
        "        temperature=0.7,     # Balanced creativity\n",
        "        top_p=0.95,          # Optional: nucleus sampling for diversity\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "# 5️⃣ Decode and print\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Play with Temperature and Max Tokens\n",
        "\n",
        "When you ask GPT‑OSS‑20B to write something, you’re basically giving it a *recipe* and letting it decide how to cook the dish. Two of the most important knobs in that recipe are **temperature** and **max tokens**. Think of temperature as the *spice level* and max tokens as the *serving size*.\n",
        "\n",
        "### Temperature – the spice level\n",
        "- **0.0**: The model is super conservative – it will almost always pick the most likely next word. The output is predictable but can feel bland.\n",
        "- **0.7–1.0**: A balanced spice level. The model still respects the prompt but occasionally chooses less obvious words, giving a bit of flair.\n",
        "- **>1.0**: The model gets wild. It will try many different words, which can produce creative but sometimes incoherent sentences.\n",
        "\n",
        "### Max Tokens – the serving size\n",
        "- **Small (e.g., 20–30 tokens)**: Quick, short answers. Great for FAQs or single‑sentence responses.\n",
        "- **Medium (e.g., 50–100 tokens)**: Short paragraphs or a brief story.\n",
        "- **Large (e.g., 200+ tokens)**: Full explanations, longer stories, or multi‑paragraph essays. Requires more GPU memory and can hit the model’s internal token limit.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Temperature** | Controls randomness of token selection | Higher values → more creative, lower values → deterministic | Too high → incoherent; too low → dull |\n",
        "| **Max tokens** | Upper bound on generated length | Prevents runaway generation and saves memory | Lower limit may truncate useful content |\n",
        "| **Top‑p (nucleus sampling)** | Alternative to temperature that limits the cumulative probability of chosen tokens | Adds diversity while keeping coherence | Requires tuning alongside temperature |\n",
        "| **Device map** | Where the model’s layers are placed (CPU vs GPU) | GPU speeds up inference | GPU memory limits how many layers can fit |\n",
        "\n",
        "Balancing these knobs is like seasoning a dish: a pinch of spice (temperature) can make a simple soup exciting, but too much can overwhelm the flavor. Similarly, setting a generous serving size (max tokens) lets the model tell a full story, but if you’re on a free GPU you might need to keep it modest to avoid out‑of‑memory errors.\n",
        "\n",
        "### Quick experiment\n",
        "Below we’ll run two small experiments: one that sweeps temperature while keeping the length fixed, and another that sweeps max tokens while keeping temperature fixed. Feel free to copy the code into a new cell and tweak the ranges.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Imports and reproducibility\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Set a fixed seed so the random choices are repeatable\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 2️⃣ Load tokenizer and model (float16 for speed, device_map auto)\n",
        "model_name = \"gpt-oss-20b\"\n",
        "print(\"Loading tokenizer and model…\")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"❌ Failed to load model. Make sure you have a GPU and HF_TOKEN set.\")\n",
        "    raise e\n",
        "\n",
        "# 3️⃣ Define a simple prompt\n",
        "prompt = \"Once upon a time, in a land of floating islands,\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "# 4️⃣ Temperature sweep (fixed max_new_tokens)\n",
        "print(\"\\nTemperature sweep (max_new_tokens=50):\")\n",
        "for temp in [0.3, 0.7, 1.2]:\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=50,\n",
        "            temperature=temp,\n",
        "            top_p=0.95,\n",
        "            do_sample=True\n",
        "        )\n",
        "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    print(f\"\\n--- Temperature {temp} ---\\n{text}\")\n",
        "\n",
        "# 5️⃣ Max tokens sweep (fixed temperature=0.7)\n",
        "print(\"\\nMax tokens sweep (temperature=0.7):\")\n",
        "for max_tok in [20, 50, 100]:\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_tok,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            do_sample=True\n",
        "        )\n",
        "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    print(f\"\\n--- Max tokens {max_tok} ---\\n{text}\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Avoid Common Pitfalls\n",
        "\n",
        "Running a 20 billion‑parameter model on a laptop or a free GPU can feel a bit like trying to bake a 10‑layer cake in a tiny oven. The cake (the model) is huge, the oven (your GPU) is limited, and if you’re not careful you’ll end up with a burnt mess or a cake that never finishes baking. In this step we’ll learn how to keep the cake from burning, how to avoid hitting the oven’s capacity, and how to handle a prompt that’s too long for a single bake.\n",
        "\n",
        "### 1️⃣ GPU Memory Limits\n",
        "- **Why it matters**: GPT‑OSS‑20B needs roughly 12 GB of VRAM just to hold the weights when you load it in `float16`. If your GPU has less than that, the model will either refuse to load or will start swapping data to the slower system RAM, which kills performance.\n",
        "- **Trade‑off**: Using `float32` gives you the most accurate numbers but doubles the memory usage. `float16` is a sweet spot for most inference tasks.\n",
        "- **What to do**: \n",
        "  * Check `torch.cuda.get_device_properties(0).total_memory`.\n",
        "  * If you’re short on memory, try `device_map=\"balanced\"` or `device_map=\"auto\"` so the library moves some layers to the CPU.\n",
        "  * For very tight budgets, consider the 6 billion‑parameter version of the model.\n",
        "\n",
        "### 2️⃣ Token Limits\n",
        "- **Why it matters**: The model can only process 4 096 tokens in one forward pass. If your prompt plus the desired output exceeds that, the model will truncate or raise an error.\n",
        "- **Trade‑off**: Shorter prompts mean faster generation but can lose context. Longer prompts give richer context but risk hitting the limit.\n",
        "- **What to do**: \n",
        "  * Use `tokenizer.encode(prompt, add_special_tokens=False)` to see how many tokens you’re using.\n",
        "  * If you need more context, split the prompt into chunks and feed them sequentially, keeping the last few tokens as a “memory” for the next chunk.\n",
        "\n",
        "### 3️⃣ Long Prompts and Sliding Windows\n",
        "- **Why it matters**: A prompt that is 3 000 tokens long leaves only 1 096 tokens for the model to generate. That might not be enough for a full answer.\n",
        "- **Trade‑off**: Sliding windows keep the most recent context but discard the earliest part, which can lose important background.\n",
        "- **What to do**: \n",
        "  * Keep a rolling buffer of the last 1 000 tokens.\n",
        "  * Use `model.generate` with `max_new_tokens` set so the total stays below 4 096.\n",
        "\n",
        "### 4️⃣ Avoiding Out‑of‑Memory (OOM) Errors\n",
        "- **Why it matters**: OOM errors stop your notebook and can leave your GPU in a bad state.\n",
        "- **Trade‑off**: Lowering `max_new_tokens` or using `torch_dtype=torch.float16` saves memory but may reduce output quality or length.\n",
        "- **What to do**: \n",
        "  * Wrap generation in a `try/except` block.\n",
        "  * If you hit OOM, reduce `max_new_tokens` or switch to a smaller model.\n",
        "\n",
        "### 5️⃣ Reproducibility and Randomness\n",
        "- **Why it matters**: Random seeds make debugging easier. Without a seed, the same prompt can produce different outputs.\n",
        "- **Trade‑off**: Setting `torch.manual_seed(42)` makes the output deterministic when `do_sample=False`, but if you want creative variation you’ll need to set `do_sample=True` and ignore the seed.\n",
        "- **What to do**: \n",
        "  * Use `torch.manual_seed(42)` at the start of your notebook.\n",
        "  * Keep `do_sample=True` only when you want varied outputs.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **VRAM** | Video RAM on your GPU | Holds model weights and activations | More VRAM = larger models, faster inference |\n",
        "| **Token** | Smallest unit of text the model processes | Determines context window size | More tokens = richer context but higher memory |\n",
        "| **float16** | 16‑bit floating point precision | Cuts memory usage in half | Slight loss of numerical precision |\n",
        "| **device_map** | Where each layer of the model lives (CPU vs GPU) | Balances speed and memory | CPU layers are slower but free up GPU memory |\n",
        "| **gradient checkpointing** | Recomputes activations during back‑prop to save memory | Useful for training, not inference | Adds compute overhead |\n",
        "\n",
        "Balancing these factors is like cooking a complex dish: you need the right amount of heat (GPU), the right amount of ingredients (tokens), and the right cooking time (generation length). If you over‑heat, the dish burns; if you under‑heat, it’s raw. The same applies to running GPT‑OSS‑20B—use the right settings, monitor your resources, and you’ll get smooth, high‑quality results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Quick sanity check: load model with memory‑friendly settings\n",
        "# This cell demonstrates how to guard against OOM and keep the prompt within limits.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Set reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load tokenizer and model with float16 and automatic device mapping\n",
        "model_name = \"gpt-oss-20b\"\n",
        "print(\"Loading tokenizer and model…\")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",  # let transformers decide best placement\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"❌ Failed to load model. Check GPU and HF_TOKEN.\")\n",
        "    raise e\n",
        "\n",
        "# 2️⃣ Define a long prompt (≈2500 tokens) – here we simulate with repetition\n",
        "prompt = (\"Once upon a time, in a land of floating islands, \" * 50).strip()\n",
        "print(f\"Prompt token length: {len(tokenizer.encode(prompt, add_special_tokens=False))}\")\n",
        "\n",
        "# 3️⃣ Keep only the last 1000 tokens to stay within 4096 limit\n",
        "max_context = 1000\n",
        "encoded = tokenizer.encode(prompt, add_special_tokens=False)\n",
        "context = encoded[-max_context:]\n",
        "input_ids = torch.tensor([context]).to(model.device)\n",
        "\n",
        "# 4️⃣ Generate with a safe max_new_tokens value\n",
        "max_new = 200  # keep total < 4096\n",
        "\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "        )\n",
        "    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    print(\"\\nGenerated text:\\n\", generated)\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e).lower():\n",
        "        print(\"⚠️ OOM detected. Try reducing max_new_tokens or using a smaller model.\")\n",
        "    else:\n",
        "        raise\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which parameter controls how random the model’s output is?\", [\"max_tokens\",\"temperature\",\"top_k\",\"batch_size\"], 1, \"Temperature adjusts randomness; higher values produce more varied outputs.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What does the 'max_tokens' parameter set?\", [\"Maximum length of input prompt\",\"Maximum length of generated output\",\"Batch size\",\"Learning rate\"], 1, \"max_tokens limits the number of tokens the model can generate in its response.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:01:50.570Z",
      "title": "Getting Started with GPT‑OSS‑20B: A Beginner’s Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:01:50.576Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}