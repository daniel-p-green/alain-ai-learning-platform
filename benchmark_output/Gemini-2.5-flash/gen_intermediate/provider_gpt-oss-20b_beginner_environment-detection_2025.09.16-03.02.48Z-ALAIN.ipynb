{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîß Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('üìù Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîê Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'üîè Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '‚àÖ'\\n    return v[:3] + '‚Ä¶' + v[-2:] if len(v) > 6 else '‚Ä¢‚Ä¢‚Ä¢'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üåê ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('‚úÖ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('‚ö†Ô∏è Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîé Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('‚ö†Ô∏è Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('‚úÖ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('‚ö†Ô∏è Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) ‚Äî 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying and Using GPT‚ÄëOSS‚Äë20B for Real‚ÄëWorld Applications\n\nThis lesson guides practitioners through the end‚Äëto‚Äëend workflow of loading, fine‚Äëtuning, and deploying the 20B‚Äëparameter GPT‚ÄëOSS model. It covers practical prompt engineering, inference optimization, and ethical considerations, enabling you to integrate GPT‚ÄëOSS into production pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ‚è±Ô∏è Estimated time to complete: 36‚Äì60 minutes (rough).  ",
        "\n> üïí Created (UTC): 2025-09-16T03:02:48.175Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Understand the architecture and key components of GPT‚ÄëOSS‚Äë20B.\n",
        "2. Load and run the model efficiently using Hugging Face Transformers and PyTorch.\n",
        "3. Apply prompt engineering and sampling techniques to generate high‚Äëquality text.\n",
        "4. Deploy a GPT‚ÄëOSS‚Äëbased inference service with Gradio and monitor its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic knowledge of PyTorch and Hugging Face Transformers.\n",
        "- Experience with Python notebooks and command‚Äëline tools.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 torch>=2.0.0 transformers>=4.40.0 datasets>=2.20.0 gradio>=4.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"torch>=2.0.0\",\"transformers>=4.40.0\",\"datasets>=2.20.0\",\"gradio>=4.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('‚úÖ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Environment Setup\n",
        "\n",
        "Welcome to the first step of our journey with GPT‚ÄëOSS‚Äë20B! Think of the model as a gigantic library of stories ‚Äì 20‚ÄØbillion words of knowledge ‚Äì that we want to read and write with. Before we can start pulling books from that library, we need to set up a *reading room* that can handle the size and speed of the library.\n",
        "\n",
        "### Why these libraries matter\n",
        "- **PyTorch** is the engine that runs the model‚Äôs math. It‚Äôs like the heavy‚Äëduty truck that moves the books.\n",
        "- **Transformers** is a high‚Äëlevel wrapper that knows how to talk to the truck and gives us a simple API.\n",
        "- **Datasets** helps us load and shuffle training data if we ever want to fine‚Äëtune.\n",
        "- **Gradio** lets us build a quick web interface to test the model.\n",
        "- **ipywidgets** powers interactive controls inside Jupyter.\n",
        "\n",
        "### The trade‚Äëoffs\n",
        "| Goal | Trade‚Äëoff | Why it matters |\n",
        "|------|-----------|----------------|\n",
        "| **Speed** | GPU memory usage | Larger batch sizes speed up inference but can exceed GPU RAM. |\n",
        "| **Reproducibility** | Random seeds | Setting a seed makes experiments repeatable, but some operations (e.g., CUDA kernels) still introduce nondeterminism. |\n",
        "| **Convenience** | High‚Äëlevel APIs | Easier to use but may hide low‚Äëlevel optimizations you could tweak later. |\n",
        "\n",
        "### Key terms defined\n",
        "- **GPU**: Graphics Processing Unit ‚Äì a parallel computer that accelerates matrix operations.\n",
        "- **Tokenizer**: Converts text into integer IDs that the model can understand.\n",
        "- **Inference**: Generating new text from a trained model.\n",
        "- **Prompt engineering**: Crafting the input text to steer the model‚Äôs output.\n",
        "\n",
        "With the environment ready, we can load the model, feed it prompts, and start generating!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Install required packages (run once per environment)\n",
        "# -------------------------------------------------------------\n",
        "# Using !pip ensures the command runs in the notebook kernel.\n",
        "# We pin versions to guarantee reproducibility.\n",
        "!pip install --quiet ipywidgets>=8.0.0 torch>=2.0.0 transformers>=4.40.0 datasets>=2.20.0 gradio>=4.0.0\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Enable ipywidgets extension (only needed once per Jupyter install)\n",
        "# -------------------------------------------------------------\n",
        "try:\n",
        "    import subprocess, sys\n",
        "    subprocess.check_call([sys.executable, '-m', 'jupyter', 'nbextension', 'enable', '--py', 'widgetsnbextension'])\n",
        "except Exception as e:\n",
        "    print(\"Widget extension already enabled or failed to enable:\", e)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3Ô∏è‚É£  Basic sanity checks and reproducibility setup\n",
        "# -------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Ensure the Hugging Face token is available\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise EnvironmentError(\"HF_TOKEN environment variable not set. Please set it to your Hugging Face access token.\")\n",
        "\n",
        "# Set deterministic seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(\"‚úÖ Environment ready! GPU available:\", torch.cuda.is_available())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: GPT‚ÄëOSS‚Äë20B Architecture Overview\n",
        "\n",
        "Imagine you‚Äôre building a giant Lego city. Each Lego block is a tiny piece of computation, and the city‚Äôs layout is the *architecture* that tells you how to connect those blocks to create a functioning metropolis. GPT‚ÄëOSS‚Äë20B is that city, but instead of bricks, it uses *transformer layers*‚Äîa stack of attention heads and feed‚Äëforward networks that learn to predict the next word in a sentence.\n",
        "\n",
        "### The high‚Äëlevel blueprint\n",
        "\n",
        "1. **Embedding layer** ‚Äì Turns each token (a word or sub‚Äëword) into a dense vector. Think of it as a map that places every word in a multi‚Äëdimensional space.\n",
        "2. **Positional encoding** ‚Äì Adds a sense of order, so the model knows that ‚Äúthe cat‚Äù comes before ‚Äúsat‚Äù in a sentence. It‚Äôs like giving each block a GPS coordinate.\n",
        "3. **Transformer blocks (√ó 32)** ‚Äì Each block contains:\n",
        "   * **Multi‚Äëhead self‚Äëattention** ‚Äì Every token looks at every other token to decide what to focus on. Imagine a group of people in a room all talking to each other at once.\n",
        "   * **Feed‚Äëforward network** ‚Äì A small neural net that transforms the attended representation. Think of it as a tiny factory that refines the information.\n",
        "   * **Layer normalization & residual connections** ‚Äì Keeps the signal stable and lets gradients flow smoothly, like a safety net.\n",
        "4. **Output head** ‚Äì Projects the final hidden state back to vocabulary size to produce logits for the next token.\n",
        "\n",
        "### Why 20‚ÄØB parameters?\n",
        "\n",
        "The number of parameters is a rough measure of the model‚Äôs *capacity*‚Äîhow much nuance it can capture. A 20‚ÄØB‚Äëparameter model is like a city with 20‚ÄØbillion streets; it can represent a vast amount of language patterns. However, more parameters mean:\n",
        "\n",
        "- **Higher memory footprint** ‚Äì You need a GPU with at least 24‚ÄØGB VRAM for full‚Äëprecision inference.\n",
        "- **Longer training time** ‚Äì Each forward pass takes longer.\n",
        "- **Greater compute cost** ‚Äì Inference latency increases unless you use batching or optimizations.\n",
        "\n",
        "These trade‚Äëoffs are why we‚Äôll later discuss batching, quantization, and model parallelism.\n",
        "\n",
        "### Key terms defined (extra paragraph)\n",
        "\n",
        "- **Transformer** ‚Äì A neural architecture that relies on self‚Äëattention to model relationships between tokens, replacing recurrent layers.\n",
        "- **Self‚Äëattention** ‚Äì Each token attends to all others, producing a weighted sum of their representations.\n",
        "- **Feed‚Äëforward network (FFN)** ‚Äì A two‚Äëlayer MLP applied to each token independently, usually with a GELU activation.\n",
        "- **LayerNorm** ‚Äì Normalizes activations across the hidden dimension to stabilize training.\n",
        "- **Residual connection** ‚Äì Adds the input of a layer to its output, helping gradients flow.\n",
        "- **Positional encoding** ‚Äì Adds a deterministic signal that encodes token positions; GPT‚ÄëOSS uses *learned* positional embeddings.\n",
        "- **Logits** ‚Äì Raw, unnormalized scores for each token in the vocabulary; passed through softmax to get probabilities.\n",
        "\n",
        "Understanding these terms helps you read the model‚Äôs code and debug issues later.\n",
        "\n",
        "### Architectural trade‚Äëoffs\n",
        "\n",
        "| Design choice | Effect | Rationale |\n",
        "|---------------|--------|-----------|\n",
        "| **Depth (32 layers)** | Increases representational power | Deeper models capture more complex dependencies but are harder to train.\n",
        "| **Width (hidden size 16‚ÄØk)** | More expressive per layer | Wider layers allow richer token representations but consume more memory.\n",
        "| **Attention heads (32)** | Parallel attention patterns | More heads can model diverse relationships but add computational cost.\n",
        "| **LayerNorm vs. RMSNorm** | Stability vs. speed | LayerNorm is more stable but slightly slower; GPT‚ÄëOSS sticks with LayerNorm for reliability.\n",
        "\n",
        "These choices reflect a balance between *accuracy* (capturing language nuances) and *efficiency* (running on available hardware). In the next step, we‚Äôll see how to load this architecture with Hugging Face and run a quick inference demo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Inspect GPT‚ÄëOSS‚Äë20B architecture details\n",
        "# -------------------------------------------------------------\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# Load the configuration (no weights needed)\n",
        "config = AutoConfig.from_pretrained(\"gpt-oss-20b\", trust_remote_code=True)\n",
        "\n",
        "print(\"Model name:\", config.model_type)\n",
        "print(\"Number of layers (depth):\", config.num_hidden_layers)\n",
        "print(\"Hidden size (width):\", config.hidden_size)\n",
        "print(\"Number of attention heads:\", config.num_attention_heads)\n",
        "print(\"Intermediate size (FFN):\", config.intermediate_size)\n",
        "print(\"Vocabulary size:\", config.vocab_size)\n",
        "print(\"Positional embeddings: learned?\", config.is_decoder)\n",
        "\n",
        "# Quick sanity check: compute total parameter count from config\n",
        "# (actual count may differ slightly due to bias terms)\n",
        "param_estimate = (config.num_hidden_layers *\n",
        "                  (config.hidden_size * config.hidden_size * 3 +  # QKV\n",
        "                   config.hidden_size * config.intermediate_size +  # FFN\n",
        "                   config.hidden_size * config.hidden_size))\n",
        "print(\"Estimated parameters (in billions):\", round(param_estimate / 1e9, 2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the Model with Hugging Face\n",
        "\n",
        "### Why we need a *loader*\n",
        "Think of GPT‚ÄëOSS‚Äë20B as a gigantic, fully‚Äëassembled Lego set. The *loader* is the instruction manual that tells your computer how to pick up each block, place it on the right spot, and keep everything organized. In the world of deep learning, this manual is the **Hugging Face `transformers` library**.\n",
        "\n",
        "### The loading pipeline\n",
        "1. **Tokenizer** ‚Äì Turns raw text into a sequence of integer IDs that the model can understand. It‚Äôs like converting a sentence into a list of Lego block IDs.\n",
        "2. **Model** ‚Äì The neural network itself. We‚Äôll pull the pre‚Äëtrained weights from the Hugging Face Hub. The model lives on a device (CPU or GPU) that can perform the heavy math.\n",
        "3. **Device placement** ‚Äì We decide whether to run on CPU, single‚ÄëGPU, or multi‚ÄëGPU. This choice balances speed, memory, and cost.\n",
        "4. **Precision** ‚Äì FP32 (full precision) gives the best quality but uses more memory. FP16 or BF16 reduces memory and speeds up inference at a small quality cost.\n",
        "5. **Safety checks** ‚Äì We guard against out‚Äëof‚Äëmemory errors and ensure the model is ready for inference.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "- **Model**: A collection of layers and weights that maps input tokens to output logits. In GPT‚ÄëOSS‚Äë20B, the model is a 32‚Äëlayer transformer with 16‚ÄØk hidden size.\n",
        "- **Tokenizer**: A deterministic mapping from text to token IDs. It handles sub‚Äëword units (e.g., `‚ñÅthe`, `‚ñÅcat`).\n",
        "- **Device**: The hardware (CPU or GPU) where tensors are stored and operations executed.\n",
        "- **Precision**: Numerical format (FP32, FP16, BF16). Lower precision reduces memory and can accelerate inference but may slightly degrade output quality.\n",
        "- **Inference**: The process of feeding a prompt through the model to generate predictions.\n",
        "- **Trade‚Äëoffs**: Using FP16 or BF16 saves memory and speeds up inference, but may introduce small numerical differences. Running on CPU is cheap but slow; GPU is fast but requires a compatible card.\n",
        "\n",
        "### Practical tip\n",
        "When you first load a 20‚ÄØB‚Äëparameter model, you‚Äôll likely hit the GPU memory limit. The code below includes a graceful fallback to CPU if the GPU cannot hold the model. This keeps the notebook running even on modest hardware.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Import libraries and set reproducibility\n",
        "# -------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Set deterministic seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Choose device: GPU if available, else CPU\n",
        "# -------------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3Ô∏è‚É£  Load tokenizer (no heavy weights)\n",
        "# -------------------------------------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\", trust_remote_code=True)\n",
        "print(\"Tokenizer loaded ‚Äì vocab size:\", tokenizer.vocab_size)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4Ô∏è‚É£  Load model with optional precision handling\n",
        "# -------------------------------------------------------------\n",
        "# Try FP16 first; fall back to FP32 if OOM\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"gpt-oss-20b\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16,  # use FP16 for memory efficiency\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # automatically place layers on GPU\n",
        "    )\n",
        "    print(\"Model loaded in FP16 on GPU.\")\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"OOM detected ‚Äì falling back to FP32 on CPU.\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"gpt-oss-20b\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float32,\n",
        "            device_map=\"cpu\",\n",
        "        )\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "model.eval()  # set to inference mode\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5Ô∏è‚É£  Quick inference demo\n",
        "# -------------------------------------------------------------\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated text:\\n\", generated_text)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Tokenization and Prompt Engineering\n",
        "\n",
        "### Why tokenization matters\n",
        "Think of the model as a super‚Äëfast librarian who can only read *codes* instead of words. Tokenization is the process of turning your natural language into a sequence of these codes (integers). It‚Äôs like converting a sentence into a barcode that the librarian can scan.\n",
        "\n",
        "### The tokenizer as a translator\n",
        "- **Tokenizer**: A deterministic mapping from text to token IDs. It handles sub‚Äëword units (e.g., `‚ñÅthe`, `‚ñÅcat`).\n",
        "- **Vocabulary**: The set of all possible token IDs the model knows. For GPT‚ÄëOSS‚Äë20B, the vocab size is 50‚ÄØk.\n",
        "- **Special tokens**: `bos_token`, `eos_token`, `pad_token`, etc. They give the model context about the start, end, or padding of a sequence.\n",
        "\n",
        "### Prompt engineering 101\n",
        "Prompt engineering is the art of crafting the *input* so that the model‚Äôs output aligns with your intent. It‚Äôs like giving a recipe to a chef: the clearer the instructions, the better the dish.\n",
        "\n",
        "#### Common strategies\n",
        "1. **System + User + Assistant** format (inspired by OpenAI‚Äôs chat API). Example:\n",
        "   ```text\n",
        "   System: You are a helpful assistant.\n",
        "   User: What is the capital of France?\n",
        "   Assistant:\n",
        "   ```\n",
        "2. **Instruction + Context**: \"Write a short poem about the sea.\" The instruction tells the model *what* to do, the context gives *how*.\n",
        "3. **Few‚Äëshot examples**: Provide a few input‚Äëoutput pairs before the new prompt to bias the model toward a style.\n",
        "4. **Prompt length control**: Keep the total token count below the model‚Äôs context window (‚âà 32‚ÄØk tokens for GPT‚ÄëOSS‚Äë20B). Truncate or summarize longer inputs.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "- **Context window**: The maximum number of tokens the model can see at once. Exceeding it forces truncation, which can lose important information.\n",
        "- **Token budget**: The sum of prompt tokens + generated tokens must stay within the context window. Managing this budget is crucial for long‚Äëform generation.\n",
        "- **Trade‚Äëoffs**: Longer prompts give the model more guidance but consume more of the token budget, leaving fewer tokens for the answer. Shorter prompts save budget but risk ambiguity.\n",
        "- **Precision vs. speed**: Using `torch_dtype=torch.float16` reduces memory usage and speeds up inference, but may introduce tiny numerical differences that can affect rare token probabilities.\n",
        "- **Determinism**: Setting `temperature=0` and `top_k=1` makes generation deterministic, useful for debugging but less creative.\n",
        "\n",
        "### Practical takeaway\n",
        "- Always inspect the tokenized prompt to ensure it contains the expected special tokens.\n",
        "- Use `tokenizer.encode` and `tokenizer.decode` to convert between text and IDs.\n",
        "- Keep an eye on the token count: `len(input_ids[0])`.\n",
        "- When building a pipeline, wrap tokenization and prompt formatting in reusable functions.\n",
        "\n",
        "### Quick sanity check\n",
        "Below we‚Äôll load the tokenizer, inspect a sample prompt, and show how to truncate it to fit the context window.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Load tokenizer (already loaded in Step 3, but re‚Äëimport for safety)\n",
        "# -------------------------------------------------------------\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Use the same model name as before\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "print(\"Tokenizer loaded ‚Äì vocab size:\", tokenizer.vocab_size)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Helper: show tokenization details\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def show_tokens(text, max_display=20):\n",
        "    \"\"\"Print token IDs and decoded tokens for a given text.\"\"\"\n",
        "    ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "    print(f\"\\nOriginal text: {text}\\n\")\n",
        "    print(f\"Token IDs (first {max_display}):\", ids[:max_display])\n",
        "    print(\"Decoded tokens:\", tokenizer.convert_ids_to_tokens(ids[:max_display]))\n",
        "    print(\"Total tokens:\", len(ids))\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"System: You are a helpful assistant.\\nUser: Explain the concept of tokenization in simple terms.\\nAssistant:\"\n",
        "show_tokens(prompt)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3Ô∏è‚É£  Truncate to fit context window (32k tokens for GPT‚ÄëOSS‚Äë20B)\n",
        "# -------------------------------------------------------------\n",
        "CONTEXT_WINDOW = 32000\n",
        "\n",
        "def truncate_prompt(text, max_tokens=CONTEXT_WINDOW):\n",
        "    ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "    if len(ids) > max_tokens:\n",
        "        # Keep the last `max_tokens` tokens (most recent context)\n",
        "        truncated_ids = ids[-max_tokens:]\n",
        "        print(f\"Prompt truncated from {len(ids)} to {len(truncated_ids)} tokens.\")\n",
        "        return truncated_ids\n",
        "    return ids\n",
        "\n",
        "truncated_ids = truncate_prompt(prompt, max_tokens=CONTEXT_WINDOW)\n",
        "print(\"Truncated token count:\", len(truncated_ids))\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4Ô∏è‚É£  Simple prompt‚Äëengineering function\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def build_prompt(system, user, assistant_prefix=\"Assistant:\"):\n",
        "    \"\"\"Return a formatted prompt string with system, user, and assistant placeholders.\"\"\"\n",
        "    return f\"System: {system}\\nUser: {user}\\n{assistant_prefix}\"\n",
        "\n",
        "# Build a new prompt\n",
        "new_prompt = build_prompt(\n",
        "    system=\"You are a friendly tutor.\",\n",
        "    user=\"What is the capital of Japan?\"\n",
        ")\n",
        "print(\"\\nNew prompt:\\n\", new_prompt)\n",
        "\n",
        "# Encode and decode to verify\n",
        "ids = tokenizer.encode(new_prompt, add_special_tokens=True)\n",
        "print(\"Decoded back:\", tokenizer.decode(ids, skip_special_tokens=False))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generating Text with Sampling Strategies\n",
        "\n",
        "When you ask GPT‚ÄëOSS‚Äë20B to write something, you‚Äôre really asking it to *pick* the next word from a huge list of possibilities. Think of it like a game of \"pick a card from a deck\" ‚Äì the deck is the model‚Äôs probability distribution over the vocabulary, and the card you draw becomes the next token in the story.\n",
        "\n",
        "### The main ways to pick a card\n",
        "1. **Greedy (temperature‚ÄØ=‚ÄØ0, top‚Äëk‚ÄØ=‚ÄØ1)** ‚Äì always pick the highest‚Äëprobability card. It‚Äôs fast but can get stuck in repetitive loops.\n",
        "2. **Random sampling** ‚Äì pick any card weighted by its probability. It‚Äôs creative but can produce nonsense.\n",
        "3. **Top‚Äëk sampling** ‚Äì keep only the top‚ÄØk most probable cards and sample from that smaller deck. It balances creativity and safety.\n",
        "4. **Top‚Äëp (nucleus) sampling** ‚Äì keep the smallest set of cards whose cumulative probability exceeds *p* (e.g., 0.9). It adapts the deck size to the distribution‚Äôs shape.\n",
        "5. **Beam search** ‚Äì keep *n* best partial sentences at each step. It‚Äôs great for tasks that need high‚Äëquality, deterministic output (e.g., translation) but is slower.\n",
        "6. **Temperature scaling** ‚Äì adjust the softness of the probability distribution. A low temperature (<1) sharpens the deck (more deterministic), while a high temperature (>1) flattens it (more random).\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "- **Sampling**: The process of selecting the next token from the model‚Äôs probability distribution. It determines the trade‚Äëoff between *diversity* (many different outputs) and *coherence* (logical, fluent text).\n",
        "- **Temperature**: A scalar that raises or lowers the logits before softmax. Mathematically, `softmax(logits / temperature)`. Lower temperatures make the distribution peakier; higher temperatures make it flatter.\n",
        "- **Top‚Äëk**: The number of highest‚Äëprobability tokens retained for sampling. Setting `k=50` means the model will only consider the 50 most likely words.\n",
        "- **Top‚Äëp (nucleus)**: The cumulative probability threshold. For `p=0.9`, the model keeps the smallest set of tokens whose summed probability is at least 90‚ÄØ%.\n",
        "- **Beam width**: In beam search, the number of partial sequences kept at each step. A larger beam gives more exhaustive search but uses more memory and time.\n",
        "- **Repetition penalty**: A factor that reduces the probability of tokens that have already appeared, discouraging loops.\n",
        "- **Length penalty**: Adjusts the score of sequences based on their length, encouraging or discouraging longer outputs.\n",
        "\n",
        "**Trade‚Äëoffs**: Greedy is fast but can be dull; random sampling is creative but risky; top‚Äëk/top‚Äëp offer a sweet spot; beam search is accurate but slow. Temperature tweaks the balance between certainty and surprise. Choosing the right strategy depends on the task: creative writing, question answering, or formal translation each have different needs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Helper to generate text with a chosen strategy\n",
        "# -------------------------------------------------------------\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Re‚Äëuse the tokenizer and model from previous steps\n",
        "# (Assume they are already loaded as `tokenizer` and `model`)\n",
        "\n",
        "# Ensure reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Generation function\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def generate_text(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 100,\n",
        "    strategy: str = \"top_k\",\n",
        "    temperature: float = 0.7,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.9,\n",
        "    num_beams: int = 1,\n",
        "    repetition_penalty: float = 1.0,\n",
        "    length_penalty: float = 1.0,\n",
        "):\n",
        "    \"\"\"Generate text using the specified sampling strategy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt: str\n",
        "        Input text to start generation.\n",
        "    max_new_tokens: int\n",
        "        How many tokens to generate.\n",
        "    strategy: str\n",
        "        One of \"greedy\", \"random\", \"top_k\", \"top_p\", \"beam\".\n",
        "    temperature: float\n",
        "        Softmax temperature.\n",
        "    top_k: int\n",
        "        Keep only the top‚Äëk tokens for sampling.\n",
        "    top_p: float\n",
        "        Keep tokens until cumulative probability >= top_p.\n",
        "    num_beams: int\n",
        "        Beam width for beam search.\n",
        "    repetition_penalty: float\n",
        "        Penalize repeated tokens.\n",
        "    length_penalty: float\n",
        "        Adjust score based on length.\n",
        "    \"\"\"\n",
        "    # Tokenize prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(model.device)\n",
        "\n",
        "    # Build generation kwargs based on strategy\n",
        "    gen_kwargs = {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"repetition_penalty\": repetition_penalty,\n",
        "        \"length_penalty\": length_penalty,\n",
        "        \"do_sample\": strategy in {\"random\", \"top_k\", \"top_p\"},\n",
        "    }\n",
        "\n",
        "    if strategy == \"greedy\":\n",
        "        gen_kwargs.update({\"do_sample\": False})\n",
        "    elif strategy == \"random\":\n",
        "        gen_kwargs.update({\"do_sample\": True, \"top_k\": 0, \"top_p\": 1.0})\n",
        "    elif strategy == \"top_k\":\n",
        "        gen_kwargs.update({\"do_sample\": True, \"top_k\": top_k, \"top_p\": 1.0})\n",
        "    elif strategy == \"top_p\":\n",
        "        gen_kwargs.update({\"do_sample\": True, \"top_k\": 0, \"top_p\": top_p})\n",
        "    elif strategy == \"beam\":\n",
        "        gen_kwargs.update({\"do_sample\": False, \"num_beams\": num_beams})\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids, **gen_kwargs)\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3Ô∏è‚É£  Quick sanity check (will run in the next cell)\n",
        "# -------------------------------------------------------------\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 4Ô∏è‚É£  Demonstrate each strategy\n",
        "# -------------------------------------------------------------\n",
        "strategies = [\n",
        "    (\"greedy\", 0.0, 1, 1.0),\n",
        "    (\"random\", 1.0, 0, 1.0),\n",
        "    (\"top_k\", 0.7, 50, 1.0),\n",
        "    (\"top_p\", 0.7, 0, 0.9),\n",
        "    (\"beam\", 0.0, 0, 1.0),\n",
        "]\n",
        "\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "\n",
        "for name, temp, k, p in strategies:\n",
        "    if name == \"beam\":\n",
        "        text = generate_text(prompt, max_new_tokens=50, strategy=name, num_beams=5)\n",
        "    else:\n",
        "        text = generate_text(prompt, max_new_tokens=50, strategy=name, temperature=temp, top_k=k, top_p=p)\n",
        "    print(f\"\\n=== {name.upper()} ===\")\n",
        "    print(text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Fine‚ÄëTuning Basics (Optional)\n",
        "\n",
        "Fine‚Äëtuning is like giving a well‚Äëeducated student a new set of notes for a specific exam. The student already knows the language, grammar, and general facts, but now they need to learn how to answer questions about a particular domain‚Äîsay, medical guidelines or legal statutes. GPT‚ÄëOSS‚Äë20B is that student: it has a huge vocabulary and a deep understanding of language, but it may not know the *nuances* of your niche.\n",
        "\n",
        "### Why fine‚Äëtune?\n",
        "- **Domain adaptation** ‚Äì Tailor the model to your jargon, style, or compliance rules.\n",
        "- **Performance boost** ‚Äì Even a tiny amount of domain data can improve accuracy on specialized tasks.\n",
        "- **Control** ‚Äì You can steer the model away from undesirable outputs by exposing it to curated examples.\n",
        "\n",
        "### Key terms (extra paragraph)\n",
        "- **Dataset** ‚Äì A collection of text pairs (input, target) that the model learns from. In fine‚Äëtuning we often use *supervised* data.\n",
        "- **Tokenizer** ‚Äì Converts raw text into token IDs. The same tokenizer used for pre‚Äëtraining must be reused to keep the embedding space consistent.\n",
        "- **Training loop** ‚Äì The iterative process of feeding batches, computing loss, and updating weights.\n",
        "- **Gradient checkpointing** ‚Äì Saves memory by recomputing intermediate activations during back‚Äëpropagation.\n",
        "- **Learning rate** ‚Äì Controls how big a step the optimizer takes in weight space. Too high ‚Üí divergence; too low ‚Üí slow convergence.\n",
        "- **Batch size** ‚Äì Number of examples processed together. Larger batches give more stable gradients but require more GPU memory.\n",
        "- **Epoch** ‚Äì One full pass over the entire training dataset.\n",
        "- **Overfitting** ‚Äì When the model memorizes the training data and performs poorly on unseen data.\n",
        "\n",
        "### Trade‚Äëoffs\n",
        "| Decision | Memory | Speed | Accuracy | Risk |\n",
        "|----------|--------|-------|----------|------|\n",
        "| Full‚Äëprecision FP32 | High | Slow | Highest | Low |\n",
        "| Mixed‚Äëprecision FP16 | Medium | Faster | Slight drop | Low |\n",
        "| Gradient checkpointing | Low | Slower | Same | Medium |\n",
        "| Small dataset | Low | Fast | Lower | High (overfitting) |\n",
        "| Large dataset | High | Slow | Higher | Medium |\n",
        "\n",
        "Fine‚Äëtuning a 20‚ÄØB‚Äëparameter model on a single GPU is usually infeasible. In practice you‚Äôll either use **LoRA** (low‚Äërank adapters) or **parameter‚Äëefficient fine‚Äëtuning (PEFT)**, but for illustration we‚Äôll show a *minimal* training script that runs on a small subset of the dataset and uses gradient checkpointing to keep memory usage manageable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Imports, reproducibility, and device setup\n",
        "# -------------------------------------------------------------\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Load a tiny subset of a public dataset (e.g., WikiText)\n",
        "# -------------------------------------------------------------\n",
        "# We use only 1k examples to keep memory low.\n",
        "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
        "print(\"Dataset loaded ‚Äì number of examples:\", len(raw_datasets))\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3Ô∏è‚É£  Tokenizer and model (same as pre‚Äëtraining)\n",
        "# -------------------------------------------------------------\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "\n",
        "# Tokenizer ‚Äì keep the same vocab\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Model ‚Äì load in FP16 with gradient checkpointing to save memory\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",  # automatically place layers on GPU\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "model.eval()\n",
        "print(\"Model loaded ‚Äì hidden size:\", model.config.hidden_size)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4Ô∏è‚É£  Prepare data for language modeling\n",
        "# -------------------------------------------------------------\n",
        "# Tokenize the entire dataset in a batched way\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Data collator that masks tokens for causal LM\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5Ô∏è‚É£  Training arguments ‚Äì very small for demo purposes\n",
        "# -------------------------------------------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt-oss-finetune-demo\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,          # one epoch over 1% of WikiText\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,  # effective batch size 8\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6Ô∏è‚É£  Trainer ‚Äì the high‚Äëlevel training loop\n",
        "# -------------------------------------------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer ready ‚Äì starting fine‚Äëtuning‚Ä¶\")\n",
        "# Uncomment the next line to actually run training in a real environment\n",
        "# trainer.train()\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 7Ô∏è‚É£  Quick evaluation (on the same tiny dataset)\n",
        "# -------------------------------------------------------------\n",
        "# We‚Äôll just generate a short continuation to see the effect.\n",
        "prompt = \"The quick brown fox\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Use the same generation helper from Step 5\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "gen_config = GenerationConfig(\n",
        "    max_new_tokens=20,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(input_ids, generation_config=gen_config)\n",
        "\n",
        "generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated continuation after fine‚Äëtuning (demo):\")\n",
        "print(generated)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>‚ö†Ô∏è Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>‚úÖ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>‚ùå Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following is NOT a recommended sampling strategy for GPT‚ÄëOSS‚Äë20B?\", [\"Top‚Äëk sampling\",\"Temperature scaling\",\"Beam search\",\"Random sampling without constraints\"], 3, \"Random sampling without constraints can lead to incoherent outputs; recommended strategies include top‚Äëk, temperature, and beam search.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary benefit of using batch inference for GPT‚ÄëOSS‚Äë20B?\", [\"Reduces GPU memory usage\",\"Increases per‚Äëtoken latency\",\"Improves throughput by parallelizing token generation\",\"Simplifies model code\"], 2, \"Batching allows the model to process multiple prompts simultaneously, maximizing GPU utilization and throughput.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:02:48.166Z",
      "title": "Deploying and Using GPT‚ÄëOSS‚Äë20B for Real‚ÄëWorld Applications",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:02:48.175Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36‚Äì60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}