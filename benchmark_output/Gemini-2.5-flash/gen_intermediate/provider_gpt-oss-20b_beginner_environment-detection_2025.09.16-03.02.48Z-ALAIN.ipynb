{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying and Using GPT‑OSS‑20B for Real‑World Applications\n\nThis lesson guides practitioners through the end‑to‑end workflow of loading, fine‑tuning, and deploying the 20B‑parameter GPT‑OSS model. It covers practical prompt engineering, inference optimization, and ethical considerations, enabling you to integrate GPT‑OSS into production pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:02:48.175Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Understand the architecture and key components of GPT‑OSS‑20B.\n",
        "2. Load and run the model efficiently using Hugging Face Transformers and PyTorch.\n",
        "3. Apply prompt engineering and sampling techniques to generate high‑quality text.\n",
        "4. Deploy a GPT‑OSS‑based inference service with Gradio and monitor its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic knowledge of PyTorch and Hugging Face Transformers.\n",
        "- Experience with Python notebooks and command‑line tools.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 torch>=2.0.0 transformers>=4.40.0 datasets>=2.20.0 gradio>=4.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"torch>=2.0.0\",\"transformers>=4.40.0\",\"datasets>=2.20.0\",\"gradio>=4.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Environment Setup\n",
        "\n",
        "Welcome to the first step of our journey with GPT‑OSS‑20B! Think of the model as a gigantic library of stories – 20 billion words of knowledge – that we want to read and write with. Before we can start pulling books from that library, we need to set up a *reading room* that can handle the size and speed of the library.\n",
        "\n",
        "### Why these libraries matter\n",
        "- **PyTorch** is the engine that runs the model’s math. It’s like the heavy‑duty truck that moves the books.\n",
        "- **Transformers** is a high‑level wrapper that knows how to talk to the truck and gives us a simple API.\n",
        "- **Datasets** helps us load and shuffle training data if we ever want to fine‑tune.\n",
        "- **Gradio** lets us build a quick web interface to test the model.\n",
        "- **ipywidgets** powers interactive controls inside Jupyter.\n",
        "\n",
        "### The trade‑offs\n",
        "| Goal | Trade‑off | Why it matters |\n",
        "|------|-----------|----------------|\n",
        "| **Speed** | GPU memory usage | Larger batch sizes speed up inference but can exceed GPU RAM. |\n",
        "| **Reproducibility** | Random seeds | Setting a seed makes experiments repeatable, but some operations (e.g., CUDA kernels) still introduce nondeterminism. |\n",
        "| **Convenience** | High‑level APIs | Easier to use but may hide low‑level optimizations you could tweak later. |\n",
        "\n",
        "### Key terms defined\n",
        "- **GPU**: Graphics Processing Unit – a parallel computer that accelerates matrix operations.\n",
        "- **Tokenizer**: Converts text into integer IDs that the model can understand.\n",
        "- **Inference**: Generating new text from a trained model.\n",
        "- **Prompt engineering**: Crafting the input text to steer the model’s output.\n",
        "\n",
        "With the environment ready, we can load the model, feed it prompts, and start generating!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1️⃣  Install required packages (run once per environment)\n",
        "# -------------------------------------------------------------\n",
        "# Using !pip ensures the command runs in the notebook kernel.\n",
        "# We pin versions to guarantee reproducibility.\n",
        "!pip install --quiet ipywidgets>=8.0.0 torch>=2.0.0 transformers>=4.40.0 datasets>=2.20.0 gradio>=4.0.0\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2️⃣  Enable ipywidgets extension (only needed once per Jupyter install)\n",
        "# -------------------------------------------------------------\n",
        "try:\n",
        "    import subprocess, sys\n",
        "    subprocess.check_call([sys.executable, '-m', 'jupyter', 'nbextension', 'enable', '--py', 'widgetsnbextension'])\n",
        "except Exception as e:\n",
        "    print(\"Widget extension already enabled or failed to enable:\", e)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3️⃣  Basic sanity checks and reproducibility setup\n",
        "# -------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Ensure the Hugging Face token is available\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise EnvironmentError(\"HF_TOKEN environment variable not set. Please set it to your Hugging Face access token.\")\n",
        "\n",
        "# Set deterministic seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(\"✅ Environment ready! GPU available:\", torch.cuda.is_available())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: GPT‑OSS‑20B Architecture Overview\n",
        "\n",
        "Imagine you’re building a giant Lego city. Each Lego block is a tiny piece of computation, and the city’s layout is the *architecture* that tells you how to connect those blocks to create a functioning metropolis. GPT‑OSS‑20B is that city, but instead of bricks, it uses *transformer layers*—a stack of attention heads and feed‑forward networks that learn to predict the next word in a sentence.\n",
        "\n",
        "### The high‑level blueprint\n",
        "\n",
        "1. **Embedding layer** – Turns each token (a word or sub‑word) into a dense vector. Think of it as a map that places every word in a multi‑dimensional space.\n",
        "2. **Positional encoding** – Adds a sense of order, so the model knows that “the cat” comes before “sat” in a sentence. It’s like giving each block a GPS coordinate.\n",
        "3. **Transformer blocks (× 32)** – Each block contains:\n",
        "   * **Multi‑head self‑attention** – Every token looks at every other token to decide what to focus on. Imagine a group of people in a room all talking to each other at once.\n",
        "   * **Feed‑forward network** – A small neural net that transforms the attended representation. Think of it as a tiny factory that refines the information.\n",
        "   * **Layer normalization & residual connections** – Keeps the signal stable and lets gradients flow smoothly, like a safety net.\n",
        "4. **Output head** – Projects the final hidden state back to vocabulary size to produce logits for the next token.\n",
        "\n",
        "### Why 20 B parameters?\n",
        "\n",
        "The number of parameters is a rough measure of the model’s *capacity*—how much nuance it can capture. A 20 B‑parameter model is like a city with 20 billion streets; it can represent a vast amount of language patterns. However, more parameters mean:\n",
        "\n",
        "- **Higher memory footprint** – You need a GPU with at least 24 GB VRAM for full‑precision inference.\n",
        "- **Longer training time** – Each forward pass takes longer.\n",
        "- **Greater compute cost** – Inference latency increases unless you use batching or optimizations.\n",
        "\n",
        "These trade‑offs are why we’ll later discuss batching, quantization, and model parallelism.\n",
        "\n",
        "### Key terms defined (extra paragraph)\n",
        "\n",
        "- **Transformer** – A neural architecture that relies on self‑attention to model relationships between tokens, replacing recurrent layers.\n",
        "- **Self‑attention** – Each token attends to all others, producing a weighted sum of their representations.\n",
        "- **Feed‑forward network (FFN)** – A two‑layer MLP applied to each token independently, usually with a GELU activation.\n",
        "- **LayerNorm** – Normalizes activations across the hidden dimension to stabilize training.\n",
        "- **Residual connection** – Adds the input of a layer to its output, helping gradients flow.\n",
        "- **Positional encoding** – Adds a deterministic signal that encodes token positions; GPT‑OSS uses *learned* positional embeddings.\n",
        "- **Logits** – Raw, unnormalized scores for each token in the vocabulary; passed through softmax to get probabilities.\n",
        "\n",
        "Understanding these terms helps you read the model’s code and debug issues later.\n",
        "\n",
        "### Architectural trade‑offs\n",
        "\n",
        "| Design choice | Effect | Rationale |\n",
        "|---------------|--------|-----------|\n",
        "| **Depth (32 layers)** | Increases representational power | Deeper models capture more complex dependencies but are harder to train.\n",
        "| **Width (hidden size 16 k)** | More expressive per layer | Wider layers allow richer token representations but consume more memory.\n",
        "| **Attention heads (32)** | Parallel attention patterns | More heads can model diverse relationships but add computational cost.\n",
        "| **LayerNorm vs. RMSNorm** | Stability vs. speed | LayerNorm is more stable but slightly slower; GPT‑OSS sticks with LayerNorm for reliability.\n",
        "\n",
        "These choices reflect a balance between *accuracy* (capturing language nuances) and *efficiency* (running on available hardware). In the next step, we’ll see how to load this architecture with Hugging Face and run a quick inference demo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1️⃣  Inspect GPT‑OSS‑20B architecture details\n",
        "# -------------------------------------------------------------\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# Load the configuration (no weights needed)\n",
        "config = AutoConfig.from_pretrained(\"gpt-oss-20b\", trust_remote_code=True)\n",
        "\n",
        "print(\"Model name:\", config.model_type)\n",
        "print(\"Number of layers (depth):\", config.num_hidden_layers)\n",
        "print(\"Hidden size (width):\", config.hidden_size)\n",
        "print(\"Number of attention heads:\", config.num_attention_heads)\n",
        "print(\"Intermediate size (FFN):\", config.intermediate_size)\n",
        "print(\"Vocabulary size:\", config.vocab_size)\n",
        "print(\"Positional embeddings: learned?\", config.is_decoder)\n",
        "\n",
        "# Quick sanity check: compute total parameter count from config\n",
        "# (actual count may differ slightly due to bias terms)\n",
        "param_estimate = (config.num_hidden_layers *\n",
        "                  (config.hidden_size * config.hidden_size * 3 +  # QKV\n",
        "                   config.hidden_size * config.intermediate_size +  # FFN\n",
        "                   config.hidden_size * config.hidden_size))\n",
        "print(\"Estimated parameters (in billions):\", round(param_estimate / 1e9, 2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the Model with Hugging Face\n",
        "\n",
        "### Why we need a *loader*\n",
        "Think of GPT‑OSS‑20B as a gigantic, fully‑assembled Lego set. The *loader* is the instruction manual that tells your computer how to pick up each block, place it on the right spot, and keep everything organized. In the world of deep learning, this manual is the **Hugging Face `transformers` library**.\n",
        "\n",
        "### The loading pipeline\n",
        "1. **Tokenizer** – Turns raw text into a sequence of integer IDs that the model can understand. It’s like converting a sentence into a list of Lego block IDs.\n",
        "2. **Model** – The neural network itself. We’ll pull the pre‑trained weights from the Hugging Face Hub. The model lives on a device (CPU or GPU) that can perform the heavy math.\n",
        "3. **Device placement** – We decide whether to run on CPU, single‑GPU, or multi‑GPU. This choice balances speed, memory, and cost.\n",
        "4. **Precision** – FP32 (full precision) gives the best quality but uses more memory. FP16 or BF16 reduces memory and speeds up inference at a small quality cost.\n",
        "5. **Safety checks** – We guard against out‑of‑memory errors and ensure the model is ready for inference.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "- **Model**: A collection of layers and weights that maps input tokens to output logits. In GPT‑OSS‑20B, the model is a 32‑layer transformer with 16 k hidden size.\n",
        "- **Tokenizer**: A deterministic mapping from text to token IDs. It handles sub‑word units (e.g., `▁the`, `▁cat`).\n",
        "- **Device**: The hardware (CPU or GPU) where tensors are stored and operations executed.\n",
        "- **Precision**: Numerical format (FP32, FP16, BF16). Lower precision reduces memory and can accelerate inference but may slightly degrade output quality.\n",
        "- **Inference**: The process of feeding a prompt through the model to generate predictions.\n",
        "- **Trade‑offs**: Using FP16 or BF16 saves memory and speeds up inference, but may introduce small numerical differences. Running on CPU is cheap but slow; GPU is fast but requires a compatible card.\n",
        "\n",
        "### Practical tip\n",
        "When you first load a 20 B‑parameter model, you’ll likely hit the GPU memory limit. The code below includes a graceful fallback to CPU if the GPU cannot hold the model. This keeps the notebook running even on modest hardware.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1️⃣  Import libraries and set reproducibility\n",
        "# -------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Set deterministic seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2️⃣  Choose device: GPU if available, else CPU\n",
        "# -------------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3️⃣  Load tokenizer (no heavy weights)\n",
        "# -------------------------------------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\", trust_remote_code=True)\n",
        "print(\"Tokenizer loaded – vocab size:\", tokenizer.vocab_size)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4️⃣  Load model with optional precision handling\n",
        "# -------------------------------------------------------------\n",
        "# Try FP16 first; fall back to FP32 if OOM\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"gpt-oss-20b\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16,  # use FP16 for memory efficiency\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # automatically place layers on GPU\n",
        "    )\n",
        "    print(\"Model loaded in FP16 on GPU.\")\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"OOM detected – falling back to FP32 on CPU.\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"gpt-oss-20b\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float32,\n",
        "            device_map=\"cpu\",\n",
        "        )\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "model.eval()  # set to inference mode\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5️⃣  Quick inference demo\n",
        "# -------------------------------------------------------------\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated text:\\n\", generated_text)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Tokenization and Prompt Engineering\n",
        "\n",
        "### Why tokenization matters\n",
        "Think of the model as a super‑fast librarian who can only read *codes* instead of words. Tokenization is the process of turning your natural language into a sequence of these codes (integers). It’s like converting a sentence into a barcode that the librarian can scan.\n",
        "\n",
        "### The tokenizer as a translator\n",
        "- **Tokenizer**: A deterministic mapping from text to token IDs. It handles sub‑word units (e.g., `▁the`, `▁cat`).\n",
        "- **Vocabulary**: The set of all possible token IDs the model knows. For GPT‑OSS‑20B, the vocab size is 50 k.\n",
        "- **Special tokens**: `bos_token`, `eos_token`, `pad_token`, etc. They give the model context about the start, end, or padding of a sequence.\n",
        "\n",
        "### Prompt engineering 101\n",
        "Prompt engineering is the art of crafting the *input* so that the model’s output aligns with your intent. It’s like giving a recipe to a chef: the clearer the instructions, the better the dish.\n",
        "\n",
        "#### Common strategies\n",
        "1. **System + User + Assistant** format (inspired by OpenAI’s chat API). Example:\n",
        "   ```text\n",
        "   System: You are a helpful assistant.\n",
        "   User: What is the capital of France?\n",
        "   Assistant:\n",
        "   ```\n",
        "2. **Instruction + Context**: \"Write a short poem about the sea.\" The instruction tells the model *what* to do, the context gives *how*.\n",
        "3. **Few‑shot examples**: Provide a few input‑output pairs before the new prompt to bias the model toward a style.\n",
        "4. **Prompt length control**: Keep the total token count below the model’s context window (≈ 32 k tokens for GPT‑OSS‑20B). Truncate or summarize longer inputs.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "- **Context window**: The maximum number of tokens the model can see at once. Exceeding it forces truncation, which can lose important information.\n",
        "- **Token budget**: The sum of prompt tokens + generated tokens must stay within the context window. Managing this budget is crucial for long‑form generation.\n",
        "- **Trade‑offs**: Longer prompts give the model more guidance but consume more of the token budget, leaving fewer tokens for the answer. Shorter prompts save budget but risk ambiguity.\n",
        "- **Precision vs. speed**: Using `torch_dtype=torch.float16` reduces memory usage and speeds up inference, but may introduce tiny numerical differences that can affect rare token probabilities.\n",
        "- **Determinism**: Setting `temperature=0` and `top_k=1` makes generation deterministic, useful for debugging but less creative.\n",
        "\n",
        "### Practical takeaway\n",
        "- Always inspect the tokenized prompt to ensure it contains the expected special tokens.\n",
        "- Use `tokenizer.encode` and `tokenizer.decode` to convert between text and IDs.\n",
        "- Keep an eye on the token count: `len(input_ids[0])`.\n",
        "- When building a pipeline, wrap tokenization and prompt formatting in reusable functions.\n",
        "\n",
        "### Quick sanity check\n",
        "Below we’ll load the tokenizer, inspect a sample prompt, and show how to truncate it to fit the context window.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1️⃣  Load tokenizer (already loaded in Step 3, but re‑import for safety)\n",
        "# -------------------------------------------------------------\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Use the same model name as before\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "print(\"Tokenizer loaded – vocab size:\", tokenizer.vocab_size)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2️⃣  Helper: show tokenization details\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def show_tokens(text, max_display=20):\n",
        "    \"\"\"Print token IDs and decoded tokens for a given text.\"\"\"\n",
        "    ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "    print(f\"\\nOriginal text: {text}\\n\")\n",
        "    print(f\"Token IDs (first {max_display}):\", ids[:max_display])\n",
        "    print(\"Decoded tokens:\", tokenizer.convert_ids_to_tokens(ids[:max_display]))\n",
        "    print(\"Total tokens:\", len(ids))\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"System: You are a helpful assistant.\\nUser: Explain the concept of tokenization in simple terms.\\nAssistant:\"\n",
        "show_tokens(prompt)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3️⃣  Truncate to fit context window (32k tokens for GPT‑OSS‑20B)\n",
        "# -------------------------------------------------------------\n",
        "CONTEXT_WINDOW = 32000\n",
        "\n",
        "def truncate_prompt(text, max_tokens=CONTEXT_WINDOW):\n",
        "    ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "    if len(ids) > max_tokens:\n",
        "        # Keep the last `max_tokens` tokens (most recent context)\n",
        "        truncated_ids = ids[-max_tokens:]\n",
        "        print(f\"Prompt truncated from {len(ids)} to {len(truncated_ids)} tokens.\")\n",
        "        return truncated_ids\n",
        "    return ids\n",
        "\n",
        "truncated_ids = truncate_prompt(prompt, max_tokens=CONTEXT_WINDOW)\n",
        "print(\"Truncated token count:\", len(truncated_ids))\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4️⃣  Simple prompt‑engineering function\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def build_prompt(system, user, assistant_prefix=\"Assistant:\"):\n",
        "    \"\"\"Return a formatted prompt string with system, user, and assistant placeholders.\"\"\"\n",
        "    return f\"System: {system}\\nUser: {user}\\n{assistant_prefix}\"\n",
        "\n",
        "# Build a new prompt\n",
        "new_prompt = build_prompt(\n",
        "    system=\"You are a friendly tutor.\",\n",
        "    user=\"What is the capital of Japan?\"\n",
        ")\n",
        "print(\"\\nNew prompt:\\n\", new_prompt)\n",
        "\n",
        "# Encode and decode to verify\n",
        "ids = tokenizer.encode(new_prompt, add_special_tokens=True)\n",
        "print(\"Decoded back:\", tokenizer.decode(ids, skip_special_tokens=False))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generating Text with Sampling Strategies\n",
        "\n",
        "When you ask GPT‑OSS‑20B to write something, you’re really asking it to *pick* the next word from a huge list of possibilities. Think of it like a game of \"pick a card from a deck\" – the deck is the model’s probability distribution over the vocabulary, and the card you draw becomes the next token in the story.\n",
        "\n",
        "### The main ways to pick a card\n",
        "1. **Greedy (temperature = 0, top‑k = 1)** – always pick the highest‑probability card. It’s fast but can get stuck in repetitive loops.\n",
        "2. **Random sampling** – pick any card weighted by its probability. It’s creative but can produce nonsense.\n",
        "3. **Top‑k sampling** – keep only the top k most probable cards and sample from that smaller deck. It balances creativity and safety.\n",
        "4. **Top‑p (nucleus) sampling** – keep the smallest set of cards whose cumulative probability exceeds *p* (e.g., 0.9). It adapts the deck size to the distribution’s shape.\n",
        "5. **Beam search** – keep *n* best partial sentences at each step. It’s great for tasks that need high‑quality, deterministic output (e.g., translation) but is slower.\n",
        "6. **Temperature scaling** – adjust the softness of the probability distribution. A low temperature (<1) sharpens the deck (more deterministic), while a high temperature (>1) flattens it (more random).\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "- **Sampling**: The process of selecting the next token from the model’s probability distribution. It determines the trade‑off between *diversity* (many different outputs) and *coherence* (logical, fluent text).\n",
        "- **Temperature**: A scalar that raises or lowers the logits before softmax. Mathematically, `softmax(logits / temperature)`. Lower temperatures make the distribution peakier; higher temperatures make it flatter.\n",
        "- **Top‑k**: The number of highest‑probability tokens retained for sampling. Setting `k=50` means the model will only consider the 50 most likely words.\n",
        "- **Top‑p (nucleus)**: The cumulative probability threshold. For `p=0.9`, the model keeps the smallest set of tokens whose summed probability is at least 90 %.\n",
        "- **Beam width**: In beam search, the number of partial sequences kept at each step. A larger beam gives more exhaustive search but uses more memory and time.\n",
        "- **Repetition penalty**: A factor that reduces the probability of tokens that have already appeared, discouraging loops.\n",
        "- **Length penalty**: Adjusts the score of sequences based on their length, encouraging or discouraging longer outputs.\n",
        "\n",
        "**Trade‑offs**: Greedy is fast but can be dull; random sampling is creative but risky; top‑k/top‑p offer a sweet spot; beam search is accurate but slow. Temperature tweaks the balance between certainty and surprise. Choosing the right strategy depends on the task: creative writing, question answering, or formal translation each have different needs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1️⃣  Helper to generate text with a chosen strategy\n",
        "# -------------------------------------------------------------\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Re‑use the tokenizer and model from previous steps\n",
        "# (Assume they are already loaded as `tokenizer` and `model`)\n",
        "\n",
        "# Ensure reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2️⃣  Generation function\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def generate_text(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 100,\n",
        "    strategy: str = \"top_k\",\n",
        "    temperature: float = 0.7,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.9,\n",
        "    num_beams: int = 1,\n",
        "    repetition_penalty: float = 1.0,\n",
        "    length_penalty: float = 1.0,\n",
        "):\n",
        "    \"\"\"Generate text using the specified sampling strategy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt: str\n",
        "        Input text to start generation.\n",
        "    max_new_tokens: int\n",
        "        How many tokens to generate.\n",
        "    strategy: str\n",
        "        One of \"greedy\", \"random\", \"top_k\", \"top_p\", \"beam\".\n",
        "    temperature: float\n",
        "        Softmax temperature.\n",
        "    top_k: int\n",
        "        Keep only the top‑k tokens for sampling.\n",
        "    top_p: float\n",
        "        Keep tokens until cumulative probability >= top_p.\n",
        "    num_beams: int\n",
        "        Beam width for beam search.\n",
        "    repetition_penalty: float\n",
        "        Penalize repeated tokens.\n",
        "    length_penalty: float\n",
        "        Adjust score based on length.\n",
        "    \"\"\"\n",
        "    # Tokenize prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(model.device)\n",
        "\n",
        "    # Build generation kwargs based on strategy\n",
        "    gen_kwargs = {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"repetition_penalty\": repetition_penalty,\n",
        "        \"length_penalty\": length_penalty,\n",
        "        \"do_sample\": strategy in {\"random\", \"top_k\", \"top_p\"},\n",
        "    }\n",
        "\n",
        "    if strategy == \"greedy\":\n",
        "        gen_kwargs.update({\"do_sample\": False})\n",
        "    elif strategy == \"random\":\n",
        "        gen_kwargs.update({\"do_sample\": True, \"top_k\": 0, \"top_p\": 1.0})\n",
        "    elif strategy == \"top_k\":\n",
        "        gen_kwargs.update({\"do_sample\": True, \"top_k\": top_k, \"top_p\": 1.0})\n",
        "    elif strategy == \"top_p\":\n",
        "        gen_kwargs.update({\"do_sample\": True, \"top_k\": 0, \"top_p\": top_p})\n",
        "    elif strategy == \"beam\":\n",
        "        gen_kwargs.update({\"do_sample\": False, \"num_beams\": num_beams})\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids, **gen_kwargs)\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3️⃣  Quick sanity check (will run in the next cell)\n",
        "# -------------------------------------------------------------\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 4️⃣  Demonstrate each strategy\n",
        "# -------------------------------------------------------------\n",
        "strategies = [\n",
        "    (\"greedy\", 0.0, 1, 1.0),\n",
        "    (\"random\", 1.0, 0, 1.0),\n",
        "    (\"top_k\", 0.7, 50, 1.0),\n",
        "    (\"top_p\", 0.7, 0, 0.9),\n",
        "    (\"beam\", 0.0, 0, 1.0),\n",
        "]\n",
        "\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "\n",
        "for name, temp, k, p in strategies:\n",
        "    if name == \"beam\":\n",
        "        text = generate_text(prompt, max_new_tokens=50, strategy=name, num_beams=5)\n",
        "    else:\n",
        "        text = generate_text(prompt, max_new_tokens=50, strategy=name, temperature=temp, top_k=k, top_p=p)\n",
        "    print(f\"\\n=== {name.upper()} ===\")\n",
        "    print(text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Fine‑Tuning Basics (Optional)\n",
        "\n",
        "Fine‑tuning is like giving a well‑educated student a new set of notes for a specific exam. The student already knows the language, grammar, and general facts, but now they need to learn how to answer questions about a particular domain—say, medical guidelines or legal statutes. GPT‑OSS‑20B is that student: it has a huge vocabulary and a deep understanding of language, but it may not know the *nuances* of your niche.\n",
        "\n",
        "### Why fine‑tune?\n",
        "- **Domain adaptation** – Tailor the model to your jargon, style, or compliance rules.\n",
        "- **Performance boost** – Even a tiny amount of domain data can improve accuracy on specialized tasks.\n",
        "- **Control** – You can steer the model away from undesirable outputs by exposing it to curated examples.\n",
        "\n",
        "### Key terms (extra paragraph)\n",
        "- **Dataset** – A collection of text pairs (input, target) that the model learns from. In fine‑tuning we often use *supervised* data.\n",
        "- **Tokenizer** – Converts raw text into token IDs. The same tokenizer used for pre‑training must be reused to keep the embedding space consistent.\n",
        "- **Training loop** – The iterative process of feeding batches, computing loss, and updating weights.\n",
        "- **Gradient checkpointing** – Saves memory by recomputing intermediate activations during back‑propagation.\n",
        "- **Learning rate** – Controls how big a step the optimizer takes in weight space. Too high → divergence; too low → slow convergence.\n",
        "- **Batch size** – Number of examples processed together. Larger batches give more stable gradients but require more GPU memory.\n",
        "- **Epoch** – One full pass over the entire training dataset.\n",
        "- **Overfitting** – When the model memorizes the training data and performs poorly on unseen data.\n",
        "\n",
        "### Trade‑offs\n",
        "| Decision | Memory | Speed | Accuracy | Risk |\n",
        "|----------|--------|-------|----------|------|\n",
        "| Full‑precision FP32 | High | Slow | Highest | Low |\n",
        "| Mixed‑precision FP16 | Medium | Faster | Slight drop | Low |\n",
        "| Gradient checkpointing | Low | Slower | Same | Medium |\n",
        "| Small dataset | Low | Fast | Lower | High (overfitting) |\n",
        "| Large dataset | High | Slow | Higher | Medium |\n",
        "\n",
        "Fine‑tuning a 20 B‑parameter model on a single GPU is usually infeasible. In practice you’ll either use **LoRA** (low‑rank adapters) or **parameter‑efficient fine‑tuning (PEFT)**, but for illustration we’ll show a *minimal* training script that runs on a small subset of the dataset and uses gradient checkpointing to keep memory usage manageable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 1️⃣  Imports, reproducibility, and device setup\n",
        "# -------------------------------------------------------------\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2️⃣  Load a tiny subset of a public dataset (e.g., WikiText)\n",
        "# -------------------------------------------------------------\n",
        "# We use only 1k examples to keep memory low.\n",
        "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
        "print(\"Dataset loaded – number of examples:\", len(raw_datasets))\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3️⃣  Tokenizer and model (same as pre‑training)\n",
        "# -------------------------------------------------------------\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "\n",
        "# Tokenizer – keep the same vocab\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Model – load in FP16 with gradient checkpointing to save memory\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",  # automatically place layers on GPU\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "model.eval()\n",
        "print(\"Model loaded – hidden size:\", model.config.hidden_size)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4️⃣  Prepare data for language modeling\n",
        "# -------------------------------------------------------------\n",
        "# Tokenize the entire dataset in a batched way\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Data collator that masks tokens for causal LM\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5️⃣  Training arguments – very small for demo purposes\n",
        "# -------------------------------------------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt-oss-finetune-demo\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,          # one epoch over 1% of WikiText\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,  # effective batch size 8\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6️⃣  Trainer – the high‑level training loop\n",
        "# -------------------------------------------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer ready – starting fine‑tuning…\")\n",
        "# Uncomment the next line to actually run training in a real environment\n",
        "# trainer.train()\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------\n",
        "# 7️⃣  Quick evaluation (on the same tiny dataset)\n",
        "# -------------------------------------------------------------\n",
        "# We’ll just generate a short continuation to see the effect.\n",
        "prompt = \"The quick brown fox\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Use the same generation helper from Step 5\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "gen_config = GenerationConfig(\n",
        "    max_new_tokens=20,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(input_ids, generation_config=gen_config)\n",
        "\n",
        "generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated continuation after fine‑tuning (demo):\")\n",
        "print(generated)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following is NOT a recommended sampling strategy for GPT‑OSS‑20B?\", [\"Top‑k sampling\",\"Temperature scaling\",\"Beam search\",\"Random sampling without constraints\"], 3, \"Random sampling without constraints can lead to incoherent outputs; recommended strategies include top‑k, temperature, and beam search.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary benefit of using batch inference for GPT‑OSS‑20B?\", [\"Reduces GPU memory usage\",\"Increases per‑token latency\",\"Improves throughput by parallelizing token generation\",\"Simplifies model code\"], 2, \"Batching allows the model to process multiple prompts simultaneously, maximizing GPU utilization and throughput.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:02:48.166Z",
      "title": "Deploying and Using GPT‑OSS‑20B for Real‑World Applications",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:02:48.175Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}