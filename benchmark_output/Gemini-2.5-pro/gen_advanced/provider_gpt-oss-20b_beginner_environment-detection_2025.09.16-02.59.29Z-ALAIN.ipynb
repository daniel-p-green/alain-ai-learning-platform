{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Dive into GPT‑Oss‑20B: Architecture, Training, and Deployment\n\nThis notebook guides advanced practitioners through the intricacies of GPT‑Oss‑20B, covering its transformer architecture, tokenization, data pipelines, fine‑tuning strategies, distributed training, evaluation, inference optimization, deployment, and ethical considerations. It balances theory with hands‑on code, enabling researchers to replicate, extend, and responsibly deploy the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T02:59:29.868Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain the architectural components and design choices of GPT‑Oss‑20B.\n",
        "2. Demonstrate how to prepare data, fine‑tune, and apply parameter‑efficient tuning techniques.\n",
        "3. Illustrate distributed training with DeepSpeed and inference optimization for production.\n",
        "4. Critically assess ethical implications and propose bias mitigation strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+\n",
        "- PyTorch 2.0+\n",
        "- Hugging Face Transformers 4.35+\n",
        "- Basic knowledge of transformer models and deep learning workflows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 torch>=2.0 transformers>=4.35 accelerate datasets deepspeed fastapi torchserve\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"torch>=2.0\",\"transformers>=4.35\",\"accelerate\",\"datasets\",\"deepspeed\",\"fastapi\",\"torchserve\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Setup and Model Loading\n",
        "\n",
        "Before we can play with GPT‑Oss‑20B, we need a clean playground where all the tools live. Think of it like setting up a kitchen: you need a stove, a fridge, a cutting board, and the right utensils. In the machine‑learning world, those utensils are libraries such as **PyTorch**, **Transformers**, **Accelerate**, **Datasets**, **DeepSpeed**, **FastAPI**, **TorchServe**, and **ipywidgets**. We’ll install them with `pip`, set an environment variable for Hugging Face cache, and verify that everything is the right version.\n",
        "\n",
        "### Why this order?\n",
        "- **`pip install`** first: ensures we have the latest compatible packages.\n",
        "- **Version check**: a quick sanity test that the installed packages match the notebook’s expectations.\n",
        "- **Model loading**: pulls GPT‑Oss‑20B from Hugging Face Hub and prepares the tokenizer.\n",
        "\n",
        "### Key terms explained\n",
        "- **Environment variable (`HF_HOME`)**: a folder where Hugging Face stores cached models and tokenizers. Setting it keeps your cache tidy and prevents accidental downloads.\n",
        "- **PyTorch**: the deep‑learning framework that powers the model’s tensors and GPU acceleration.\n",
        "- **Transformers**: the library that provides the GPT‑Oss‑20B architecture and tokenizer.\n",
        "- **Accelerate**: a helper that abstracts device placement (CPU/GPU/TPU) and distributed training.\n",
        "- **Datasets**: a fast data loading library that handles large text corpora.\n",
        "- **DeepSpeed**: a library for efficient large‑model training (memory optimization, mixed‑precision, etc.).\n",
        "- **FastAPI**: a lightweight web framework for building REST APIs.\n",
        "- **TorchServe**: a production‑ready model serving tool.\n",
        "- **ipywidgets**: interactive widgets for Jupyter notebooks.\n",
        "- **Seed**: a number that initializes random number generators to make experiments reproducible.\n",
        "\n",
        "### Trade‑offs\n",
        "- **`pip` vs. `conda`**: `pip` gives you the latest releases but may require manual CUDA setup; `conda` bundles CUDA but can lag behind.\n",
        "- **TorchServe vs. FastAPI**: TorchServe is opinionated and optimized for serving, while FastAPI offers more flexibility for custom endpoints.\n",
        "- **Local vs. cloud**: Running locally keeps data private but may hit GPU memory limits; cloud instances provide more resources but add cost.\n",
        "\n",
        "With this foundation, we’re ready to pull GPT‑Oss‑20B into memory and start experimenting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Install required packages\n",
        "# This cell uses subprocess to run pip commands.\n",
        "# It will silently ignore errors if a package is already installed.\n",
        "import subprocess, sys\n",
        "\n",
        "packages = [\n",
        "    \"torch>=2.0\",\n",
        "    \"transformers>=4.35\",\n",
        "    \"accelerate\",\n",
        "    \"datasets\",\n",
        "    \"deepspeed\",\n",
        "    \"fastapi\",\n",
        "    \"torchserve\",\n",
        "    \"ipywidgets>=8.0.0\"\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", pkg]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "        except Exception as exc:\n",
        "            if IN_COLAB:\n",
        "                packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                if packages:\n",
        "                    try:\n",
        "                        import IPython\n",
        "                        ip = IPython.get_ipython()\n",
        "                        if ip is not None:\n",
        "                            ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                        else:\n",
        "                            import subprocess as _subprocess\n",
        "                            _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                    except Exception as colab_exc:\n",
        "                        print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                        raise\n",
        "                else:\n",
        "                    print('No packages specified for pip install; skipping fallback')\n",
        "            else:\n",
        "                raise\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Warning: failed to install {pkg}. Continuing...\", file=sys.stderr)\n",
        "\n",
        "print(\"Package installation complete.\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Verify installed versions\n",
        "import torch, transformers, accelerate, datasets, deepspeed, fastapi, torchserve, ipywidgets\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"Accelerate version:\", accelerate.__version__)\n",
        "print(\"Datasets version:\", datasets.__version__)\n",
        "print(\"DeepSpeed version:\", deepspeed.__version__)\n",
        "print(\"FastAPI version:\", fastapi.__version__)\n",
        "print(\"TorchServe version:\", torchserve.__version__)\n",
        "print(\"ipywidgets version:\", ipywidgets.__version__)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Set environment variable and load GPT‑Oss‑20B\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1️⃣ Set HF_HOME to keep cache tidy\n",
        "os.environ[\"HF_HOME\"] = \"/tmp/hf_cache\"\n",
        "print(\"HF_HOME set to\", os.environ[\"HF_HOME\"])\n",
        "\n",
        "# 2️⃣ Reproducibility seed\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(\"Random seed set to\", SEED)\n",
        "\n",
        "# 3️⃣ Load tokenizer and model (this may download the 20B weights if not cached)\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "except Exception as e:\n",
        "    print(\"Error loading tokenizer:\", e)\n",
        "    raise\n",
        "\n",
        "print(f\"Loading model for {MODEL_NAME} (this may take a while)...\")\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,  # use FP16 for memory efficiency\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"          # automatically place layers on available GPUs\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"Error loading model:\", e)\n",
        "    raise\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully.\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: GPT‑Oss‑20B Architecture Overview\n",
        "\n",
        "Imagine a gigantic factory that can read a sentence, understand it, and then write a continuation. Each station in this factory is a *transformer layer* that takes the raw text, turns it into a set of numbers (embeddings), lets the numbers talk to each other through *self‑attention*, and then refines the result with a small neural network (the *feed‑forward* block). The factory is built on a *stack* of these layers—20 B‑GPT‑Oss has 32 such layers, each with 32 attention heads and a hidden dimension of 8 192.\n",
        "\n",
        "### Why this design?\n",
        "- **Depth (32 layers)**: More layers let the model learn increasingly abstract patterns, like how a sentence can be broken down into syntax, semantics, and world knowledge.\n",
        "- **Width (32 heads, 8 192 hidden size)**: Wider layers allow the model to capture a richer set of relationships between tokens.\n",
        "- **Self‑attention**: Every token can directly look at every other token, which is essential for long‑context reasoning.\n",
        "- **LayerNorm + Residuals**: These keep gradients stable and help the network learn faster.\n",
        "- **Positional Encoding**: Since transformers have no inherent sense of order, positional embeddings give each token a unique “address” in the sequence.\n",
        "\n",
        "### Key terms explained\n",
        "- **Transformer**: A neural architecture that relies on self‑attention to process sequences.\n",
        "- **Self‑attention**: A mechanism where each token computes a weighted sum of all tokens, allowing global context.\n",
        "- **Feed‑forward network (FFN)**: A two‑layer MLP applied to each token independently.\n",
        "- **LayerNorm**: Normalizes activations across the hidden dimension to stabilize training.\n",
        "- **Residual connection**: Adds the input of a sub‑module to its output, helping gradients flow.\n",
        "- **Positional embedding**: A learned vector added to token embeddings to encode position.\n",
        "- **ZeRO**: A memory‑optimization technique used during training (not in inference).\n",
        "\n",
        "### Rationale & trade‑offs\n",
        "The 20 B parameter count is a sweet spot between *expressive power* and *resource feasibility*. A larger model can capture more nuanced language patterns but requires more GPU memory and longer inference times. The chosen depth/width balance ensures that each layer can learn complex interactions without exploding memory usage. However, this also means that inference on a single GPU can be slow; sharding or quantization is often necessary for production.\n",
        "\n",
        "In the next step we’ll dive into how the model turns raw text into these embeddings—tokenization—and how we manage the vocabulary that feeds into the transformer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inspect GPT‑Oss‑20B configuration and a quick forward pass\n",
        "# This cell is short (<30 lines) and includes error handling.\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Set a reproducible seed\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "\n",
        "# Load tokenizer and model (device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\" shards across GPUs if available)\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"Failed to load model/tokenizer:\", e)\n",
        "    raise\n",
        "\n",
        "# Print key hyperparameters\n",
        "config = model.config\n",
        "print(\"\\n=== Model Hyperparameters ===\")\n",
        "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
        "print(f\"Hidden size: {config.hidden_size}\")\n",
        "print(f\"Attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Intermediate size (FFN): {config.intermediate_size}\")\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"Max sequence length: {config.max_position_embeddings}\")\n",
        "\n",
        "# Quick forward pass on a sample sentence\n",
        "sample = \"The quick brown fox jumps over the lazy dog\"\n",
        "inputs = tokenizer(sample, return_tensors=\"pt\")\n",
        "if torch.cuda.is_available():\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    print(\"\\nLogits shape:\", logits.shape)  # (batch, seq_len, vocab)\n",
        "\n",
        "print(\"\\n--- End of inspection ---\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Tokenization and Vocabulary Management\n",
        "\n",
        "### 1️⃣ What is tokenization?\n",
        "Think of a sentence as a long string of letters. A tokenizer is like a *smart cutting board* that slices this string into pieces (tokens) that the model can understand. For GPT‑Oss‑20B we use a **Byte‑Pair Encoding (BPE)** tokenizer, which learns the most common sub‑word units from the training data. The result is a *vocabulary* – a dictionary that maps each token to a unique integer ID.\n",
        "\n",
        "### 2️⃣ Why BPE and not words?\n",
        "- **Memory efficiency**: A full word‑level vocab for English would need >200k entries, blowing up the embedding matrix. BPE keeps the vocab around 50k–80k tokens.\n",
        "- **Robustness to OOV**: Rare or unseen words are broken into known sub‑words, so the model can still process them.\n",
        "- **Speed**: Tokenization is linear in the number of characters, and BPE tables are fast to look up.\n",
        "\n",
        "### 3️⃣ Key terms\n",
        "- **Tokenizer**: The software that turns raw text into token IDs.\n",
        "- **Vocabulary (vocab)**: The mapping from token strings to integer IDs.\n",
        "- **Special tokens**: Tokens like ```, ```, or `` that signal start/end of a sequence or padding.\n",
        "- **Token IDs**: Integers that index into the model’s embedding matrix.\n",
        "- **Embedding matrix**: A 2‑D tensor of shape `(vocab_size, hidden_size)` that converts token IDs into dense vectors.\n",
        "- **`tokenizer.add_tokens()`**: A method to extend the vocab with new tokens.\n",
        "- **`model.resize_token_embeddings()`**: Adjusts the embedding matrix to match the new vocab size.\n",
        "\n",
        "### 4️⃣ Managing the vocabulary\n",
        "1. **Inspect the current vocab** – you can print the size, list special tokens, and see how many tokens a sentence expands into.\n",
        "2. **Add new tokens** – useful for domain‑specific terminology or control tokens.\n",
        "3. **Resize embeddings** – after adding tokens you must enlarge the embedding matrix; otherwise the model will crash.\n",
        "4. **Save and reload** – keep the updated tokenizer in a folder so you can reuse it.\n",
        "\n",
        "### 5️⃣ Trade‑offs\n",
        "| Aspect | Large vocab | Small vocab |\n",
        "|--------|-------------|-------------|\n",
        "| **Embedding size** | Larger matrix → more GPU memory | Smaller matrix → less memory |\n",
        "| **Tokenization granularity** | Fewer sub‑word splits → longer tokens | More splits → shorter tokens |\n",
        "| **Speed** | Slightly slower lookup due to larger table | Faster lookup |\n",
        "| **Coverage** | Better coverage of rare words | More OOV tokens, more sub‑word splits |\n",
        "\n",
        "Choosing the right vocab size is a balancing act: a bigger vocab reduces the number of tokens per sentence (good for memory‑bound inference) but increases the embedding matrix (bad for GPU memory). GPT‑Oss‑20B uses a 50k‑token BPE vocab, which is a sweet spot for English‑style text.\n",
        "\n",
        "### 6️⃣ Next step preview\n",
        "In the next section we’ll build a **data pipeline**: clean raw text, split it into training examples, and feed it into the tokenizer to create the datasets that will train or fine‑tune the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Load the GPT‑Oss‑20B tokenizer and inspect its vocabulary\n",
        "# ---------------------------------------------------------------\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1️⃣ Load the tokenizer (this may download the vocab if not cached)\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(f\"Loading tokenizer for {MODEL_NAME}…\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "except Exception as e:\n",
        "    print(\"Error loading tokenizer:\", e)\n",
        "    raise\n",
        "\n",
        "# 2️⃣ Basic stats\n",
        "print(\"\\n=== Tokenizer statistics ===\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"Number of special tokens: {len(tokenizer.special_tokens_map)}\")\n",
        "print(\"Special tokens mapping:\")\n",
        "for key, value in tokenizer.special_tokens_map.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# 3️⃣ Encode a sample sentence and show token IDs\n",
        "sample = \"The quick brown fox jumps over the lazy dog.\"\n",
        "encoded = tokenizer(sample, return_tensors=\"pt\")\n",
        "print(\"\\nEncoded token IDs:\", encoded[\"input_ids\"][0])\n",
        "print(\"Number of tokens (including special tokens):\", encoded[\"input_ids\"][0].size(0))\n",
        "\n",
        "# 4️⃣ Decode back to text to verify round‑trip\n",
        "decoded = tokenizer.decode(encoded[\"input_ids\"][0])\n",
        "print(\"\\nDecoded text:\", decoded)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Adding new tokens and resizing the model’s embeddings\n",
        "# ---------------------------------------------------------------\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1️⃣ Load tokenizer and model (device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\" shards across GPUs if available)\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(f\"\\nLoading model for {MODEL_NAME}…\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"Error loading model/tokenizer:\", e)\n",
        "    raise\n",
        "\n",
        "# 2️⃣ Define new domain‑specific tokens\n",
        "new_tokens = [\"<BILL>\", \"<DATE>\", \"<ORG>\"]\n",
        "print(\"\\nAdding new tokens to tokenizer:\", new_tokens)\n",
        "added = tokenizer.add_tokens(new_tokens)\n",
        "print(f\"Number of tokens added: {added}\")\n",
        "\n",
        "# 3️⃣ Resize the model’s embedding matrix to accommodate new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "print(\"Resized embedding matrix to\", len(tokenizer))\n",
        "\n",
        "# 4️⃣ Verify that new tokens can be encoded\n",
        "sample = \"<BILL> 2023-08-15 <ORG>\"\n",
        "encoded = tokenizer(sample, return_tensors=\"pt\")\n",
        "print(\"\\nEncoded IDs for new tokens:\", encoded[\"input_ids\"][0])\n",
        "\n",
        "# 5️⃣ Optional: save the updated tokenizer for future reuse\n",
        "save_dir = \"./gpt-oss-20b-tokenizer\"\n",
        "print(f\"\\nSaving updated tokenizer to {save_dir}…\")\n",
        "try:\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(\"Tokenizer saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Failed to save tokenizer:\", e)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Data Pipeline – Preprocessing and Dataset Construction\n",
        "\n",
        "When you train a language model, the raw text you hand it is like a messy pile of LEGO bricks. The model needs a clean, well‑structured set of bricks that it can stack into sentences, paragraphs, and eventually entire books. This section walks through the *data pipeline* that turns raw text into a Hugging Face `Dataset` ready for fine‑tuning. We’ll cover:\n",
        "\n",
        "1. **Data ingestion** – loading from local files or public datasets.\n",
        "2. **Cleaning & filtering** – removing noise, normalizing whitespace, and enforcing a maximum sequence length.\n",
        "3. **Tokenization** – converting text to token IDs with the GPT‑Oss‑20B tokenizer.\n",
        "4. **Dataset construction** – shuffling, splitting, and caching for efficient training.\n",
        "5. **Batching & collating** – preparing mini‑batches that the model can consume.\n",
        "\n",
        "### Why a pipeline matters\n",
        "Think of the pipeline as a factory line. If one station is slow or buggy, the whole line stalls. A well‑designed pipeline ensures that:\n",
        "- **Speed**: Data is pre‑processed once and cached, so training loops run fast.\n",
        "- **Reproducibility**: Fixed random seeds and deterministic shuffling mean you can hit the same data split every run.\n",
        "- **Scalability**: Streaming large corpora keeps memory usage low, allowing you to train on datasets that would otherwise not fit in RAM.\n",
        "\n",
        "### Key terms explained\n",
        "- **Tokenizer**: Turns raw text into a list of token IDs.\n",
        "- **Dataset**: A collection of examples (here, tokenized sequences) that can be iterated over.\n",
        "- **Collator**: A function that pads a batch of sequences to the same length.\n",
        "- **Streaming**: Loading data on‑the‑fly from disk or the internet, rather than loading everything into memory.\n",
        "- **Cache**: A local copy of processed data that speeds up subsequent runs.\n",
        "- **Seed**: A number that initializes random number generators to make shuffling deterministic.\n",
        "\n",
        "### Trade‑offs\n",
        "| Decision | Pros | Cons |\n",
        "|----------|------|------|\n",
        "| **Full‑text tokenization vs. chunking** | Keeps context intact; fewer padding tokens | Requires more memory; longer sequences may hit model limits |\n",
        "| **Caching vs. on‑the‑fly processing** | Faster subsequent runs | Disk space usage; stale cache if preprocessing changes |\n",
        "| **Streaming vs. loading into RAM** | Handles arbitrarily large datasets | Slightly slower per‑epoch throughput due to I/O |\n",
        "| **Deterministic shuffling vs. random shuffling** | Reproducible experiments | May introduce subtle bias if the same order is always used |\n",
        "\n",
        "Choosing the right balance depends on your compute budget, dataset size, and the level of reproducibility you need. In the code below we’ll use a streaming approach with deterministic shuffling and cache the processed dataset to disk.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Load, clean, and tokenize a streaming dataset\n",
        "# ---------------------------------------------------------------\n",
        "# Imports\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1️⃣ Set reproducibility seed\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# 2️⃣ Choose a public dataset (e.g., WikiText-2) or a local text file\n",
        "# For illustration we use the \"wikitext\" dataset; replace with your own path if needed\n",
        "DATASET_NAME = \"wikitext\"\n",
        "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
        "\n",
        "print(f\"Loading {DATASET_NAME} ({DATASET_CONFIG}) in streaming mode…\")\n",
        "raw_ds = load_dataset(DATASET_NAME, DATASET_CONFIG, split=\"train\", streaming=True)\n",
        "\n",
        "# 3️⃣ Define a simple cleaning function\n",
        "\n",
        "def clean_text(example):\n",
        "    \"\"\"Strip leading/trailing whitespace and collapse multiple spaces.\"\"\"\n",
        "    text = example[\"text\"].strip()\n",
        "    text = \" \".join(text.split())  # collapse whitespace\n",
        "    return {\"text\": text}\n",
        "\n",
        "# 4️⃣ Apply cleaning and tokenization in a single pass\n",
        "TOKENIZER_NAME = \"gpt-oss-20b\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
        "\n",
        "MAX_LENGTH = 512  # truncate long sequences to fit model context\n",
        "\n",
        "def tokenize(example):\n",
        "    \"\"\"Tokenize and truncate to MAX_LENGTH.\"\"\"\n",
        "    tokens = tokenizer(example[\"text\"], truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
        "    return {\"input_ids\": tokens[\"input_ids\"].squeeze(0)}\n",
        "\n",
        "# 5️⃣ Stream, clean, and tokenize\n",
        "print(\"Processing stream… (this may take a few minutes)\")\n",
        "processed = raw_ds.map(clean_text).map(tokenize, batched=False)\n",
        "\n",
        "# 6️⃣ Convert to a Dataset (not streaming) and cache to disk\n",
        "CACHE_DIR = \"./cached_wikitext\"\n",
        "print(f\"Caching processed dataset to {CACHE_DIR}…\")\n",
        "processed = processed.with_format(\"torch\")  # ensure torch tensors\n",
        "processed.save_to_disk(CACHE_DIR)\n",
        "print(\"Dataset cached successfully.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Load cached dataset, split, shuffle, and create a DataLoader\n",
        "# ---------------------------------------------------------------\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_from_disk\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# 1️⃣ Load the cached dataset\n",
        "print(\"Loading cached dataset…\")\n",
        "cached_ds = load_from_disk(\"./cached_wikitext\")\n",
        "print(f\"Total examples: {len(cached_ds)}\")\n",
        "\n",
        "# 2️⃣ Split into train/validation (80/20) with deterministic shuffling\n",
        "print(\"Splitting dataset…\")\n",
        "train_val = cached_ds.train_test_split(test_size=0.2, seed=SEED)\n",
        "train_ds = train_val[\"train\"]\n",
        "val_ds = train_val[\"test\"]\n",
        "print(f\"Train size: {len(train_ds)}, Validation size: {len(val_ds)}\")\n",
        "\n",
        "# 3️⃣ Create a collator that pads to the longest sequence in the batch\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# 4️⃣ Build DataLoaders\n",
        "BATCH_SIZE = 8\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
        "\n",
        "print(\"DataLoaders ready. Example batch shape:\")\n",
        "for batch in train_loader:\n",
        "    print(batch[\"input_ids\"].shape)  # (batch, seq_len)\n",
        "    break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Fine‑Tuning with Parameter‑Efficient Tuning (PEFT)\n",
        "\n",
        "Fine‑tuning a 20B‑parameter model on a single GPU is like trying to teach a giant elephant to do a tiny trick – it’s doable but expensive. Parameter‑Efficient Tuning (PEFT) tricks the elephant into learning by only moving a few *tweak‑points* instead of reshaping its entire body. In practice, we freeze the bulk of GPT‑Oss‑20B and insert lightweight adapters (e.g., LoRA) that learn the task‑specific signal. The result is a model that behaves like a fully fine‑tuned one but costs a fraction of the memory and compute.\n",
        "\n",
        "### Why PEFT?\n",
        "- **Memory savings**: Only a few thousand extra parameters are trained, so a single 24 GB GPU can handle the whole fine‑tune.\n",
        "- **Speed**: Training time drops because gradients flow through a tiny network.\n",
        "- **Flexibility**: You can swap adapters for different tasks without touching the base weights.\n",
        "- **Safety**: The frozen backbone preserves the original knowledge, reducing catastrophic forgetting.\n",
        "\n",
        "### Key terms & trade‑offs\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **LoRA (Low‑Rank Adaptation)** | Adds rank‑`r` matrices to the weight matrices of a transformer layer. | Keeps the number of trainable parameters low while still capturing task‑specific patterns. | Choosing a very low rank may hurt performance; a high rank increases memory again. |\n",
        "| **Adapter** | A small neural module inserted between layers. | Allows fine‑tuning without touching the original weights. | Adds a tiny inference overhead. |\n",
        "| **Freeze** | Keep the original weights fixed during training. | Prevents over‑fitting and saves memory. | Limits the model’s ability to fully adapt to extreme domain shifts. |\n",
        "| **PEFT library** | Hugging Face’s `peft` package that implements LoRA, Prefix, and other adapters. | Provides a clean API to wrap any Hugging Face model. | Requires an extra dependency and a bit of boilerplate. |\n",
        "\n",
        "The trade‑off is essentially *precision vs. efficiency*. If your downstream task is very different from the pre‑training data, a full fine‑tune might still win. For most domain‑adaptation scenarios, LoRA gives you a sweet spot.\n",
        "\n",
        "### What we’ll do in code\n",
        "1. Load GPT‑Oss‑20B and its tokenizer.\n",
        "2. Wrap the model with a LoRA adapter (rank = 8, alpha = 32).\n",
        "3. Prepare a small synthetic dataset (for demo purposes) – in practice you’d use your domain corpus.\n",
        "4. Set up `TrainingArguments` and a `Trainer` that only updates the LoRA weights.\n",
        "5. Run a quick training loop and inspect the adapter weights.\n",
        "\n",
        "All code cells are under 30 lines and include comments for clarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Install PEFT (if not already installed) and import libraries\n",
        "# ---------------------------------------------------------------\n",
        "import subprocess, sys\n",
        "\n",
        "try:\n",
        "    import peft\n",
        "except ImportError:\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"peft==0.5.0\", \"--quiet\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "    import peft\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Set reproducibility\n",
        "SEED = 123\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load base model and tokenizer\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\"Loading base model (FP16)…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Define LoRA configuration\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8,          # rank of the low‑rank matrices\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # apply to query & value projections\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "print(\"LoRA adapters added – trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Create a tiny synthetic dataset for demonstration\n",
        "# ---------------------------------------------------------------\n",
        "from datasets import Dataset\n",
        "\n",
        "# Simple sentences – replace with your real data\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Deep learning models can be fine‑tuned efficiently.\",\n",
        "    \"Parameter‑efficient tuning saves memory and time.\",\n",
        "    \"LoRA adds low‑rank adapters to transformer layers.\",\n",
        "    \"GPT‑Oss‑20B is a large language model.\",\n",
        "]\n",
        "\n",
        "# Tokenize and create a Dataset\n",
        "tokenized = tokenizer(sentences, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "train_ds = Dataset.from_dict({\"input_ids\": tokenized[\"input_ids\"], \"attention_mask\": tokenized[\"attention_mask\"]})\n",
        "\n",
        "# Define training arguments – small epoch count for demo\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./peft_output\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"no\",\n",
        "    fp16=True,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "# Trainer that only updates LoRA weights\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=lambda data: {\n",
        "        \"input_ids\": torch.stack([f[\"input_ids\"] for f in data]),\n",
        "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in data]),\n",
        "    },\n",
        ")\n",
        "\n",
        "print(\"Starting fine‑tune…\")\n",
        "trainer.train()\n",
        "print(\"Training finished. Adapter weights are now updated.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Distributed Training Using DeepSpeed\n",
        "\n",
        "### Why go distributed?\n",
        "Imagine you’re baking a huge cake that needs 20 B layers of frosting. One oven can only fit a few layers at a time, so you split the job across many ovens and let them work in parallel. Distributed training does the same thing for a gigantic language model: it splits the model and data across several GPUs so that each GPU does a piece of the work. The result is a *faster* training loop and a *smaller* memory footprint per device.\n",
        "\n",
        "### DeepSpeed in a nutshell\n",
        "DeepSpeed is a library that makes distributed training efficient and easy. It adds three main ingredients:\n",
        "\n",
        "1. **ZeRO (Zero Redundancy Optimizer)** – a memory‑optimization technique that removes duplicate copies of model parameters, gradients, and optimizer states across GPUs.\n",
        "2. **Automatic Mixed‑Precision (AMP)** – runs most operations in FP16 or BF16 to cut memory usage and speed up compute.\n",
        "3. **Dynamic Loss Scaling** – keeps gradients stable when using low‑precision arithmetic.\n",
        "\n",
        "Think of ZeRO as a *shared pantry* where each kitchen only keeps the ingredients it needs, instead of each kitchen storing a full set of spices.\n",
        "\n",
        "### Key terms & trade‑offs\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Distributed Data Parallel (DDP)** | Each GPU holds a copy of the model and processes a mini‑batch; gradients are averaged across GPUs. | Enables parallelism across GPUs. | Requires careful synchronization; communication overhead can grow with GPU count. |\n",
        "| **ZeRO Stage 1** | Shards optimizer states across GPUs. | Reduces memory by ~1/num_gpus. | Still keeps full parameters on each GPU. |\n",
        "| **ZeRO Stage 2** | Shards optimizer states *and* gradients. | Further memory savings, enabling larger batch sizes. | Slightly more communication during backward pass. |\n",
        "| **ZeRO Stage 3** | Shards parameters, optimizer states, and gradients. | Max memory efficiency; can train models that otherwise would not fit. | Highest communication cost; best suited for very large models. |\n",
        "| **AMP** | Uses FP16/BF16 arithmetic. | Cuts memory and speeds up GPU kernels. | Requires loss scaling to avoid underflow. |\n",
        "| **Dynamic Loss Scaling** | Adjusts the scaling factor during training. | Keeps gradients in a safe range. | Adds a small runtime overhead. |\n",
        "\n",
        "### Rationale for DeepSpeed + PEFT\n",
        "When fine‑tuning GPT‑Oss‑20B with LoRA adapters, the majority of the 20 B parameters stay frozen. However, the *optimizer state* for the LoRA weights still needs to be stored on each GPU. ZeRO‑2 or ZeRO‑3 lets us shard that tiny state across GPUs, freeing up memory for larger batch sizes or higher‑rank adapters. AMP ensures we can keep the model in FP16 without sacrificing stability.\n",
        "\n",
        "### What we’ll do in code\n",
        "1. Create a minimal `deepspeed_config.json` that enables ZeRO‑2, AMP, and dynamic loss scaling.\n",
        "2. Write a lightweight training script that uses Hugging Face `Accelerate` to launch DeepSpeed.\n",
        "3. Show the command line to start training on 4 GPUs.\n",
        "\n",
        "All code cells are under 30 lines and include comments for clarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Create a DeepSpeed config file (deepspeed_config.json)\n",
        "# ---------------------------------------------------------------\n",
        "import json\n",
        "config = {\n",
        "    \"train_batch_size\": 8,          # global batch size (per GPU * num_gpus)\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"fp16\": {\"enabled\": True},    # enable automatic mixed precision\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,                # ZeRO‑2: shard optimizer states & gradients\n",
        "        \"allgather_partitions\": True,\n",
        "        \"reduce_scatter\": True,\n",
        "        \"contiguous_gradients\": True\n",
        "    },\n",
        "    \"optimizer\": {\"type\": \"AdamW\", \"params\": {\"lr\": 5e-5}},\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"loss_scale\": 0,               # dynamic loss scaling\n",
        "    \"zero_allow_untested_optimizer\": true\n",
        "}\n",
        "with open(\"deepspeed_config.json\", \"w\") as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "print(\"DeepSpeed config written to deepspeed_config.json\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Minimal training script using Accelerate + DeepSpeed\n",
        "# ---------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "# 1️⃣ Setup accelerator (will detect DeepSpeed if available)\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# 2️⃣ Load base model & tokenizer\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\"Loading base model (FP16)…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# 3️⃣ Wrap with LoRA (rank 8) – only these weights will be trained\n",
        "lora_cfg = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "print(\"LoRA adapters added – trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "# 4️⃣ Prepare a tiny synthetic dataset (replace with real data)\n",
        "from datasets import Dataset\n",
        "sentences = [\"DeepSpeed makes training large models fast.\", \"LoRA adapters keep memory low.\"]\n",
        "enc = tokenizer(sentences, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "train_ds = Dataset.from_dict({\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"]})\n",
        "\n",
        "# 5️⃣ Training arguments – point to the DeepSpeed config\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ds_output\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_steps=1,\n",
        "    fp16=True,\n",
        "    deepspeed=\"deepspeed_config.json\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# 6️⃣ Build Trainer (Accelerator will wrap it for DeepSpeed)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=lambda data: {\n",
        "        \"input_ids\": torch.stack([f[\"input_ids\"] for f in data]),\n",
        "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in data])\n",
        "    }\n",
        ")\n",
        "\n",
        "# 7️⃣ Launch training – Accelerator handles DeepSpeed launch\n",
        "print(\"Starting distributed training…\")\n",
        "trainer.train()\n",
        "print(\"Training finished. LoRA weights updated.\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Command line to launch training on 4 GPUs\n",
        "# ---------------------------------------------------------------\n",
        "# Save this script as train_ds.py and run:\n",
        "#   deepspeed --num_gpus=4 train_ds.py\n",
        "#\n",
        "# If you prefer Accelerate’s launcher:\n",
        "#   accelerate launch --num_gpus=4 train_ds.py\n",
        "#\n",
        "# The `deepspeed_config.json` file created earlier will be automatically picked up.\n",
        "print(\"Use the above command to start distributed training.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following is NOT a benefit of using DeepSpeed for training large language models?\", [\"Memory optimization through ZeRO stages\",\"Automatic mixed‑precision training\",\"Built‑in support for LoRA parameter‑efficient tuning\",\"Dynamic loss scaling for numerical stability\"], 2, \"DeepSpeed provides memory optimization, mixed‑precision, and loss scaling, but LoRA tuning is a separate library (PEFT) that can be integrated with DeepSpeed.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary advantage of parameter‑efficient tuning over full fine‑tuning?\", [\"Higher training speed due to fewer parameters\",\"Better generalization on unseen data\",\"Reduced GPU memory footprint during training\",\"Elimination of the need for a validation set\"], 2, \"PEFT methods freeze most of the model, training only a small set of adapters, which dramatically reduces memory usage and speeds up training while maintaining performance.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:59:29.860Z",
      "title": "Deep Dive into GPT‑Oss‑20B: Architecture, Training, and Deployment",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T02:59:29.868Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}