{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with GPT‑OSS‑20B: A Beginner’s Guide\n\nThis lesson introduces the GPT‑OSS‑20B language model, walks through installing the necessary tools, loading the model, and creating a simple interactive chat interface—all explained with everyday analogies and clear, jargon‑free language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T02:55:30.279Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what GPT‑OSS‑20B is and why it matters.\n",
        "2. Install and configure the required Python packages, including ipywidgets.\n",
        "3. Load the model safely on a laptop or cloud instance and run basic prompts.\n",
        "4. Build a minimal chat UI with ipywidgets and troubleshoot common issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+ installed on your machine\n",
        "- Basic familiarity with running terminal commands\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.40.0 accelerate>=0.28.0 torch>=2.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.40.0\",\"accelerate>=0.28.0\",\"torch>=2.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Setup\n",
        "\n",
        "Welcome to the first step of our journey with GPT‑OSS‑20B! Think of GPT‑OSS‑20B as a gigantic library of stories, facts, and conversations that you can ask questions to. In this section we’ll make sure your notebook is ready to read from that library.\n",
        "\n",
        "### Why do we need a setup?\n",
        "- **Dependencies**: The model relies on a handful of Python packages (PyTorch, Hugging Face Transformers, Accelerate, and ipywidgets). These are like the tools you need to open a book.\n",
        "- **Environment**: Some packages need to be enabled in Jupyter so that interactive widgets work.\n",
        "- **Reproducibility**: Setting a random seed guarantees that the same prompt will produce the same answer every time you run the notebook.\n",
        "\n",
        "### Key terms explained\n",
        "- **PyTorch**: A deep‑learning framework that handles tensors (multi‑dimensional arrays) and GPU acceleration.\n",
        "- **Transformers**: A library that provides pre‑trained language models and tokenizers.\n",
        "- **Accelerate**: A helper that automatically places the model on the best device (CPU or GPU) and manages memory.\n",
        "- **ipywidgets**: A Jupyter extension that lets you build interactive UI components.\n",
        "\n",
        "### Trade‑offs\n",
        "- **Speed vs. Memory**: Using `torch.float16` speeds up inference but uses less memory. However, if your GPU doesn’t support float16, you’ll need to fall back to float32.\n",
        "- **Local vs. API**: Running the model locally gives you full control and no API key, but requires a powerful machine. Using the OpenAI API is easier but incurs costs and latency.\n",
        "\n",
        "### Quick checklist\n",
        "1. **Python 3.10+**: Make sure you’re running a recent Python version.\n",
        "2. **Terminal access**: You’ll need to run a few `pip` commands.\n",
        "3. **Jupyter Notebook**: We’ll be working inside a notebook.\n",
        "\n",
        "Let’s get the environment ready!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install the required packages with specific versions for reproducibility\n",
        "# If you already have these installed, you can skip or use --upgrade\n",
        "!pip install --quiet \"torch>=2.0.0\" \"transformers>=4.40.0\" \"accelerate>=0.28.0\" \"ipywidgets>=8.0.0\"\n",
        "\n",
        "# Enable the ipywidgets extension for Jupyter\n",
        "try:\n",
        "    !jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
        "except Exception as e:\n",
        "    print(\"Widget extension already enabled or failed to enable.\", e)\n",
        "\n",
        "# Verify installations\n",
        "import torch, transformers, accelerate, ipywidgets\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "print(f\"ipywidgets version: {ipywidgets.__version__}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What just happened?\n",
        "- The `pip install` command fetched the latest compatible versions of the four packages. The `--quiet` flag keeps the output tidy.\n",
        "- `jupyter nbextension enable` turns on the widget system so that interactive controls will appear.\n",
        "- The final block imports each library and prints its version, confirming that everything is in place.\n",
        "\n",
        "If you see any errors, double‑check that you’re running a recent Python interpreter and that you have internet access.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: What is GPT‑OSS‑20B? (The Big Brain Analogy)\n",
        "\n",
        "Imagine a gigantic library that contains **every** book, article, conversation, and piece of text you can think of. Now imagine that library has a super‑smart librarian who can read any page instantly, understand the context, and write a brand‑new paragraph that sounds like it came from the original author. That librarian is what GPT‑OSS‑20B is in the world of artificial intelligence.\n",
        "\n",
        "### The “brain” behind the library\n",
        "GPT‑OSS‑20B is a *large language model* built with the **Transformer** architecture. Transformers are like a team of tiny workers that each look at a sentence, remember what they saw, and then pass that memory to the next worker. The workers repeat this process many times (12‑24 layers in GPT‑OSS‑20B), so the final output is a highly contextualized prediction of the next word.\n",
        "\n",
        "### Why 20 B?  What does that number mean?\n",
        "- **20 B** stands for *20 billion* trainable parameters. Think of each parameter as a tiny knob that the model can adjust while learning. More knobs usually mean the model can capture more subtle patterns, but it also means it needs more memory and compute.\n",
        "- The model was trained on a diverse mix of public text (books, Wikipedia, code, news, etc.) totaling roughly 1 TB of raw data. That’s why it can answer questions about almost any topic.\n",
        "\n",
        "### How does it “think”?\n",
        "When you give GPT‑OSS‑20B a prompt, it tokenizes the text into sub‑words, feeds those tokens through the transformer layers, and produces a probability distribution over the next token. It then samples from that distribution (or picks the most likely token) and repeats until it reaches an end‑of‑sentence marker or a user‑defined limit.\n",
        "\n",
        "### The trade‑offs you’ll encounter\n",
        "| Decision | Speed | Memory | Accuracy | Typical Use‑Case |\n",
        "|----------|-------|--------|----------|------------------|\n",
        "| `torch.float16` | ↑ | ↓ | ↓ (tiny) | Fast inference on GPUs |\n",
        "| `torch.float32` | ↓ | ↑ | ↑ | Precise generation on CPUs |\n",
        "| `device_map='auto'` | ↑ | ↓ | – | Automatic placement on best device |\n",
        "| `low_cpu_mem_usage=True` | – | ↓ | – | Load huge models on limited RAM |\n",
        "\n",
        "- **Speed vs. Memory**: Using half‑precision (`float16`) cuts memory in half and speeds up GPU inference, but if your GPU doesn’t support it you’ll fall back to `float32`.\n",
        "- **Local vs. API**: Running the model locally gives you full control and no external cost, but requires a powerful GPU or a cloud instance. The OpenAI API is easier but adds latency and cost.\n",
        "\n",
        "### Key terms explained (extra paragraph)\n",
        "- **Transformer**: A neural network architecture that relies on *self‑attention* to weigh the importance of each token relative to others.\n",
        "- **Self‑attention**: The mechanism that lets the model look at all tokens in a sentence simultaneously, assigning a weight to each pair.\n",
        "- **Tokenizer**: A tool that splits raw text into tokens (sub‑words) that the model can process.\n",
        "- **Pre‑training**: The phase where the model learns language patterns from massive corpora before any task‑specific fine‑tuning.\n",
        "- **Inference**: Generating text from a trained model given a prompt.\n",
        "- **Accelerate**: A helper library that automatically distributes the model across available devices and manages memory.\n",
        "- **`device_map='auto'`**: Tells Accelerate to place layers on the best device (CPU or GPU) based on available memory.\n",
        "- **`low_cpu_mem_usage=True`**: Loads only the necessary parts of the model into CPU memory, swapping the rest to disk to keep RAM usage low.\n",
        "\n",
        "### Quick sanity check\n",
        "Below we’ll load the tokenizer and the model, set a random seed for reproducibility, and generate a short response. This will confirm that everything is wired up correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load GPT‑OSS‑20B with Hugging Face Transformers and Accelerate\n",
        "# We set a seed for reproducibility and use a few tricks to keep memory usage low\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "\n",
        "# 1️⃣ Set a deterministic seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 2️⃣ Load the tokenizer (fast tokenizer is usually faster)\n",
        "model_name = \"TheBloke/GPT-OSS-20B-Chat\"  # replace with the exact repo if different\n",
        "print(\"Loading tokenizer…\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "except Exception as e:\n",
        "    print(\"Tokenizer load failed:\", e)\n",
        "    raise\n",
        "\n",
        "# 3️⃣ Load the model with memory‑saving flags\n",
        "print(\"Loading model… (this may take a few minutes)\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,          # use half‑precision for speed & lower memory\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",                 # let Accelerate decide where to place layers\n",
        "    low_cpu_mem_usage=True,            # keep CPU RAM usage low\n",
        "    trust_remote_code=True,            # allow custom model code if needed\n",
        ")\n",
        "\n",
        "# 4️⃣ Quick inference test\n",
        "prompt = \"Explain the concept of a transformer in simple terms.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "print(\"Generating…\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\n--- Generated Response ---\\n\")\n",
        "print(generated_text)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the Model Safely\n",
        "\n",
        "When you think of a giant library, you might imagine a librarian who can pull any book out instantly.  In the same way, GPT‑OSS‑20B is a *model* that lives in memory and can generate text on demand.  But just like a librarian needs a well‑organized shelf and a clear map of where each book is, the model needs a *device* (CPU or GPU), a *precision* (float16 or float32), and a *memory‑management strategy* to fit in your computer’s RAM.\n",
        "\n",
        "### Why “safely” matters\n",
        "- **Memory limits**: The 20 B‑parameter model is roughly 30 GB in float32.  Loading it all at once on a laptop can crash the notebook.\n",
        "- **Device placement**: If you have a GPU, you want the heavy layers on it; otherwise you fall back to the CPU.\n",
        "- **Precision trade‑offs**: `float16` cuts memory in half and speeds up inference on GPUs, but may not be supported on older hardware.\n",
        "- **Determinism**: Setting a random seed ensures that the same prompt always produces the same output, which is handy for debugging.\n",
        "\n",
        "### Key terms (extra paragraph)\n",
        "- **`torch_dtype`**: The numeric type used for tensors (e.g., `torch.float16` or `torch.float32`).  Lower precision reduces memory but can slightly degrade quality.\n",
        "- **`device_map`**: A dictionary or string that tells Hugging Face where each layer of the model should live (CPU, GPU, or a mix).  `\"auto\"` lets the library decide.\n",
        "- **`low_cpu_mem_usage`**: A flag that loads only the parts of the model that are needed at a time, swapping the rest to disk.  This is essential when RAM is limited.\n",
        "- **`accelerate`**: A helper that automatically distributes the model across available devices and handles memory‑saving tricks.\n",
        "- **`trust_remote_code`**: Allows the model repository to provide custom code (e.g., a custom `__init__`).  Use only with trusted sources.\n",
        "\n",
        "### Trade‑offs\n",
        "| Decision | Speed | Memory | Accuracy | When to use |\n",
        "|----------|-------|--------|----------|-------------|\n",
        "| `torch.float16` | ↑ | ↓ | ↓ (tiny) | Fast GPU inference |\n",
        "| `torch.float32` | ↓ | ↑ | ↑ | Precise CPU inference |\n",
        "| `device_map='auto'` | ↑ | ↓ | – | Automatic placement |\n",
        "| `low_cpu_mem_usage=True` | – | ↓ | – | Load on low‑RAM machines |\n",
        "\n",
        "The goal of this section is to give you a reusable function that handles all these knobs for you, so you can focus on the *what* (your prompts) instead of the *how* (device juggling).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 1️⃣  Safe model loader\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "def load_model_safely(\n",
        "    model_name: str,\n",
        "    dtype: torch.dtype = torch.float16,\n",
        "    device_map: str | dict = \"auto\",\n",
        "    low_cpu_mem: bool = True,\n",
        "    seed: int | None = 42,\n",
        "):\n",
        "    \"\"\"Load GPT‑OSS‑20B with memory‑saving options.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_name: str\n",
        "        Hugging Face repo id.\n",
        "    dtype: torch.dtype\n",
        "        Precision for tensors.\n",
        "    device_map: str or dict\n",
        "        Where to place layers.\n",
        "    low_cpu_mem: bool\n",
        "        Enable low‑CPU‑memory mode.\n",
        "    seed: int or None\n",
        "        Random seed for reproducibility.\n",
        "    \"\"\"\n",
        "    # 1️⃣ Set seed for deterministic runs\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        print(f\"🔒 Random seed set to {seed}\")\n",
        "\n",
        "    # 2️⃣ Load tokenizer\n",
        "    print(\"📦 Loading tokenizer…\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    # 3️⃣ Load model with safety flags\n",
        "    print(\"🚀 Loading model… (this may take a few minutes)\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=dtype,\n",
        "        device_map=device_map,\n",
        "        low_cpu_mem_usage=low_cpu_mem,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # 4️⃣ Print useful info\n",
        "    print(\"✅ Model loaded successfully!\")\n",
        "    print(f\"Model device: {next(model.parameters()).device}\")\n",
        "    print(f\"Model dtype: {dtype}\")\n",
        "    print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    tokenizer, model = load_model_safely(\n",
        "        model_name=\"TheBloke/GPT-OSS-20B-Chat\",\n",
        "        dtype=torch.float16,\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "        low_cpu_mem=True,\n",
        "    )\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 2️⃣  Quick inference test\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "prompt = \"Explain the concept of a transformer in simple terms.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "print(\"🧠 Generating response…\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\n--- Generated Response ---\\n\")\n",
        "print(generated)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4\n",
        "\n",
        "Thinking...\n",
        ">We need to output JSON with section_number 4, title \"Step 4: Running Your First Prompt\". Must include content array with markdown and code cells. Must follow guidelines: 800-1000 tokens per section. Provide callouts. Provide estimated_tokens 1000. Provide prerequisites_check. Provide next_section_hint. Must be beginner-friendly, analogies, precise terms, extra explanatory paragraph defining key terms and rationale/trade-offs. Provide code cells <=30 lines each. Use reproducibility s...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Tweaking Generation Settings\n",
        "\n",
        "Imagine you’re a chef preparing a dish. The **temperature** is the heat of the stove – a higher temperature cooks the food faster but can also burn it. The **top‑p** (nucleus sampling) is like a sieve that only lets the most promising ingredients through, keeping the dish focused. The **repetition penalty** is a seasoning that prevents the same flavor from dominating the plate. In the world of language models, these knobs let you control how *creative*, *coherent*, and *lengthy* the generated text will be.\n",
        "\n",
        "### What each knob does\n",
        "| Parameter | What it controls | Typical effect | When to use it |\n",
        "|-----------|------------------|----------------|----------------|\n",
        "| `temperature` | Randomness of token selection | 0.0 = deterministic, 1.0+ = more creative | Use low values for factual answers, higher for brainstorming |\n",
        "| `top_p` | Nucleus sampling threshold | Keeps only tokens that cumulatively make up `p` of the probability mass | Use 0.8‑0.95 for balanced output |\n",
        "| `max_new_tokens` | Length of the generated text | Larger values produce longer responses | Set based on your prompt length and memory limits |\n",
        "| `repetition_penalty` | Penalizes repeated tokens | Reduces loops and stuttering | Useful for long‑form generation |\n",
        "| `seed` | Random seed for reproducibility | Same output for same prompt | Set for debugging or demos |\n",
        "\n",
        "### Trade‑offs to keep in mind\n",
        "- **Creativity vs. Coherence**: A high temperature can produce surprising ideas but may also introduce hallucinations or incoherence. A low temperature gives safe, predictable answers but can feel bland.\n",
        "- **Speed vs. Quality**: Lower `top_p` values (e.g., 0.8) cut down the search space, speeding up generation but potentially missing good tokens. Higher `top_p` (e.g., 0.95) keeps more options, which can improve quality at the cost of speed.\n",
        "- **Memory vs. Length**: `max_new_tokens` directly impacts GPU/CPU memory usage. Generating 512 tokens can double the memory footprint compared to 128 tokens.\n",
        "- **Determinism vs. Exploration**: Setting a fixed `seed` makes the output reproducible, which is great for debugging but removes the natural variability you might want in creative tasks.\n",
        "\n",
        "### Key terms (extra paragraph)\n",
        "- **Softmax**: The mathematical function that turns raw model logits into probabilities.\n",
        "- **Sampling**: Choosing the next token based on the probability distribution.\n",
        "- **Deterministic decoding**: Selecting the token with the highest probability (e.g., `greedy` decoding).\n",
        "- **Nucleus sampling**: Selecting from the smallest set of tokens whose cumulative probability exceeds `top_p`.\n",
        "- **Repetition penalty**: A factor applied to the logits of tokens that have already appeared, discouraging repetition.\n",
        "\n",
        "By mastering these knobs, you can tailor GPT‑OSS‑20B’s output to match the tone, length, and reliability you need for your project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 1️⃣  Import libraries and set a reproducible seed\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Set a deterministic seed for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "print(f\"🔒 Random seed set to {SEED}\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 2️⃣  Load tokenizer and model (assumes model already downloaded)\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "MODEL_NAME = \"TheBloke/GPT-OSS-20B-Chat\"\n",
        "print(\"📦 Loading tokenizer…\")\n",
        " tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(\"🚀 Loading model… (this may take a few minutes)\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "print(\"✅ Model loaded!\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 3️⃣  Define a helper that runs generation with adjustable settings\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def generate_text(\n",
        "    prompt: str,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.9,\n",
        "    max_new_tokens: int = 128,\n",
        "    repetition_penalty: float = 1.0,\n",
        "):\n",
        "    \"\"\"Generate text from GPT‑OSS‑20B with user‑tunable parameters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt: str\n",
        "        The input text.\n",
        "    temperature: float\n",
        "        Controls randomness.\n",
        "    top_p: float\n",
        "        Nucleus sampling threshold.\n",
        "    max_new_tokens: int\n",
        "        How many tokens to generate.\n",
        "    repetition_penalty: float\n",
        "        Penalizes repeated tokens.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 4️⃣  Demo: compare low vs. high temperature\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "prompt = \"Explain why the sky is blue in simple terms.\"\n",
        "print(\"\\n--- Low temperature (0.2) ---\")\n",
        "print(generate_text(prompt, temperature=0.2, top_p=0.9, max_new_tokens=64))\n",
        "\n",
        "print(\"\\n--- High temperature (1.2) ---\")\n",
        "print(generate_text(prompt, temperature=1.2, top_p=0.9, max_new_tokens=64))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Managing Memory on a Laptop\n",
        "\n",
        "When you run GPT‑OSS‑20B on a laptop, you’re basically trying to fit a *very* large book into a *very* small backpack. The book is the model’s 20 billion parameters, and the backpack is your GPU/CPU memory. If you cram too many pages in, the backpack will break (the notebook crashes). This section shows you how to keep the backpack from tearing while still letting the model read and write.\n",
        "\n",
        "### Why memory matters\n",
        "- **GPU memory** is the fastest place to keep the model, but it’s limited (often 4–8 GB on consumer laptops). \n",
        "- **CPU memory** is larger (16–32 GB on many laptops) but slower for inference.\n",
        "- **Disk** can hold the whole model, but swapping data in and out is slow.\n",
        "\n",
        "### Key terms (extra paragraph)\n",
        "- **`torch.cuda.memory_allocated()`** – the amount of GPU memory currently used by tensors.\n",
        "- **`torch.cuda.memory_reserved()`** – the total GPU memory that PyTorch has reserved (including cached memory).\n",
        "- **`torch.cuda.empty_cache()`** – frees unused cached memory so the GPU can be reused by other processes.\n",
        "- **`torch.backends.cudnn.benchmark`** – when `True`, CuDNN will try different algorithms to find the fastest one for your current input size.\n",
        "- **`torch.backends.cudnn.deterministic`** – when `True`, forces deterministic algorithms (slower but reproducible).\n",
        "- **`torch.set_default_tensor_type()`** – sets the default dtype for tensors (e.g., `torch.float16` for half‑precision).\n",
        "- **`torch.cuda.set_device()`** – selects which GPU to use when multiple GPUs are present.\n",
        "\n",
        "### Rationale & trade‑offs\n",
        "| Strategy | Speed | Memory | Reproducibility | When to use |\n",
        "|----------|-------|--------|-----------------|-------------|\n",
        "| `float16` | ↑ | ↓ | ↓ (tiny) | Fast GPU inference, but may produce slightly noisier outputs on older GPUs |\n",
        "| `float32` | ↓ | ↑ | ↑ | Precise CPU inference or when float16 is unsupported |\n",
        "| `device_map='auto'` | ↑ | ↓ | – | Automatically places layers on the best device |\n",
        "| `low_cpu_mem_usage=True` | – | ↓ | – | Load huge models on machines with <8 GB RAM |\n",
        "| `torch.cuda.empty_cache()` | – | ↓ | – | Free GPU memory after inference |\n",
        "| `cudnn.benchmark=True` | ↑ | – | – | Faster inference for fixed input sizes |\n",
        "| `cudnn.deterministic=True` | ↓ | – | ↑ | Reproducible results for debugging |\n",
        "\n",
        "The goal is to keep the model inside the backpack while still letting it do its job. The code snippets below show how to *measure* memory, *clean up* after inference, and *optimize* for speed or reproducibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 1️⃣  Measure GPU memory before and after a simple inference\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Reproducibility seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load tokenizer and model (assumes model already downloaded)\n",
        "MODEL_NAME = \"TheBloke/GPT-OSS-20B-Chat\"\n",
        "print(\"📦 Loading tokenizer…\")\n",
        " tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(\"🚀 Loading model… (this may take a few minutes)\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Helper to print memory summary\n",
        "def print_mem(msg: str):\n",
        "    print(f\"\\n{msg}\")\n",
        "    print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "\n",
        "# Before inference\n",
        "print_mem(\"🧩 Memory before inference\")\n",
        "\n",
        "# Simple prompt\n",
        "prompt = \"What is the capital of France?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=32, do_sample=False)\n",
        "\n",
        "# After inference\n",
        "print_mem(\"🧩 Memory after inference\")\n",
        "\n",
        "# Clean up tensors that are no longer needed\n",
        "del inputs, outputs\n",
        "print(\"🧹 Cleaning up tensors…\")\n",
        "print_mem(\"🧩 Memory after deleting tensors\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 2️⃣  Free unused GPU memory and set deterministic flags\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# Free any cached memory that PyTorch keeps for speed\n",
        "print(\"⚡ Emptying CUDA cache…\")\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "\n",
        "# Optional: make inference deterministic (slower but reproducible)\n",
        "print(\"🔒 Setting deterministic CuDNN algorithms…\")\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Verify that the settings took effect\n",
        "print(f\"Deterministic: {torch.backends.cudnn.deterministic}\")\n",
        "print(f\"Benchmark: {torch.backends.cudnn.benchmark}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following is NOT a recommended way to reduce memory usage when running GPT‑OSS‑20B on a laptop?\", [\"Use torch_dtype=torch.float16\",\"Set device_map='auto'\",\"Increase max_new_tokens to 2000\",\"Load the model with low_cpu_mem_usage=True\"], 2, \"Increasing max_new_tokens increases the size of the generated text buffer, which can actually increase memory usage. The other options help keep the model lightweight.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which option correctly loads GPT‑OSS‑20B on a GPU using Accelerate?\", [\"torch_dtype='golden'\",\"device_map='auto', torch_dtype='float16', batch_size=8\",\"use_gpu=False\",\"load_in_8bit=True\"], 1, \"To utilize a GPU, you should set device_map='auto' and use a lower precision dtype such as float16, optionally adjusting batch_size for performance.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:55:30.272Z",
      "title": "Getting Started with GPT‑OSS‑20B: A Beginner’s Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T02:55:30.279Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}