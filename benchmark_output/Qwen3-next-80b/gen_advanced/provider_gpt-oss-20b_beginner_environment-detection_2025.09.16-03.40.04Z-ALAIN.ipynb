{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîß Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('üìù Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîê Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'üîè Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '‚àÖ'\\n    return v[:3] + '‚Ä¶' + v[-2:] if len(v) > 6 else '‚Ä¢‚Ä¢‚Ä¢'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üåê ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('‚úÖ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('‚ö†Ô∏è Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîé Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('‚ö†Ô∏è Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('‚úÖ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('‚ö†Ô∏è Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) ‚Äî 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Dive into GPT‚ÄëOSS‚Äë20B: Architecture, Trade‚Äëoffs, and Advanced Fine‚ÄëTuning\n\nThis notebook guides advanced practitioners through the inner workings of the GPT‚ÄëOSS‚Äë20B model, exploring its transformer architecture, parameter scaling, and practical fine‚Äëtuning strategies. It balances theoretical depth with hands‚Äëon code, enabling researchers to benchmark, adapt, and extend the model for domain‚Äëspecific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ‚è±Ô∏è Estimated time to complete: 36‚Äì60 minutes (rough).  ",
        "\n> üïí Created (UTC): 2025-09-16T03:40:04.083Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain the architectural choices that enable GPT‚ÄëOSS‚Äë20B to achieve 20B parameters while maintaining computational efficiency.\n",
        "2. Analyze the trade‚Äëoffs between model size, inference latency, and memory footprint on modern GPU clusters.\n",
        "3. Demonstrate advanced fine‚Äëtuning workflows using LoRA, QLoRA, and parameter‚Äëefficient transfer learning.\n",
        "4. Evaluate the model‚Äôs performance on benchmark datasets and design experiments to measure domain adaptation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+\n",
        "- Basic knowledge of PyTorch and Hugging Face Transformers\n",
        "- Experience with GPU programming and distributed training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.40.0 accelerate>=0.28.0 datasets>=2.20.0 bitsandbytes>=0.43.0 peft>=0.10.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.40.0\",\"accelerate>=0.28.0\",\"datasets>=2.20.0\",\"bitsandbytes>=0.43.0\",\"peft>=0.10.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('‚úÖ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Setup\n",
        "\n",
        "Welcome to the first step of our deep dive into **GPT‚ÄëOSS‚Äë20B**. Think of GPT‚ÄëOSS‚Äë20B as a gigantic library of 20‚ÄØbillion books (parameters). To read from this library efficiently, we need a well‚Äëorganized desk (the environment) and a reliable flashlight (the GPU). In this section we will:\n",
        "\n",
        "1. **Verify prerequisites** ‚Äì make sure you have the right tools.\n",
        "2. **Install the required libraries** ‚Äì the latest versions of `transformers`, `accelerate`, `datasets`, `bitsandbytes`, and `peft`.\n",
        "3. **Set up reproducibility** ‚Äì lock down random seeds and CUDA device selection.\n",
        "4. **Load a tiny demo model** ‚Äì just to confirm everything works.\n",
        "\n",
        "### Key Terms Explained\n",
        "- **Parameters**: The knobs inside a neural network that are tuned during training. 20‚ÄØbillion parameters is like having 20‚ÄØbillion adjustable dials.\n",
        "- **Quantization**: Reducing the precision of weights (e.g., from 32‚Äëbit float to 8‚Äëbit integer) to save memory while keeping performance close to the original.\n",
        "- **Reproducibility**: Setting random seeds and deterministic flags so that running the same code twice yields identical results.\n",
        "- **CUDA_VISIBLE_DEVICES**: An environment variable that tells PyTorch which GPUs to use.\n",
        "\n",
        "Trade‚Äëoffs: Using 8‚Äëbit quantization cuts memory usage by ~4√ó but may introduce a tiny drop in accuracy. For a 20‚ÄØbillion‚Äëparameter model, this trade‚Äëoff is often worth it because it allows the model to fit on a single 80‚ÄØGB GPU.\n",
        "\n",
        "### Why These Steps Matter\n",
        "Setting up the environment correctly is like laying a solid foundation before building a skyscraper. A misconfigured GPU or a missing library can cause silent failures that are hard to debug later. By installing the exact versions we tested against, you avoid ‚Äúworks on my machine‚Äù headaches.\n",
        "\n",
        "### Quick Checklist\n",
        "- Python 3.10+ installed\n",
        "- CUDA 12.1+ and cuDNN compatible with PyTorch\n",
        "- Hugging Face access token stored in `HF_TOKEN`\n",
        "- At least one GPU visible via `CUDA_VISIBLE_DEVICES`\n",
        "\n",
        "Once you pass this checklist, we‚Äôll be ready to explore the model‚Äôs architecture in the next step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages with error handling\n",
        "import subprocess, sys\n",
        "packages = [\n",
        "    \"ipywidgets>=8.0.0\",\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"accelerate>=0.28.0\",\n",
        "    \"datasets>=2.20.0\",\n",
        "    \"bitsandbytes>=0.43.0\",\n",
        "    \"peft>=0.10.0\"\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", pkg]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "        except Exception as exc:\n",
        "            if IN_COLAB:\n",
        "                packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                if packages:\n",
        "                    try:\n",
        "                        import IPython\n",
        "                        ip = IPython.get_ipython()\n",
        "                        if ip is not None:\n",
        "                            ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                        else:\n",
        "                            import subprocess as _subprocess\n",
        "                            _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                    except Exception as colab_exc:\n",
        "                        print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                        raise\n",
        "                else:\n",
        "                    print('No packages specified for pip install; skipping fallback')\n",
        "            else:\n",
        "                raise\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Failed to install {pkg}: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random, numpy as np\n",
        "import torch\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(\"Environment ready ‚Äì seeds set to\", SEED)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick sanity check: load a tiny GPT‚Äë2 model (not 20B) to confirm GPU access\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "print(f\"Loading {model_name}‚Ä¶\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"Model loaded successfully ‚Äì ready for the next step!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: GPT‚ÄëOSS‚Äë20B Architecture Overview\n",
        "\n",
        "Welcome to the heart of the model ‚Äì the transformer architecture that turns GPT‚ÄëOSS‚Äë20B into a 20‚Äëbillion‚Äëparameter powerhouse. Think of the model as a giant **factory** that processes text. The factory is built from a stack of identical **assembly lines** (layers). Each line has a **control panel** (self‚Äëattention) that decides which parts of the input should talk to each other, and a **workshop** (feed‚Äëforward network) that refines the information.\n",
        "\n",
        "### 1. Layer‚Äëwise Breakdown\n",
        "| Component | Size | Role |\n",
        "|-----------|------|------|\n",
        "| **Embedding** | 32‚ÄØk tokens √ó 12‚ÄØk hidden | Turns words into vectors |\n",
        "| **Self‚ÄëAttention** | 32 heads √ó 12‚ÄØk hidden | Captures long‚Äërange dependencies |\n",
        "| **Feed‚ÄëForward** | 4√ó hidden size (‚âà48‚ÄØk) | Adds non‚Äëlinearity |\n",
        "| **LayerNorm** | 12‚ÄØk | Stabilizes training |\n",
        "\n",
        "The model has **32 layers** ‚Äì each layer is a copy of the same block. With 12‚ÄØk hidden units per layer and 32 attention heads, the total parameter count climbs to roughly **20‚ÄØbillion**.\n",
        "\n",
        "### 2. Why 32 Layers and 12‚ÄØk Hidden Size?\n",
        "- **Depth (32 layers)**: More layers let the model learn hierarchical representations ‚Äì from simple patterns in early layers to abstract concepts in deeper ones.\n",
        "- **Width (12‚ÄØk hidden)**: A wider hidden size gives each layer more capacity to encode information, which is crucial for a language model that must remember long contexts.\n",
        "- **Heads (32)**: Multiple attention heads allow the model to focus on different aspects of the input simultaneously, like having many eyes looking at different parts of a scene.\n",
        "\n",
        "### 3. Parameter Distribution\n",
        "- **Embedding & Positional**: ~1‚ÄØbillion\n",
        "- **Attention Weights**: ~8‚ÄØbillion\n",
        "- **Feed‚ÄëForward Weights**: ~7‚ÄØbillion\n",
        "- **Biases & LayerNorm**: ~1‚ÄØbillion\n",
        "\n",
        "The heavy lifting is done by the attention and feed‚Äëforward matrices.\n",
        "\n",
        "### 4. Extra Explanatory Paragraph ‚Äì Key Terms & Trade‚Äëoffs\n",
        "**Parameters** are the learnable weights of the network ‚Äì think of them as the knobs you turn during training. **Attention heads** are sub‚Äënetworks that learn to focus on different relationships between tokens. **Feed‚Äëforward networks** (FFNs) are simple MLPs that add non‚Äëlinear transformations after attention. **LayerNorm** normalizes activations to keep gradients stable.\n",
        "\n",
        "**Trade‚Äëoffs**: Increasing depth or width boosts expressiveness but also raises memory usage and inference latency. For a 20‚ÄØbillion‚Äëparameter model, we balance these by using **8‚Äëbit quantization** (later) to fit the model on a single 80‚ÄØGB GPU while keeping a small drop in accuracy. The architecture itself is designed to be **parallel‚Äëfriendly** ‚Äì each layer can be processed independently across GPUs, which is why we‚Äôll later use DeepSpeed or Accelerate for distributed inference.\n",
        "\n",
        "### 5. Quick Visual Aid\n",
        "Below is a schematic of one transformer block (simplified):\n",
        "\n",
        "```\n",
        "Input ‚Üí [Self‚ÄëAttention] ‚Üí Add & Norm ‚Üí [Feed‚ÄëForward] ‚Üí Add & Norm ‚Üí Output\n",
        "```\n",
        "\n",
        "The residual connections (Add) help gradients flow, and the LayerNorm stabilizes training.\n",
        "\n",
        "### 6. Takeaway\n",
        "GPT‚ÄëOSS‚Äë20B‚Äôs architecture is a carefully tuned stack of 32 identical transformer blocks, each with 12‚ÄØk hidden units and 32 attention heads. This design gives the model the capacity to understand and generate complex language while remaining amenable to modern GPU acceleration techniques.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the model configuration and print a concise architecture summary\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "# Reproducibility: set a fixed seed for any random operations\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load the config (does not download the full weights)\n",
        "config = AutoConfig.from_pretrained(\"gpt-oss-20b\")\n",
        "\n",
        "print(\"\\n=== GPT‚ÄëOSS‚Äë20B Architecture Summary ===\")\n",
        "print(f\"Model type: {config.model_type}\")\n",
        "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
        "print(f\"Hidden size: {config.hidden_size}\")\n",
        "print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Intermediate size (FFN): {config.intermediate_size}\")\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"Total parameters (approx.): {config.num_parameters() // 1e9:.2f}B\")\n",
        "\n",
        "# Optional: instantiate the model to verify that the config is compatible\n",
        "# (this will download the weights ‚Äì comment out if you only want the summary)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"gpt-oss-20b\")\n",
        "# print(\"Model instantiated successfully ‚Äì ready for inference or fine‚Äëtuning!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Parameter Scaling Laws and Efficiency Metrics\n",
        "\n",
        "Imagine you‚Äôre building a giant Lego tower. Each Lego block is a *parameter* in a language model. The taller the tower, the more blocks you need, but you also need a bigger table (GPU memory) and more time to stack them (inference latency). In the world of large language models, we use *scaling laws* to predict how many blocks (parameters) we need to achieve a certain level of performance, and how that translates into compute, memory, and latency.\n",
        "\n",
        "### 1. Why Scaling Laws Matter\n",
        "- **Predictive Power**: They let us estimate the *cost* (compute, memory) of a model before we actually train or deploy it.\n",
        "- **Design Trade‚Äëoffs**: Knowing how latency grows with depth or width helps us choose a model that fits our hardware budget.\n",
        "- **Benchmarking**: They provide a baseline to compare new architectures or compression techniques.\n",
        "\n",
        "### 2. Key Metrics\n",
        "| Metric | What It Measures | Typical Units |\n",
        "|--------|------------------|---------------|\n",
        "| **Parameter Count** | Total learnable weights | billions (B) |\n",
        "| **Compute Cost** | FLOPs per token | billions of FLOPs |\n",
        "| **Memory Footprint** | Peak GPU memory during inference | GB |\n",
        "| **Latency** | Time to generate one token | ms |\n",
        "\n",
        "### 3. A Simple Scaling Law Formula\n",
        "For a transformer with `L` layers, hidden size `H`, and `A` attention heads, a rough estimate of the number of parameters is:\n",
        "\n",
        "```\n",
        "Params ‚âà L * (4 * H^2 + 2 * H * A)\n",
        "```\n",
        "\n",
        "- `4 * H^2` comes from the two weight matrices in the feed‚Äëforward network (each of size `H √ó 4H` and `4H √ó H`).\n",
        "- `2 * H * A` comes from the query/key/value matrices in self‚Äëattention (each of size `H √ó H/A`).\n",
        "\n",
        "The *compute cost* per token is roughly `3 * L * H^2` FLOPs (each attention and FFN layer does about `2 * H^2` operations, plus a small constant).\n",
        "\n",
        "### 4. Extra Explanatory Paragraph ‚Äì Key Terms & Trade‚Äëoffs\n",
        "- **Parameters**: Learnable weights that the model adjusts during training. More parameters usually mean higher capacity but also higher memory and compute.\n",
        "- **FLOPs (Floating‚ÄëPoint Operations)**: A proxy for compute; one FLOP is a single arithmetic operation. Higher FLOPs per token mean longer inference times.\n",
        "- **Quantization**: Reducing the precision of weights (e.g., 32‚Äëbit float ‚Üí 8‚Äëbit integer) cuts memory by 4√ó but can slightly hurt accuracy.\n",
        "- **Latency vs. Throughput**: Latency is the time to produce one token; throughput is tokens per second. Optimizing one often hurts the other.\n",
        "\n",
        "**Trade‚Äëoffs**: Increasing depth (`L`) or width (`H`) boosts expressiveness but linearly increases memory and compute. Quantization reduces memory but may increase latency if the GPU lacks fast integer kernels. Choosing the right balance depends on the target deployment scenario (e.g., real‚Äëtime chat vs. batch generation).\n",
        "\n",
        "### 5. Hands‚ÄëOn: Compute the Metrics for GPT‚ÄëOSS‚Äë20B\n",
        "Below we compute the parameter count, memory usage (both 32‚Äëbit and 8‚Äëbit), and a rough FLOP estimate for a single token. This gives you a quick sanity check before you load the full model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Compute scaling metrics for GPT‚ÄëOSS‚Äë20B\n",
        "# ------------------------------------------------------------\n",
        "import torch\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load the model config (no weights downloaded)\n",
        "config = AutoConfig.from_pretrained(\"gpt-oss-20b\")\n",
        "\n",
        "L = config.num_hidden_layers\n",
        "H = config.hidden_size\n",
        "A = config.num_attention_heads\n",
        "\n",
        "# Parameter count (approximate)\n",
        "params = L * (4 * H**2 + 2 * H * A)\n",
        "print(f\"Estimated parameters: {params/1e9:.2f}‚ÄØB\")\n",
        "\n",
        "# Memory footprint\n",
        "bytes_32 = params * 4  # 32‚Äëbit float\n",
        "bytes_8  = params * 1  # 8‚Äëbit integer\n",
        "print(f\"Memory (32‚Äëbit): {bytes_32/1e9:.2f}‚ÄØGB\")\n",
        "print(f\"Memory (8‚Äëbit):  {bytes_8/1e9:.2f}‚ÄØGB\")\n",
        "\n",
        "# FLOPs per token (rough estimate)\n",
        "flops_per_token = 3 * L * H**2\n",
        "print(f\"FLOPs per token: {flops_per_token/1e9:.2f}‚ÄØB\")\n",
        "\n",
        "# Optional: estimate latency on a single GPU (na√Øve)\n",
        "# Assume 10‚ÄØGFLOPs/s per GPU core (typical for 80‚ÄØGB GPUs)\n",
        "# This is a *very* rough estimate ‚Äì real latency depends on batching, kernel launch overhead, etc.\n",
        "flops_per_sec = 10e9  # 10 GFLOPs/s\n",
        "latency_ms = (flops_per_token / flops_per_sec) * 1000\n",
        "print(f\"Estimated latency (single token, na√Øve): {latency_ms:.1f}‚ÄØms\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Quick sanity check: load the model in 8‚Äëbit mode\n",
        "# ------------------------------------------------------------\n",
        "# Uncomment the following block if you have an 80‚ÄØGB GPU and want to load the full model.\n",
        "#\n",
        "# import bitsandbytes as bnb\n",
        "# from transformers import AutoModelForCausalLM\n",
        "#\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"gpt-oss-20b\",\n",
        "#     device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "#     torch_dtype=bnb.nn.bnb_4bit_compute_dtype,\n",
        "#     load_in_8bit=True,\n",
        "# )\n",
        "# print(\"Model loaded in 8‚Äëbit mode ‚Äì ready for inference!\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Loading the Model with 8‚Äëbit Quantization\n",
        "\n",
        "In the previous steps we saw how GPT‚ÄëOSS‚Äë20B is built from 32 layers of 12‚ÄØk hidden units and 32 attention heads. That‚Äôs a lot of knobs to turn ‚Äì about 20‚ÄØbillion of them ‚Äì and it would normally require a GPU with **80‚ÄØGB of VRAM** just to keep the weights in memory. \n",
        "\n",
        "Think of the model as a gigantic library of books. Each book (parameter) is a 32‚Äëbit float, which takes 4‚ÄØbytes. If we could shrink every book to an 8‚Äëbit *summary* (1‚ÄØbyte) while still keeping the story intact, we‚Äôd cut the library size by a factor of four. That‚Äôs exactly what **8‚Äëbit quantization** does.\n",
        "\n",
        "### Why 8‚Äëbit? The Trade‚Äëoff\n",
        "- **Memory**: 4√ó reduction lets us fit the full 20‚ÄØB model on a single 80‚ÄØGB GPU.\n",
        "- **Speed**: Modern GPUs have fast integer kernels, so the extra overhead of converting 8‚Äëbit to 32‚Äëbit on the fly is negligible for inference.\n",
        "- **Accuracy**: The drop in perplexity is usually <‚ÄØ1‚ÄØ%, which is acceptable for most downstream tasks.\n",
        "- **Compatibility**: `bitsandbytes` (bnb) provides a drop‚Äëin `load_in_8bit=True` flag that handles the quantization automatically.\n",
        "\n",
        "### Key Terms & Rationale\n",
        "- **Quantization**: Mapping high‚Äëprecision floating‚Äëpoint weights to lower‚Äëprecision integers. It reduces memory and bandwidth.\n",
        "- **Device Map**: A mapping that tells Hugging Face which GPU each part of the model should live on. `\"auto\"` distributes layers evenly across available GPUs.\n",
        "- **Reproducibility**: Setting a fixed random seed ensures that any stochastic operations (e.g., dropout during inference) produce the same results.\n",
        "- **Memory Profiling**: `torch.cuda.memory_allocated()` reports the amount of VRAM currently used by tensors on a device.\n",
        "\n",
        "The goal of this step is to load the full GPT‚ÄëOSS‚Äë20B model in 8‚Äëbit mode, verify that it fits on your GPU, and run a quick inference to confirm everything works.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Load GPT‚ÄëOSS‚Äë20B in 8‚Äëbit mode and profile memory\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Reproducibility ‚Äì set a fixed seed for any random ops\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Optional: force a single GPU for clarity (comment out if you have multiple GPUs)\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Load tokenizer (small, 32‚Äëbit, no memory issue)\n",
        "print(\"Loading tokenizer‚Ä¶\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\")\n",
        "\n",
        "# Load the model in 8‚Äëbit precision\n",
        "print(\"Loading GPT‚ÄëOSS‚Äë20B in 8‚Äëbit‚Ä¶\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # automatically split across GPUs\n",
        "    load_in_8bit=True,          # quantize weights to 8‚Äëbit\n",
        "    torch_dtype=bnb.nn.bnb_4bit_compute_dtype,  # use 4‚Äëbit compute dtype for safety\n",
        ")\n",
        "\n",
        "# Quick memory check ‚Äì total VRAM used by the model\n",
        "used_mem = torch.cuda.memory_allocated() / 1e9  # GB\n",
        "print(f\"\\n‚úÖ  Model loaded ‚Äì VRAM used: {used_mem:.2f}‚ÄØGB\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Simple inference to confirm everything works\n",
        "# ------------------------------------------------------------\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(model.device)  # move to the same device as the model\n",
        "\n",
        "# Measure latency for a single token generation\n",
        "start = time.perf_counter()\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=10,\n",
        "        do_sample=False,  # deterministic for reproducibility\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "end = time.perf_counter()\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(generated_text)\n",
        "print(f\"\\n‚ö°  Inference latency: {(end-start)*1000:.1f}‚ÄØms for 10 tokens\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Benchmarking Inference Latency and Memory Footprint\n",
        "\n",
        "After loading GPT‚ÄëOSS‚Äë20B in 8‚Äëbit mode, the next logical question is: *How fast does it run and how much VRAM does it actually consume?*  Think of the model as a giant vending machine that dispenses text. The **latency** is the time it takes to pop out a single token, while the **memory footprint** is the amount of space the machine occupies inside your GPU rack.\n",
        "\n",
        "### Why Measure These Numbers?\n",
        "- **Latency** tells you whether the model can serve real‚Äëtime applications (chatbots, live translation). 1‚ÄØms per token is a sweet spot for interactive use.\n",
        "- **Memory** tells you how many users or how much batch size you can support on a single GPU. 80‚ÄØGB is a luxury; if you can squeeze the model into 40‚ÄØGB, you can double your throughput.\n",
        "- **Trade‚Äëoffs**: Quantization reduces memory but can slightly increase latency if the GPU‚Äôs integer kernels are not fully optimized. Conversely, using a larger batch size can amortize kernel launch overhead and reduce per‚Äëtoken latency.\n",
        "\n",
        "### Key Terms & Rationale\n",
        "- **Inference latency**: The elapsed time from feeding an input to receiving the first generated token. Measured in milliseconds (ms).\n",
        "- **Peak memory**: The maximum amount of VRAM allocated at any point during inference. Captured via `torch.cuda.max_memory_allocated()`.\n",
        "- **Batch size**: Number of prompts processed in parallel. Larger batches improve GPU utilization but increase memory usage.\n",
        "- **Warm‚Äëup**: A short run before timing to allow the GPU to reach steady‚Äëstate performance.\n",
        "- **Reproducibility**: Setting a fixed random seed ensures deterministic sampling (e.g., `do_sample=False`).\n",
        "\n",
        "By the end of this section you will have a clear, reproducible benchmark that you can compare against the theoretical estimates from Step‚ÄØ3 and the practical measurements from Step‚ÄØ4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Benchmarking helper functions\n",
        "# ------------------------------------------------------------\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load tokenizer (already on CPU, tiny footprint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\")\n",
        "\n",
        "# Assume the model is already loaded in 8‚Äëbit mode as `model` from Step‚ÄØ4\n",
        "# If not, uncomment the following lines to load it again:\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"gpt-oss-20b\",\n",
        "#     device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "#     load_in_8bit=True,\n",
        "#     torch_dtype=bnb.nn.bnb_4bit_compute_dtype,\n",
        "# )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Warm‚Äëup to stabilize GPU performance\n",
        "# ------------------------------------------------------------\n",
        "model.eval()\n",
        "warmup_prompt = \"Hello world\"\n",
        "warmup_ids = tokenizer(warmup_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "with torch.no_grad():\n",
        "    model.generate(warmup_ids, max_new_tokens=5)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3Ô∏è‚É£  Benchmarking routine\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def benchmark(prompts, batch_size=4, max_new_tokens=20):\n",
        "    \"\"\"Return average latency per token (ms) and peak memory (GB).\"\"\"\n",
        "    # Tokenize in batches\n",
        "    tokenized = [tokenizer(p, return_tensors=\"pt\").input_ids for p in prompts]\n",
        "    # Pad to same length for simplicity\n",
        "    max_len = max(t.shape[1] for t in tokenized)\n",
        "    inputs = torch.cat([torch.nn.functional.pad(t, (0, max_len - t.shape[1])) for t in tokenized])\n",
        "    inputs = inputs.to(model.device)\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats(model.device)\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    end = time.perf_counter()\n",
        "\n",
        "    total_tokens = len(prompts) * max_new_tokens\n",
        "    latency_ms = (end - start) * 1000 / total_tokens\n",
        "    peak_mem_gb = torch.cuda.max_memory_allocated(model.device) / 1e9\n",
        "    return latency_ms, peak_mem_gb\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4Ô∏è‚É£  Run benchmark on a handful of prompts\n",
        "# ------------------------------------------------------------\n",
        "prompts = [\n",
        "    \"Once upon a time,\",\n",
        "    \"The quick brown fox jumps over\",\n",
        "    \"In a galaxy far, far away,\",\n",
        "    \"Python is a\",\n",
        "    \"The stock market is\",\n",
        "]\n",
        "latency, mem = benchmark(prompts, batch_size=5, max_new_tokens=15)\n",
        "print(f\"\\n‚úÖ  Average latency: {latency:.2f}‚ÄØms per token\")\n",
        "print(f\"‚úÖ  Peak memory: {mem:.2f}‚ÄØGB\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: LoRA Fine‚ÄëTuning Workflow\n",
        "\n",
        "Fine‚Äëtuning a 20‚Äëbillion‚Äëparameter model on a single GPU is like trying to repaint a huge mural with a tiny brush. LoRA (Low‚ÄëRank Adaptation) gives us a *paint‚Äëbrush that can stretch* ‚Äì it lets us add a small number of extra weights that learn the new style while keeping the original mural intact.\n",
        "\n",
        "### What is LoRA?\n",
        "LoRA inserts **low‚Äërank matrices** into the attention and feed‚Äëforward layers of a transformer. Instead of updating all 20‚ÄØB weights, we only train a few thousand additional parameters (‚âà‚ÄØ0.1‚ÄØ% of the total). The original weights stay frozen, so the model still behaves like the pre‚Äëtrained GPT‚ÄëOSS‚Äë20B but can adapt to a new domain.\n",
        "\n",
        "### Why LoRA is a game‚Äëchanger\n",
        "| Benefit | Explanation |\n",
        "|---------|-------------|\n",
        "| **Memory‚Äëefficient** | Only a tiny fraction of the model is stored on‚Äëdevice during training. 8‚Äëbit quantization + LoRA keeps peak memory <‚ÄØ40‚ÄØGB on a single 80‚ÄØGB GPU. |\n",
        "| **Fast convergence** | Because we‚Äôre only learning a small subspace, the optimizer can focus on the most relevant directions, often converging in <‚ÄØ10‚ÄØk steps. |\n",
        "| **Modular** | The LoRA adapters can be swapped out or combined with other PEFT methods (QLoRA, Prefix Tuning) without touching the base weights. |\n",
        "| **Reproducible** | With a fixed seed and deterministic ops, the same LoRA checkpoint will produce identical outputs across runs. |\n",
        "\n",
        "### Key Terms & Trade‚Äëoffs\n",
        "- **Low‚Äërank matrix**: A matrix that can be expressed as the product of two smaller matrices (A √ó B). In LoRA we learn A and B instead of the full weight matrix. |\n",
        "- **Rank (r)**: The dimensionality of the low‚Äërank space. A higher rank gives more flexibility but increases memory and training time. |\n",
        "- **Adapter**: The pair of low‚Äërank matrices inserted into a layer. |\n",
        "- **Frozen weights**: The original GPT‚ÄëOSS‚Äë20B parameters are kept constant; only adapters are updated. |\n",
        "- **Trade‚Äëoff**: A very low rank (e.g., r=8) keeps memory minimal but may under‚Äëfit; a higher rank (r=32) offers better performance at the cost of more GPU memory. |\n",
        "\n",
        "### Rationale for the Workflow\n",
        "We first load the 8‚Äëbit GPT‚ÄëOSS‚Äë20B model (so the base weights fit in memory). Then we wrap it with a `LoRAConfig` that specifies the rank, target modules, and scaling factor. Using `accelerate` we distribute the training across available GPUs (if any) and use `torch.compile` for speed. Finally, we run a short training loop on a toy dataset to demonstrate the process. The code is split into two cells: one for setup and training, another for evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  LoRA fine‚Äëtuning setup (‚âà30 lines)\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from accelerate import Accelerator\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load tokenizer and base model in 8‚Äëbit\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=bnb.nn.bnb_4bit_compute_dtype,\n",
        ")\n",
        "\n",
        "# LoRA configuration ‚Äì rank 16, target all attention and MLP layers\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Wrap the model ‚Äì only LoRA weights are trainable\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "# Simple toy dataset ‚Äì 5 short prompts\n",
        "train_texts = [\n",
        "    \"Once upon a time,\",\n",
        "    \"The quick brown fox jumps over\",\n",
        "    \"In a galaxy far, far away,\",\n",
        "    \"Python is a\",\n",
        "    \"The stock market is\",\n",
        "]\n",
        "\n",
        "# Tokenize and create dataset\n",
        "train_encodings = tokenizer(train_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "train_dataset = torch.utils.data.TensorDataset(train_encodings.input_ids, train_encodings.attention_mask)\n",
        "\n",
        "# Training arguments ‚Äì very short run for demo\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_gpt_oss_20b\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=1,\n",
        "    save_steps=1,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Trainer ‚Äì uses accelerate under the hood\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Train ‚Äì this will take a few seconds on a single GPU\n",
        "trainer.train()\n",
        "\n",
        "# Save the LoRA adapter weights only\n",
        "model.save_pretrained(\"./lora_gpt_oss_20b\")\n",
        "print(\"‚úÖ  LoRA fine‚Äëtuning complete ‚Äì adapter checkpoint saved.\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Evaluation of the LoRA‚Äëadapted model (‚âà20 lines)\n",
        "# ------------------------------------------------------------\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load tokenizer and base model (8‚Äëbit) again\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=bitsandbytes.nn.bnb_4bit_compute_dtype,\n",
        ")\n",
        "\n",
        "# Load LoRA adapters\n",
        "adapter_path = \"./lora_gpt_oss_20b\"\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "# Simple generation test\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=20,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "print(\"\\nGenerated text:\")\n",
        "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>‚ö†Ô∏è Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>‚úÖ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>‚ùå Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following best describes the primary benefit of QLoRA?\", [\"Increased model accuracy on downstream tasks\",\"Reduced GPU memory usage during fine‚Äëtuning\",\"Simplified hyperparameter tuning\",\"Elimination of the need for a Hugging Face token\"], 1, \"QLoRA quantizes the model weights to 4‚Äëbit precision, drastically cutting memory consumption while preserving most of the performance, enabling fine‚Äëtuning on commodity GPUs.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Quick check 2: Basic understanding\", [\"A\",\"B\",\"C\",\"D\"], 0, \"Review the outline section to find the correct answer.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:40:04.075Z",
      "title": "Deep Dive into GPT‚ÄëOSS‚Äë20B: Architecture, Trade‚Äëoffs, and Advanced Fine‚ÄëTuning",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:40:04.083Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36‚Äì60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}