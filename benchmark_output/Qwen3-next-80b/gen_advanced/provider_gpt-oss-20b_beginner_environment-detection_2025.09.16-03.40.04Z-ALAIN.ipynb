{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Dive into GPT‑OSS‑20B: Architecture, Trade‑offs, and Advanced Fine‑Tuning\n\nThis notebook guides advanced practitioners through the inner workings of the GPT‑OSS‑20B model, exploring its transformer architecture, parameter scaling, and practical fine‑tuning strategies. It balances theoretical depth with hands‑on code, enabling researchers to benchmark, adapt, and extend the model for domain‑specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:40:04.083Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain the architectural choices that enable GPT‑OSS‑20B to achieve 20B parameters while maintaining computational efficiency.\n",
        "2. Analyze the trade‑offs between model size, inference latency, and memory footprint on modern GPU clusters.\n",
        "3. Demonstrate advanced fine‑tuning workflows using LoRA, QLoRA, and parameter‑efficient transfer learning.\n",
        "4. Evaluate the model’s performance on benchmark datasets and design experiments to measure domain adaptation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+\n",
        "- Basic knowledge of PyTorch and Hugging Face Transformers\n",
        "- Experience with GPU programming and distributed training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.40.0 accelerate>=0.28.0 datasets>=2.20.0 bitsandbytes>=0.43.0 peft>=0.10.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.40.0\",\"accelerate>=0.28.0\",\"datasets>=2.20.0\",\"bitsandbytes>=0.43.0\",\"peft>=0.10.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Setup\n",
        "\n",
        "Welcome to the first step of our deep dive into **GPT‑OSS‑20B**. Think of GPT‑OSS‑20B as a gigantic library of 20 billion books (parameters). To read from this library efficiently, we need a well‑organized desk (the environment) and a reliable flashlight (the GPU). In this section we will:\n",
        "\n",
        "1. **Verify prerequisites** – make sure you have the right tools.\n",
        "2. **Install the required libraries** – the latest versions of `transformers`, `accelerate`, `datasets`, `bitsandbytes`, and `peft`.\n",
        "3. **Set up reproducibility** – lock down random seeds and CUDA device selection.\n",
        "4. **Load a tiny demo model** – just to confirm everything works.\n",
        "\n",
        "### Key Terms Explained\n",
        "- **Parameters**: The knobs inside a neural network that are tuned during training. 20 billion parameters is like having 20 billion adjustable dials.\n",
        "- **Quantization**: Reducing the precision of weights (e.g., from 32‑bit float to 8‑bit integer) to save memory while keeping performance close to the original.\n",
        "- **Reproducibility**: Setting random seeds and deterministic flags so that running the same code twice yields identical results.\n",
        "- **CUDA_VISIBLE_DEVICES**: An environment variable that tells PyTorch which GPUs to use.\n",
        "\n",
        "Trade‑offs: Using 8‑bit quantization cuts memory usage by ~4× but may introduce a tiny drop in accuracy. For a 20 billion‑parameter model, this trade‑off is often worth it because it allows the model to fit on a single 80 GB GPU.\n",
        "\n",
        "### Why These Steps Matter\n",
        "Setting up the environment correctly is like laying a solid foundation before building a skyscraper. A misconfigured GPU or a missing library can cause silent failures that are hard to debug later. By installing the exact versions we tested against, you avoid “works on my machine” headaches.\n",
        "\n",
        "### Quick Checklist\n",
        "- Python 3.10+ installed\n",
        "- CUDA 12.1+ and cuDNN compatible with PyTorch\n",
        "- Hugging Face access token stored in `HF_TOKEN`\n",
        "- At least one GPU visible via `CUDA_VISIBLE_DEVICES`\n",
        "\n",
        "Once you pass this checklist, we’ll be ready to explore the model’s architecture in the next step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages with error handling\n",
        "import subprocess, sys\n",
        "packages = [\n",
        "    \"ipywidgets>=8.0.0\",\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"accelerate>=0.28.0\",\n",
        "    \"datasets>=2.20.0\",\n",
        "    \"bitsandbytes>=0.43.0\",\n",
        "    \"peft>=0.10.0\"\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", pkg]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "        except Exception as exc:\n",
        "            if IN_COLAB:\n",
        "                packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                if packages:\n",
        "                    try:\n",
        "                        import IPython\n",
        "                        ip = IPython.get_ipython()\n",
        "                        if ip is not None:\n",
        "                            ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                        else:\n",
        "                            import subprocess as _subprocess\n",
        "                            _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                    except Exception as colab_exc:\n",
        "                        print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                        raise\n",
        "                else:\n",
        "                    print('No packages specified for pip install; skipping fallback')\n",
        "            else:\n",
        "                raise\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Failed to install {pkg}: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random, numpy as np\n",
        "import torch\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(\"Environment ready – seeds set to\", SEED)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick sanity check: load a tiny GPT‑2 model (not 20B) to confirm GPU access\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "print(f\"Loading {model_name}…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"Model loaded successfully – ready for the next step!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: GPT‑OSS‑20B Architecture Overview\n",
        "\n",
        "Welcome to the heart of the model – the transformer architecture that turns GPT‑OSS‑20B into a 20‑billion‑parameter powerhouse. Think of the model as a giant **factory** that processes text. The factory is built from a stack of identical **assembly lines** (layers). Each line has a **control panel** (self‑attention) that decides which parts of the input should talk to each other, and a **workshop** (feed‑forward network) that refines the information.\n",
        "\n",
        "### 1. Layer‑wise Breakdown\n",
        "| Component | Size | Role |\n",
        "|-----------|------|------|\n",
        "| **Embedding** | 32 k tokens × 12 k hidden | Turns words into vectors |\n",
        "| **Self‑Attention** | 32 heads × 12 k hidden | Captures long‑range dependencies |\n",
        "| **Feed‑Forward** | 4× hidden size (≈48 k) | Adds non‑linearity |\n",
        "| **LayerNorm** | 12 k | Stabilizes training |\n",
        "\n",
        "The model has **32 layers** – each layer is a copy of the same block. With 12 k hidden units per layer and 32 attention heads, the total parameter count climbs to roughly **20 billion**.\n",
        "\n",
        "### 2. Why 32 Layers and 12 k Hidden Size?\n",
        "- **Depth (32 layers)**: More layers let the model learn hierarchical representations – from simple patterns in early layers to abstract concepts in deeper ones.\n",
        "- **Width (12 k hidden)**: A wider hidden size gives each layer more capacity to encode information, which is crucial for a language model that must remember long contexts.\n",
        "- **Heads (32)**: Multiple attention heads allow the model to focus on different aspects of the input simultaneously, like having many eyes looking at different parts of a scene.\n",
        "\n",
        "### 3. Parameter Distribution\n",
        "- **Embedding & Positional**: ~1 billion\n",
        "- **Attention Weights**: ~8 billion\n",
        "- **Feed‑Forward Weights**: ~7 billion\n",
        "- **Biases & LayerNorm**: ~1 billion\n",
        "\n",
        "The heavy lifting is done by the attention and feed‑forward matrices.\n",
        "\n",
        "### 4. Extra Explanatory Paragraph – Key Terms & Trade‑offs\n",
        "**Parameters** are the learnable weights of the network – think of them as the knobs you turn during training. **Attention heads** are sub‑networks that learn to focus on different relationships between tokens. **Feed‑forward networks** (FFNs) are simple MLPs that add non‑linear transformations after attention. **LayerNorm** normalizes activations to keep gradients stable.\n",
        "\n",
        "**Trade‑offs**: Increasing depth or width boosts expressiveness but also raises memory usage and inference latency. For a 20 billion‑parameter model, we balance these by using **8‑bit quantization** (later) to fit the model on a single 80 GB GPU while keeping a small drop in accuracy. The architecture itself is designed to be **parallel‑friendly** – each layer can be processed independently across GPUs, which is why we’ll later use DeepSpeed or Accelerate for distributed inference.\n",
        "\n",
        "### 5. Quick Visual Aid\n",
        "Below is a schematic of one transformer block (simplified):\n",
        "\n",
        "```\n",
        "Input → [Self‑Attention] → Add & Norm → [Feed‑Forward] → Add & Norm → Output\n",
        "```\n",
        "\n",
        "The residual connections (Add) help gradients flow, and the LayerNorm stabilizes training.\n",
        "\n",
        "### 6. Takeaway\n",
        "GPT‑OSS‑20B’s architecture is a carefully tuned stack of 32 identical transformer blocks, each with 12 k hidden units and 32 attention heads. This design gives the model the capacity to understand and generate complex language while remaining amenable to modern GPU acceleration techniques.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the model configuration and print a concise architecture summary\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "# Reproducibility: set a fixed seed for any random operations\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load the config (does not download the full weights)\n",
        "config = AutoConfig.from_pretrained(\"gpt-oss-20b\")\n",
        "\n",
        "print(\"\\n=== GPT‑OSS‑20B Architecture Summary ===\")\n",
        "print(f\"Model type: {config.model_type}\")\n",
        "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
        "print(f\"Hidden size: {config.hidden_size}\")\n",
        "print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Intermediate size (FFN): {config.intermediate_size}\")\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"Total parameters (approx.): {config.num_parameters() // 1e9:.2f}B\")\n",
        "\n",
        "# Optional: instantiate the model to verify that the config is compatible\n",
        "# (this will download the weights – comment out if you only want the summary)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"gpt-oss-20b\")\n",
        "# print(\"Model instantiated successfully – ready for inference or fine‑tuning!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Parameter Scaling Laws and Efficiency Metrics\n",
        "\n",
        "Imagine you’re building a giant Lego tower. Each Lego block is a *parameter* in a language model. The taller the tower, the more blocks you need, but you also need a bigger table (GPU memory) and more time to stack them (inference latency). In the world of large language models, we use *scaling laws* to predict how many blocks (parameters) we need to achieve a certain level of performance, and how that translates into compute, memory, and latency.\n",
        "\n",
        "### 1. Why Scaling Laws Matter\n",
        "- **Predictive Power**: They let us estimate the *cost* (compute, memory) of a model before we actually train or deploy it.\n",
        "- **Design Trade‑offs**: Knowing how latency grows with depth or width helps us choose a model that fits our hardware budget.\n",
        "- **Benchmarking**: They provide a baseline to compare new architectures or compression techniques.\n",
        "\n",
        "### 2. Key Metrics\n",
        "| Metric | What It Measures | Typical Units |\n",
        "|--------|------------------|---------------|\n",
        "| **Parameter Count** | Total learnable weights | billions (B) |\n",
        "| **Compute Cost** | FLOPs per token | billions of FLOPs |\n",
        "| **Memory Footprint** | Peak GPU memory during inference | GB |\n",
        "| **Latency** | Time to generate one token | ms |\n",
        "\n",
        "### 3. A Simple Scaling Law Formula\n",
        "For a transformer with `L` layers, hidden size `H`, and `A` attention heads, a rough estimate of the number of parameters is:\n",
        "\n",
        "```\n",
        "Params ≈ L * (4 * H^2 + 2 * H * A)\n",
        "```\n",
        "\n",
        "- `4 * H^2` comes from the two weight matrices in the feed‑forward network (each of size `H × 4H` and `4H × H`).\n",
        "- `2 * H * A` comes from the query/key/value matrices in self‑attention (each of size `H × H/A`).\n",
        "\n",
        "The *compute cost* per token is roughly `3 * L * H^2` FLOPs (each attention and FFN layer does about `2 * H^2` operations, plus a small constant).\n",
        "\n",
        "### 4. Extra Explanatory Paragraph – Key Terms & Trade‑offs\n",
        "- **Parameters**: Learnable weights that the model adjusts during training. More parameters usually mean higher capacity but also higher memory and compute.\n",
        "- **FLOPs (Floating‑Point Operations)**: A proxy for compute; one FLOP is a single arithmetic operation. Higher FLOPs per token mean longer inference times.\n",
        "- **Quantization**: Reducing the precision of weights (e.g., 32‑bit float → 8‑bit integer) cuts memory by 4× but can slightly hurt accuracy.\n",
        "- **Latency vs. Throughput**: Latency is the time to produce one token; throughput is tokens per second. Optimizing one often hurts the other.\n",
        "\n",
        "**Trade‑offs**: Increasing depth (`L`) or width (`H`) boosts expressiveness but linearly increases memory and compute. Quantization reduces memory but may increase latency if the GPU lacks fast integer kernels. Choosing the right balance depends on the target deployment scenario (e.g., real‑time chat vs. batch generation).\n",
        "\n",
        "### 5. Hands‑On: Compute the Metrics for GPT‑OSS‑20B\n",
        "Below we compute the parameter count, memory usage (both 32‑bit and 8‑bit), and a rough FLOP estimate for a single token. This gives you a quick sanity check before you load the full model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Compute scaling metrics for GPT‑OSS‑20B\n",
        "# ------------------------------------------------------------\n",
        "import torch\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load the model config (no weights downloaded)\n",
        "config = AutoConfig.from_pretrained(\"gpt-oss-20b\")\n",
        "\n",
        "L = config.num_hidden_layers\n",
        "H = config.hidden_size\n",
        "A = config.num_attention_heads\n",
        "\n",
        "# Parameter count (approximate)\n",
        "params = L * (4 * H**2 + 2 * H * A)\n",
        "print(f\"Estimated parameters: {params/1e9:.2f} B\")\n",
        "\n",
        "# Memory footprint\n",
        "bytes_32 = params * 4  # 32‑bit float\n",
        "bytes_8  = params * 1  # 8‑bit integer\n",
        "print(f\"Memory (32‑bit): {bytes_32/1e9:.2f} GB\")\n",
        "print(f\"Memory (8‑bit):  {bytes_8/1e9:.2f} GB\")\n",
        "\n",
        "# FLOPs per token (rough estimate)\n",
        "flops_per_token = 3 * L * H**2\n",
        "print(f\"FLOPs per token: {flops_per_token/1e9:.2f} B\")\n",
        "\n",
        "# Optional: estimate latency on a single GPU (naïve)\n",
        "# Assume 10 GFLOPs/s per GPU core (typical for 80 GB GPUs)\n",
        "# This is a *very* rough estimate – real latency depends on batching, kernel launch overhead, etc.\n",
        "flops_per_sec = 10e9  # 10 GFLOPs/s\n",
        "latency_ms = (flops_per_token / flops_per_sec) * 1000\n",
        "print(f\"Estimated latency (single token, naïve): {latency_ms:.1f} ms\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Quick sanity check: load the model in 8‑bit mode\n",
        "# ------------------------------------------------------------\n",
        "# Uncomment the following block if you have an 80 GB GPU and want to load the full model.\n",
        "#\n",
        "# import bitsandbytes as bnb\n",
        "# from transformers import AutoModelForCausalLM\n",
        "#\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"gpt-oss-20b\",\n",
        "#     device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "#     torch_dtype=bnb.nn.bnb_4bit_compute_dtype,\n",
        "#     load_in_8bit=True,\n",
        "# )\n",
        "# print(\"Model loaded in 8‑bit mode – ready for inference!\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Loading the Model with 8‑bit Quantization\n",
        "\n",
        "In the previous steps we saw how GPT‑OSS‑20B is built from 32 layers of 12 k hidden units and 32 attention heads. That’s a lot of knobs to turn – about 20 billion of them – and it would normally require a GPU with **80 GB of VRAM** just to keep the weights in memory. \n",
        "\n",
        "Think of the model as a gigantic library of books. Each book (parameter) is a 32‑bit float, which takes 4 bytes. If we could shrink every book to an 8‑bit *summary* (1 byte) while still keeping the story intact, we’d cut the library size by a factor of four. That’s exactly what **8‑bit quantization** does.\n",
        "\n",
        "### Why 8‑bit? The Trade‑off\n",
        "- **Memory**: 4× reduction lets us fit the full 20 B model on a single 80 GB GPU.\n",
        "- **Speed**: Modern GPUs have fast integer kernels, so the extra overhead of converting 8‑bit to 32‑bit on the fly is negligible for inference.\n",
        "- **Accuracy**: The drop in perplexity is usually < 1 %, which is acceptable for most downstream tasks.\n",
        "- **Compatibility**: `bitsandbytes` (bnb) provides a drop‑in `load_in_8bit=True` flag that handles the quantization automatically.\n",
        "\n",
        "### Key Terms & Rationale\n",
        "- **Quantization**: Mapping high‑precision floating‑point weights to lower‑precision integers. It reduces memory and bandwidth.\n",
        "- **Device Map**: A mapping that tells Hugging Face which GPU each part of the model should live on. `\"auto\"` distributes layers evenly across available GPUs.\n",
        "- **Reproducibility**: Setting a fixed random seed ensures that any stochastic operations (e.g., dropout during inference) produce the same results.\n",
        "- **Memory Profiling**: `torch.cuda.memory_allocated()` reports the amount of VRAM currently used by tensors on a device.\n",
        "\n",
        "The goal of this step is to load the full GPT‑OSS‑20B model in 8‑bit mode, verify that it fits on your GPU, and run a quick inference to confirm everything works.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Load GPT‑OSS‑20B in 8‑bit mode and profile memory\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Reproducibility – set a fixed seed for any random ops\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Optional: force a single GPU for clarity (comment out if you have multiple GPUs)\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Load tokenizer (small, 32‑bit, no memory issue)\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\")\n",
        "\n",
        "# Load the model in 8‑bit precision\n",
        "print(\"Loading GPT‑OSS‑20B in 8‑bit…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # automatically split across GPUs\n",
        "    load_in_8bit=True,          # quantize weights to 8‑bit\n",
        "    torch_dtype=bnb.nn.bnb_4bit_compute_dtype,  # use 4‑bit compute dtype for safety\n",
        ")\n",
        "\n",
        "# Quick memory check – total VRAM used by the model\n",
        "used_mem = torch.cuda.memory_allocated() / 1e9  # GB\n",
        "print(f\"\\n✅  Model loaded – VRAM used: {used_mem:.2f} GB\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Simple inference to confirm everything works\n",
        "# ------------------------------------------------------------\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(model.device)  # move to the same device as the model\n",
        "\n",
        "# Measure latency for a single token generation\n",
        "start = time.perf_counter()\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=10,\n",
        "        do_sample=False,  # deterministic for reproducibility\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "end = time.perf_counter()\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(generated_text)\n",
        "print(f\"\\n⚡  Inference latency: {(end-start)*1000:.1f} ms for 10 tokens\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Benchmarking Inference Latency and Memory Footprint\n",
        "\n",
        "After loading GPT‑OSS‑20B in 8‑bit mode, the next logical question is: *How fast does it run and how much VRAM does it actually consume?*  Think of the model as a giant vending machine that dispenses text. The **latency** is the time it takes to pop out a single token, while the **memory footprint** is the amount of space the machine occupies inside your GPU rack.\n",
        "\n",
        "### Why Measure These Numbers?\n",
        "- **Latency** tells you whether the model can serve real‑time applications (chatbots, live translation). 1 ms per token is a sweet spot for interactive use.\n",
        "- **Memory** tells you how many users or how much batch size you can support on a single GPU. 80 GB is a luxury; if you can squeeze the model into 40 GB, you can double your throughput.\n",
        "- **Trade‑offs**: Quantization reduces memory but can slightly increase latency if the GPU’s integer kernels are not fully optimized. Conversely, using a larger batch size can amortize kernel launch overhead and reduce per‑token latency.\n",
        "\n",
        "### Key Terms & Rationale\n",
        "- **Inference latency**: The elapsed time from feeding an input to receiving the first generated token. Measured in milliseconds (ms).\n",
        "- **Peak memory**: The maximum amount of VRAM allocated at any point during inference. Captured via `torch.cuda.max_memory_allocated()`.\n",
        "- **Batch size**: Number of prompts processed in parallel. Larger batches improve GPU utilization but increase memory usage.\n",
        "- **Warm‑up**: A short run before timing to allow the GPU to reach steady‑state performance.\n",
        "- **Reproducibility**: Setting a fixed random seed ensures deterministic sampling (e.g., `do_sample=False`).\n",
        "\n",
        "By the end of this section you will have a clear, reproducible benchmark that you can compare against the theoretical estimates from Step 3 and the practical measurements from Step 4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Benchmarking helper functions\n",
        "# ------------------------------------------------------------\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load tokenizer (already on CPU, tiny footprint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\")\n",
        "\n",
        "# Assume the model is already loaded in 8‑bit mode as `model` from Step 4\n",
        "# If not, uncomment the following lines to load it again:\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"gpt-oss-20b\",\n",
        "#     device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "#     load_in_8bit=True,\n",
        "#     torch_dtype=bnb.nn.bnb_4bit_compute_dtype,\n",
        "# )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Warm‑up to stabilize GPU performance\n",
        "# ------------------------------------------------------------\n",
        "model.eval()\n",
        "warmup_prompt = \"Hello world\"\n",
        "warmup_ids = tokenizer(warmup_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "with torch.no_grad():\n",
        "    model.generate(warmup_ids, max_new_tokens=5)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Benchmarking routine\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def benchmark(prompts, batch_size=4, max_new_tokens=20):\n",
        "    \"\"\"Return average latency per token (ms) and peak memory (GB).\"\"\"\n",
        "    # Tokenize in batches\n",
        "    tokenized = [tokenizer(p, return_tensors=\"pt\").input_ids for p in prompts]\n",
        "    # Pad to same length for simplicity\n",
        "    max_len = max(t.shape[1] for t in tokenized)\n",
        "    inputs = torch.cat([torch.nn.functional.pad(t, (0, max_len - t.shape[1])) for t in tokenized])\n",
        "    inputs = inputs.to(model.device)\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats(model.device)\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    end = time.perf_counter()\n",
        "\n",
        "    total_tokens = len(prompts) * max_new_tokens\n",
        "    latency_ms = (end - start) * 1000 / total_tokens\n",
        "    peak_mem_gb = torch.cuda.max_memory_allocated(model.device) / 1e9\n",
        "    return latency_ms, peak_mem_gb\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Run benchmark on a handful of prompts\n",
        "# ------------------------------------------------------------\n",
        "prompts = [\n",
        "    \"Once upon a time,\",\n",
        "    \"The quick brown fox jumps over\",\n",
        "    \"In a galaxy far, far away,\",\n",
        "    \"Python is a\",\n",
        "    \"The stock market is\",\n",
        "]\n",
        "latency, mem = benchmark(prompts, batch_size=5, max_new_tokens=15)\n",
        "print(f\"\\n✅  Average latency: {latency:.2f} ms per token\")\n",
        "print(f\"✅  Peak memory: {mem:.2f} GB\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: LoRA Fine‑Tuning Workflow\n",
        "\n",
        "Fine‑tuning a 20‑billion‑parameter model on a single GPU is like trying to repaint a huge mural with a tiny brush. LoRA (Low‑Rank Adaptation) gives us a *paint‑brush that can stretch* – it lets us add a small number of extra weights that learn the new style while keeping the original mural intact.\n",
        "\n",
        "### What is LoRA?\n",
        "LoRA inserts **low‑rank matrices** into the attention and feed‑forward layers of a transformer. Instead of updating all 20 B weights, we only train a few thousand additional parameters (≈ 0.1 % of the total). The original weights stay frozen, so the model still behaves like the pre‑trained GPT‑OSS‑20B but can adapt to a new domain.\n",
        "\n",
        "### Why LoRA is a game‑changer\n",
        "| Benefit | Explanation |\n",
        "|---------|-------------|\n",
        "| **Memory‑efficient** | Only a tiny fraction of the model is stored on‑device during training. 8‑bit quantization + LoRA keeps peak memory < 40 GB on a single 80 GB GPU. |\n",
        "| **Fast convergence** | Because we’re only learning a small subspace, the optimizer can focus on the most relevant directions, often converging in < 10 k steps. |\n",
        "| **Modular** | The LoRA adapters can be swapped out or combined with other PEFT methods (QLoRA, Prefix Tuning) without touching the base weights. |\n",
        "| **Reproducible** | With a fixed seed and deterministic ops, the same LoRA checkpoint will produce identical outputs across runs. |\n",
        "\n",
        "### Key Terms & Trade‑offs\n",
        "- **Low‑rank matrix**: A matrix that can be expressed as the product of two smaller matrices (A × B). In LoRA we learn A and B instead of the full weight matrix. |\n",
        "- **Rank (r)**: The dimensionality of the low‑rank space. A higher rank gives more flexibility but increases memory and training time. |\n",
        "- **Adapter**: The pair of low‑rank matrices inserted into a layer. |\n",
        "- **Frozen weights**: The original GPT‑OSS‑20B parameters are kept constant; only adapters are updated. |\n",
        "- **Trade‑off**: A very low rank (e.g., r=8) keeps memory minimal but may under‑fit; a higher rank (r=32) offers better performance at the cost of more GPU memory. |\n",
        "\n",
        "### Rationale for the Workflow\n",
        "We first load the 8‑bit GPT‑OSS‑20B model (so the base weights fit in memory). Then we wrap it with a `LoRAConfig` that specifies the rank, target modules, and scaling factor. Using `accelerate` we distribute the training across available GPUs (if any) and use `torch.compile` for speed. Finally, we run a short training loop on a toy dataset to demonstrate the process. The code is split into two cells: one for setup and training, another for evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  LoRA fine‑tuning setup (≈30 lines)\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from accelerate import Accelerator\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load tokenizer and base model in 8‑bit\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=bnb.nn.bnb_4bit_compute_dtype,\n",
        ")\n",
        "\n",
        "# LoRA configuration – rank 16, target all attention and MLP layers\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Wrap the model – only LoRA weights are trainable\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "# Simple toy dataset – 5 short prompts\n",
        "train_texts = [\n",
        "    \"Once upon a time,\",\n",
        "    \"The quick brown fox jumps over\",\n",
        "    \"In a galaxy far, far away,\",\n",
        "    \"Python is a\",\n",
        "    \"The stock market is\",\n",
        "]\n",
        "\n",
        "# Tokenize and create dataset\n",
        "train_encodings = tokenizer(train_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "train_dataset = torch.utils.data.TensorDataset(train_encodings.input_ids, train_encodings.attention_mask)\n",
        "\n",
        "# Training arguments – very short run for demo\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_gpt_oss_20b\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=1,\n",
        "    save_steps=1,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Trainer – uses accelerate under the hood\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Train – this will take a few seconds on a single GPU\n",
        "trainer.train()\n",
        "\n",
        "# Save the LoRA adapter weights only\n",
        "model.save_pretrained(\"./lora_gpt_oss_20b\")\n",
        "print(\"✅  LoRA fine‑tuning complete – adapter checkpoint saved.\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Evaluation of the LoRA‑adapted model (≈20 lines)\n",
        "# ------------------------------------------------------------\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load tokenizer and base model (8‑bit) again\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss-20b\",\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=bitsandbytes.nn.bnb_4bit_compute_dtype,\n",
        ")\n",
        "\n",
        "# Load LoRA adapters\n",
        "adapter_path = \"./lora_gpt_oss_20b\"\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "# Simple generation test\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=20,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "print(\"\\nGenerated text:\")\n",
        "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following best describes the primary benefit of QLoRA?\", [\"Increased model accuracy on downstream tasks\",\"Reduced GPU memory usage during fine‑tuning\",\"Simplified hyperparameter tuning\",\"Elimination of the need for a Hugging Face token\"], 1, \"QLoRA quantizes the model weights to 4‑bit precision, drastically cutting memory consumption while preserving most of the performance, enabling fine‑tuning on commodity GPUs.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Quick check 2: Basic understanding\", [\"A\",\"B\",\"C\",\"D\"], 0, \"Review the outline section to find the correct answer.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:40:04.075Z",
      "title": "Deep Dive into GPT‑OSS‑20B: Architecture, Trade‑offs, and Advanced Fine‑Tuning",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:40:04.083Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}