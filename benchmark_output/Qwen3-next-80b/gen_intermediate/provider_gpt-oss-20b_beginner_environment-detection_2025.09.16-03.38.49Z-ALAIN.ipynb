{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying and Fine‑Tuning GPT‑Oss‑20B in Jupyter: A Practitioner’s Guide\n\nThis notebook walks experienced ML practitioners through the end‑to‑end workflow of loading, fine‑tuning, evaluating, and deploying the 20B‑parameter GPT‑Oss model. It emphasizes practical code snippets, GPU acceleration, and interactive demos using ipywidgets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:38:49.451Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Load GPT‑Oss‑20B efficiently with Hugging Face Transformers and Accelerate.\n",
        "2. Fine‑tune the model on a custom dataset using LoRA and PEFT.\n",
        "3. Deploy the fine‑tuned model as a REST API and create an interactive Jupyter demo.\n",
        "4. Optimize inference latency and understand best‑practice pitfalls.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+ with GPU support (CUDA 11.8 or higher).\n",
        "- Basic familiarity with PyTorch, Hugging Face Transformers, and Jupyter notebooks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 torch==2.0.0+cu118 transformers==4.40.0 accelerate==0.28.0 datasets==2.20.0 bitsandbytes==0.43.1 peft==0.6.2 flask==3.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"torch==2.0.0+cu118\",\"transformers==4.40.0\",\"accelerate==0.28.0\",\"datasets==2.20.0\",\"bitsandbytes==0.43.1\",\"peft==0.6.2\",\"flask==3.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Environment Setup\n",
        "\n",
        "Welcome to the first step of deploying GPT‑Oss‑20B in Jupyter! In this section we’ll:\n",
        "\n",
        "1. **Explain the overall workflow** – from installing the right libraries to configuring GPU acceleration.\n",
        "2. **Show you how to create a reproducible environment** that works on any machine with a CUDA‑enabled GPU.\n",
        "3. **Highlight the key terms** you’ll encounter in the rest of the notebook.\n",
        "\n",
        "Think of this as setting up a kitchen before you start cooking a complex recipe. If the stove, pans, and utensils are all in the right place, the cooking process will go smoothly.\n",
        "\n",
        "> **Why a clean environment matters**\n",
        "> \n",
        "> Large language models like GPT‑Oss‑20B are sensitive to library versions. A mismatch between `torch`, `transformers`, or `bitsandbytes` can lead to subtle bugs or even crashes. By pinning exact versions and installing them in a fresh virtual environment, we avoid the “it worked on my machine” syndrome.\n",
        "\n",
        "### Key terms and trade‑offs\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **CUDA** | NVIDIA’s parallel computing platform. | Enables GPU acceleration for deep learning. | Requires matching driver and toolkit versions. |\n",
        "| **BitsAndBytes** | Library for 8‑bit/4‑bit quantization. | Reduces memory footprint, allowing larger models on limited GPUs. | Slight loss in numerical precision; may affect generation quality. |\n",
        "| **LoRA (Low‑Rank Adaptation)** | Parameter‑efficient fine‑tuning method. | Adds only a few thousand trainable parameters. | Requires careful hyper‑parameter tuning to avoid over‑fitting. |\n",
        "| **PEFT** | Hugging Face library that implements LoRA, QLoRA, etc. | Simplifies adapter integration. | Adds an extra dependency layer. |\n",
        "| **Accelerate** | Tool for distributed and mixed‑precision training. | Handles device placement automatically. | Requires a `accelerate config` step. |\n",
        "\n",
        "Understanding these terms early on will help you make informed decisions when you tweak the training pipeline later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1️⃣ Install the required packages\n",
        "\n",
        "Below we install the exact versions that have been tested with GPT‑Oss‑20B. If you’re using a conda environment, you can replace the `pip` commands with `conda install` equivalents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install the exact library versions required for GPT‑Oss‑20B\n",
        "# The `--quiet` flag keeps the output tidy\n",
        "!pip install --quiet torch==2.0.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install --quiet transformers==4.40.0 accelerate==0.28.0 datasets==2.20.0 bitsandbytes==0.43.1 peft==0.6.2 flask==3.0.0 ipywidgets>=8.0.0\n",
        "\n",
        "# Optional: install Jupyter widgets extensions if you plan to use ipywidgets\n",
        "!pip install --quiet jupyterlab_widgets\n",
        "\n",
        "# Verify installations (optional but useful for debugging)\n",
        "import pkg_resources\n",
        "print('Installed packages:')\n",
        "for dist in pkg_resources.working_set:\n",
        "    if dist.project_name.lower() in ['torch', 'transformers', 'accelerate', 'datasets', 'bitsandbytes', 'peft', 'flask', 'ipywidgets']:\n",
        "        print(f\"  {dist.project_name}=={dist.version}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2️⃣ Configure environment variables and reproducibility\n",
        "\n",
        "Before we import the heavy libraries, we’ll set up a few safeguards:\n",
        "\n",
        "* **HF_TOKEN** – Hugging Face access token for private models.\n",
        "* **CUDA_VISIBLE_DEVICES** – Select which GPU(s) to use.\n",
        "* **Random seeds** – Ensure deterministic results across runs.\n",
        "\n",
        "If any of these are missing, the notebook will raise a clear error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import standard libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 1️⃣ Check for required environment variables\n",
        "HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "CUDA_VISIBLE_DEVICES = os.getenv('CUDA_VISIBLE_DEVICES', '0')  # default to GPU 0\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    raise EnvironmentError('HF_TOKEN is not set. Please export your Hugging Face token before running the notebook.')\n",
        "\n",
        "# 2️⃣ Set CUDA device(s)\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES\n",
        "print(f'Using GPU(s): {CUDA_VISIBLE_DEVICES}')\n",
        "\n",
        "# 3️⃣ Reproducibility settings\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 4️⃣ Optional: set float32 matmul precision for speed (PyTorch 2.0+)\n",
        "if hasattr(torch, 'set_float32_matmul_precision'):\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "print('Environment configured successfully!')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Understanding GPT‑Oss‑20B Architecture\n",
        "\n",
        "Imagine a gigantic library where every book is a *token* (a word or sub‑word). GPT‑Oss‑20B is like a super‑intelligent librarian that has read **20 billion** pages of that library. It uses a *transformer* neural network to decide what the next page (token) should be, based on all the pages it has already seen.\n",
        "\n",
        "### 1️⃣ The Transformer Skeleton\n",
        "\n",
        "| Component | What it does | Analogy |\n",
        "|-----------|--------------|---------|\n",
        "| **Embedding layer** | Turns each token into a dense vector (a point in a high‑dimensional space). | Think of it as converting a word into a unique fingerprint. |\n",
        "| **Self‑Attention** | Lets every token look at every other token in the sequence and decide how much it should pay attention to each. | Like a group discussion where each person listens to everyone else. |\n",
        "| **Feed‑Forward (FFN)** | Applies a small neural net to each token’s representation to add non‑linearity. | A quick mental calculation after the discussion. |\n",
        "| **Layer Normalization** | Keeps the activations stable across layers. | Like a referee ensuring the discussion stays on track. |\n",
        "| **Residual Connections** | Adds the input of a sub‑module to its output. | Keeps the original idea while adding new insights. |\n",
        "\n",
        "The GPT‑Oss‑20B model stacks **48** of these transformer blocks. Each block contains:\n",
        "\n",
        "- **12 attention heads** (so the librarian can focus on 12 different aspects of the conversation at once).\n",
        "- A **hidden size of 4 096** (the dimensionality of the fingerprints).\n",
        "- A **feed‑forward size of 16 384** (the size of the quick mental calculation).\n",
        "\n",
        "With 48 layers, 12 heads, and 4 096 hidden units, the total parameter count climbs to **~20 billion**. That’s why we call it GPT‑Oss‑20B.\n",
        "\n",
        "### 2️⃣ Tokenization & Vocabulary\n",
        "\n",
        "GPT‑Oss‑20B uses **Byte‑Pair Encoding (BPE)** to split text into sub‑words. The vocabulary size is **50 k** tokens. BPE is like a dictionary that learns the most common word fragments, allowing the model to handle rare words by combining familiar pieces.\n",
        "\n",
        "### 3️⃣ Why 20 B? Trade‑offs\n",
        "\n",
        "| Benefit | Trade‑off |\n",
        "|---------|-----------|\n",
        "| **Rich language understanding** | Requires huge GPU memory (≈24 GB per GPU for inference). |\n",
        "| **Better few‑shot performance** | Longer training time and higher compute cost. |\n",
        "| **Versatility across domains** | More parameters can lead to over‑fitting if fine‑tuned on small datasets. |\n",
        "\n",
        "When you decide to fine‑tune, you’ll often use *parameter‑efficient* methods (LoRA, QLoRA) to keep the memory footprint manageable.\n",
        "\n",
        "### 4️⃣ Quick sanity check: Inspect the config\n",
        "\n",
        "Below we load the model configuration (without the heavy weights) and print key hyper‑parameters. This is a lightweight operation that lets you confirm you’re looking at the right model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import the config class from Hugging Face Transformers\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# Load the configuration for GPT‑Oss‑20B (no weights downloaded)\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt-oss/gpt-oss-20b\",\n",
        "    trust_remote_code=True,  # required for custom architectures\n",
        "    use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        ")\n",
        "\n",
        "# Print a concise summary\n",
        "print(\"\\n=== GPT‑Oss‑20B Configuration ===\")\n",
        "print(f\"Model name: {config._name_or_path}\")\n",
        "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
        "print(f\"Hidden size: {config.hidden_size}\")\n",
        "print(f\"Attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Feed‑forward size: {config.intermediate_size}\")\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"Total parameters (approx.): {config.num_parameters() // 1e9:.2f} B\")\n",
        "\n",
        "# Verify reproducibility seed\n",
        "import random, numpy as np, torch\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "print(\"\\nReproducibility seeds set to 42.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the Model with Hugging Face Transformers\n",
        "\n",
        "In the previous step we inspected the configuration of GPT‑Oss‑20B. Now we’ll pull the **full weights** into memory so we can start generating text. Think of it like loading a gigantic library into a computer’s RAM: the more books you have, the more memory you need.\n",
        "\n",
        "### 1️⃣ Why do we need a *device map*?\n",
        "\n",
        "A *device map* tells Hugging Face where each part of the model should live – on which GPU or CPU. With a 20‑billion‑parameter model, a single GPU can’t hold everything. The `accelerate` library automatically shards the model across available GPUs if you set `device_map=\"auto\"`. If you only have one GPU, it will still try to keep the model in that GPU’s memory, but you’ll need to enable **mixed‑precision** (float16 or bfloat16) to fit.\n",
        "\n",
        "### 2️⃣ Mixed‑Precision and Quantization\n",
        "\n",
        "- **float16 (fp16)**: Halves the memory usage compared to float32, with a tiny drop in numerical precision. Most modern GPUs support fast fp16.\n",
        "- **bfloat16 (bf16)**: Similar to fp16 but with a larger exponent range, which can be more stable for some models.\n",
        "- **4‑bit quantization (bitsandbytes)**: Cuts memory usage by 8×, but requires the `bitsandbytes` library and a GPU that supports 4‑bit kernels.\n",
        "\n",
        "Choosing the right precision is a trade‑off: lower precision saves memory and speeds up inference, but can slightly degrade text quality.\n",
        "\n",
        "### 3️⃣ Key Terms Explained\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **device_map** | A mapping from model layers to devices (GPU/CPU). | Enables multi‑GPU sharding and prevents out‑of‑memory errors. | Requires `accelerate` to be configured; adds a small overhead during loading. |\n",
        "| **torch_dtype** | The data type used for model weights (e.g., `torch.float16`). | Controls memory footprint and compute speed. | Lower precision may introduce rounding errors. |\n",
        "| **trust_remote_code** | Allows loading custom model architectures from the Hugging Face hub. | Needed for GPT‑Oss‑20B because it uses a non‑standard config. | Potential security risk if the source is untrusted. |\n",
        "| **use_auth_token** | Passes your Hugging Face token to authenticate private model downloads. | Required for private or gated models. | Must keep the token secret. |\n",
        "| **accelerate** | A library that abstracts device placement and mixed‑precision. | Simplifies distributed training and inference. | Adds a dependency and a small startup cost. |\n",
        "\n",
        "### 4️⃣ Loading the Model\n",
        "\n",
        "Below we load the model with `device_map=\"auto\"` and `torch_dtype=torch.float16`. If you have a GPU with 24 GB of memory, this should fit comfortably. If you’re on a smaller GPU, you can switch to `bitsandbytes` 4‑bit quantization – see the optional snippet below.\n",
        "\n",
        "> **Tip**: Always check the GPU memory usage after loading. If you hit an out‑of‑memory error, try lowering the precision or using 4‑bit quantization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load GPT‑Oss‑20B with Hugging Face Transformers\n",
        "# -------------------------------------------------------\n",
        "# 1️⃣ Import required classes\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "import torch\n",
        "\n",
        "# 2️⃣ Set up the configuration\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt-oss/gpt-oss-20b\",\n",
        "    trust_remote_code=True,  # required for custom architecture\n",
        "    use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        ")\n",
        "\n",
        "# 3️⃣ Load the model with mixed‑precision and automatic sharding\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt-oss/gpt-oss-20b\",\n",
        "    config=config,\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        "    torch_dtype=torch.float16,          # fp16 for memory efficiency\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",                 # auto‑shard across available GPUs\n",
        ")\n",
        "\n",
        "# 4️⃣ Quick sanity check: print device placement\n",
        "print(\"\\n=== Model device map ===\")\n",
        "for name, device in model.named_parameters():\n",
        "    if device.device.type == \"cpu\":\n",
        "        print(f\"{name[:30]:30} -> CPU\")\n",
        "        break\n",
        "else:\n",
        "    print(\"All parameters are on GPU(s).\")\n",
        "\n",
        "# 5️⃣ Optional: enable 4‑bit quantization with bitsandbytes\n",
        "# Uncomment the following block if you have a GPU with <24GB RAM\n",
        "#\n",
        "# from bitsandbytes.nn import Linear8bitLt\n",
        "# model = model.to(\"cuda\")\n",
        "# for name, module in model.named_modules():\n",
        "#     if isinstance(module, torch.nn.Linear):\n",
        "#         module.weight = Linear8bitLt(module.weight)\n",
        "# print(\"4‑bit quantization applied.\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Measure GPU memory usage after loading\n",
        "# -------------------------------------------------\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gpu_stats = torch.cuda.memory_summary(device=0, abbreviated=False)\n",
        "    print(\"\\n=== GPU Memory Summary ===\")\n",
        "    print(gpu_stats)\n",
        "else:\n",
        "    print(\"CUDA not available – model is on CPU (this will be slow!).\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Setting Up GPU Acceleration with Accelerate\n",
        "\n",
        "When you’re working with a 20‑billion‑parameter model, the GPU is your best friend – it turns the heavy math into lightning‑fast operations. Think of **Accelerate** as a traffic‑cop that directs each part of the model to the right GPU lane, keeps the lanes balanced, and makes sure the cars (tensor operations) run smoothly.\n",
        "\n",
        "### Why use Accelerate?\n",
        "\n",
        "* **Automatic device placement** – you don’t have to manually move each layer to a GPU; Accelerate figures out the best mapping.\n",
        "* **Mixed‑precision support** – it can automatically cast weights to fp16 or bf16, saving memory.\n",
        "* **Distributed training** – if you later scale to multiple GPUs or nodes, the same config works.\n",
        "* **Simplified code** – you can keep your training loop almost identical to the single‑GPU version.\n",
        "\n",
        "### Key terms and trade‑offs\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Accelerator** | The device (GPU/CPU) that runs the tensors. | Determines compute speed and memory capacity. | More GPUs = more cost and complexity. |\n",
        "| **Device map** | A dictionary mapping model layers to accelerators. | Prevents out‑of‑memory errors by sharding. | Requires `accelerate` to be configured; adds a small startup overhead. |\n",
        "| **Mixed‑precision** | Using fp16 or bf16 instead of fp32 for weights and activations. | Cuts memory usage by ~50 % and speeds up inference. | Slight loss in numerical precision; may affect generation quality. |\n",
        "| **Gradient accumulation** | Accumulating gradients over several micro‑batches before an optimizer step. | Allows larger effective batch sizes on limited GPU memory. | Increases training time per epoch. |\n",
        "| **DistributedDataParallel (DDP)** | PyTorch wrapper that synchronizes gradients across GPUs. | Enables efficient multi‑GPU training. | Requires careful setup of environment variables and launch scripts. |\n",
        "\n",
        "Understanding these terms early on will help you make informed decisions when you tweak the training pipeline later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Install Accelerate (if not already installed)\n",
        "# ------------------------------------------------------------\n",
        "# !pip install --quiet accelerate==0.28.0\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Import libraries and set reproducibility\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Create an Accelerator instance\n",
        "# ------------------------------------------------------------\n",
        "# The `mixed_precision` flag tells Accelerate to cast weights to fp16 or bf16.\n",
        "# Use \"fp16\" for most NVIDIA GPUs, \"bf16\" for newer Ampere/Grace GPUs.\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "print(f\"Accelerator initialized: {accelerator.device_type}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Wrap a simple model for demonstration\n",
        "# ------------------------------------------------------------\n",
        "# In practice you would wrap your GPT‑Oss‑20B model here.\n",
        "# For brevity we use a tiny linear model.\n",
        "from torch import nn\n",
        "\n",
        "class DummyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = DummyModel()\n",
        "model = accelerator.prepare(model)\n",
        "print(\"Model wrapped with Accelerator – ready for training/inference.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Example training loop (single‑step)\n",
        "# ------------------------------------------------------------\n",
        "optimizer = accelerator.prepare(torch.optim.AdamW(model.parameters(), lr=1e-4))\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Dummy data\n",
        "x = torch.randn(8, 512).to(accelerator.device)\n",
        "y = torch.randn(8, 512).to(accelerator.device)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "output = model(x)\n",
        "loss = criterion(output, y)\n",
        "accelerator.backward(loss)\n",
        "optimizer.step()\n",
        "print(f\"Training step completed. Loss: {loss.item():.4f}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Running an Inference Demo\n",
        "\n",
        "Now that we have the 20‑billion‑parameter GPT‑Oss model loaded on our GPU, it’s time to see it in action. Think of the model as a gigantic, super‑fast vending machine that can produce text when you feed it a prompt. In this section we’ll:\n",
        "\n",
        "1. **Wrap the model for inference** – make sure it runs in *no‑gradient* mode to save memory.\n",
        "2. **Create a small helper function** that takes a prompt, tokenizes it, runs generation, and decodes the output.\n",
        "3. **Build an interactive widget** with `ipywidgets` so you can type prompts directly in the notebook and see the model’s response instantly.\n",
        "\n",
        "> **Why use `torch.no_grad()`?**\n",
        "> \n",
        "> During inference we never need gradients. Turning them off tells PyTorch to skip the expensive bookkeeping that would otherwise be required for back‑propagation. This cuts memory usage by roughly 50 % and speeds up the forward pass.\n",
        "\n",
        "### Key terms and trade‑offs\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Tokenizer** | Converts raw text into a sequence of integer token IDs that the model understands. | The model only works with numbers, so tokenization is the first step. | A poor tokenizer can split words oddly, hurting generation quality. |\n",
        "| **Generation parameters** (`max_new_tokens`, `temperature`, `top_k`, `top_p`) | Hyper‑parameters that control how the model samples from its probability distribution. | They let you balance creativity vs. coherence. | Too high temperature → nonsense; too low → repetitive. |\n",
        "| **`torch.no_grad()`** | Context manager that disables gradient tracking. | Saves memory and computation. | You cannot fine‑tune inside this block. |\n",
        "| **`accelerator`** | Object from `accelerate` that handles device placement. | Keeps the code portable across single‑GPU, multi‑GPU, or CPU setups. | Requires a prior `accelerate config` step. |\n",
        "| **`ipywidgets`** | Interactive UI components for Jupyter. | Lets non‑technical users play with the model without writing code. | Adds a small runtime dependency. |\n",
        "\n",
        "Understanding these terms early on helps you tweak the demo later – for example, you might want to experiment with different sampling strategies or add a temperature slider.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Import required libraries and set reproducibility\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from accelerate import Accelerator\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Prepare the accelerator (assumes it was created in Step 4)\n",
        "# ------------------------------------------------------------\n",
        "# If you ran Step 4 in the same notebook, the `accelerator` variable already exists.\n",
        "# Otherwise create a new one with the same mixed‑precision setting.\n",
        "try:\n",
        "    accelerator\n",
        "except NameError:\n",
        "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Load the tokenizer (no weights, just the vocab)\n",
        "# ------------------------------------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"gpt-oss/gpt-oss-20b\",\n",
        "    use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Helper function for inference\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def generate_text(prompt: str,\n",
        "                   max_new_tokens: int = 64,\n",
        "                   temperature: float = 0.7,\n",
        "                   top_k: int = 50,\n",
        "                   top_p: float = 0.95) -> str:\n",
        "    \"\"\"Generate a continuation for *prompt* using the loaded GPT‑Oss‑20B.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt: str\n",
        "        The text to start generation from.\n",
        "    max_new_tokens: int\n",
        "        How many tokens to generate beyond the prompt.\n",
        "    temperature: float\n",
        "        Controls randomness – higher = more creative.\n",
        "    top_k: int\n",
        "        Keep only the top‑k most probable tokens.\n",
        "    top_p: float\n",
        "        Keep the smallest set of tokens whose cumulative probability exceeds top_p.\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(accelerator.device)\n",
        "\n",
        "    # Run generation inside no‑grad context\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode the generated tokens, skipping the prompt part\n",
        "    generated = tokenizer.decode(output_ids[0, input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return generated\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Quick sanity check: generate a short response\n",
        "# ------------------------------------------------------------\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nGenerated text:\\n\", generate_text(prompt, max_new_tokens=32))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 6️⃣  Build an interactive demo with ipywidgets\n",
        "# ------------------------------------------------------------\n",
        "from ipywidgets import Textarea, Button, Output, VBox, HBox, FloatSlider, IntSlider\n",
        "from IPython.display import display\n",
        "\n",
        "# Text area for user prompt\n",
        "prompt_box = Textarea(\n",
        "    value=\"Hello, GPT‑Oss!\",\n",
        "    placeholder=\"Type your prompt here…\",\n",
        "    description=\"Prompt:\",\n",
        "    layout={'width': '100%'}\n",
        ")\n",
        "\n",
        "# Slider for temperature (creativity)\n",
        "temp_slider = FloatSlider(value=0.7, min=0.1, max=1.5, step=0.05, description=\"Temperature\")\n",
        "\n",
        "# Slider for max new tokens\n",
        "max_slider = IntSlider(value=64, min=16, max=256, step=16, description=\"Max tokens\")\n",
        "\n",
        "# Generate button\n",
        "generate_btn = Button(description=\"Generate\", button_style='success')\n",
        "\n",
        "# Output area\n",
        "output_area = Output(layout={'border': '1px solid gray', 'padding': '10px'})\n",
        "\n",
        "# Callback function\n",
        "def on_generate_clicked(_):\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "        print(\"Generating…\")\n",
        "        try:\n",
        "            result = generate_text(\n",
        "                prompt_box.value,\n",
        "                max_new_tokens=max_slider.value,\n",
        "                temperature=temp_slider.value\n",
        "            )\n",
        "            print(result)\n",
        "        except Exception as e:\n",
        "            print(\"Error during generation:\", e)\n",
        "\n",
        "generate_btn.on_click(on_generate_clicked)\n",
        "\n",
        "# Arrange widgets\n",
        "ui = VBox([\n",
        "    prompt_box,\n",
        "    HBox([temp_slider, max_slider]),\n",
        "    generate_btn,\n",
        "    output_area\n",
        "])\n",
        "\n",
        "display(ui)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Dataset Preparation for Fine‑Tuning\n",
        "\n",
        "Before we can teach GPT‑Oss‑20B a new style or domain, we need to feed it a *clean, well‑structured* dataset. Think of the dataset as a recipe book: each entry is a paragraph of text that the model will learn to imitate. Just like a chef needs fresh ingredients, the model needs high‑quality, tokenized text.\n",
        "\n",
        "### 1️⃣ Why we preprocess first\n",
        "\n",
        "* **Tokenization** turns raw text into numbers the model can understand.\n",
        "* **Cleaning** removes noise (HTML tags, emojis, etc.) that could confuse the model.\n",
        "* **Splitting** into training/validation sets lets us monitor over‑fitting.\n",
        "* **Batching** groups examples to make efficient GPU usage.\n",
        "\n",
        "### 2️⃣ Key terms and trade‑offs\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑off |\n",
        "|------|---------------|----------------|-----------|\n",
        "| **Dataset** | A collection of text examples (e.g., a CSV, JSON, or Hugging Face dataset). | Provides the raw material for learning. | Too small → under‑fitting; too large → longer training time. |\n",
        "| **Tokenizer** | The algorithm that splits text into tokens and maps them to integer IDs. | GPT‑Oss‑20B expects BPE tokens from its own tokenizer. | Using a mismatched tokenizer breaks the model. |\n",
        "| **Tokenization** | The actual conversion of strings to token IDs. | Enables the model to process text. | Over‑tokenizing (e.g., too many special tokens) can waste memory. |\n",
        "| **Train/Val split** | Dividing data into a training set and a validation set. | Allows us to check generalization. | A very small validation set may not reflect true performance. |\n",
        "| **Batch size** | Number of examples processed together. | Larger batches give more stable gradients. | Larger batches require more GPU memory; may need gradient accumulation. |\n",
        "| **Gradient accumulation** | Accumulating gradients over several micro‑batches before an optimizer step. | Lets you simulate a larger batch on limited memory. | Increases training time per epoch. |\n",
        "| **Data collator** | Function that pads sequences to the same length within a batch. | Keeps tensors rectangular for efficient GPU ops. | Padding can waste memory if max length is too high. |\n",
        "\n",
        "Understanding these terms early on helps you make informed decisions when you tweak the training pipeline later. For example, you might decide to use a smaller `max_length` to reduce padding, or increase `gradient_accumulation_steps` to keep the effective batch size high without blowing up memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Load and preprocess the dataset\n",
        "# ------------------------------------------------------------\n",
        "# We’ll use the Hugging Face `datasets` library because it handles\n",
        "# caching, shuffling, and tokenization efficiently.\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 1️⃣ Choose a dataset – here we use a public news dataset as an example\n",
        "# Replace \"wikipedia\" with your own dataset path if needed.\n",
        "raw_datasets = load_dataset(\"wikipedia\", \"20220301.en\", split={\"train\": \"train[:1%]\", \"validation\": \"validation[:1%]\"})\n",
        "# For a real project, use the full split or a custom CSV/JSON.\n",
        "\n",
        "# 2️⃣ Load the GPT‑Oss tokenizer (must match the model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"gpt-oss/gpt-oss-20b\",\n",
        "    use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# 3️⃣ Tokenization function – we keep only the input_ids and drop special tokens\n",
        "max_length = 512  # truncate long articles to fit GPU memory\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "# 4️⃣ Apply tokenization in parallel (datasets handles multiprocessing)\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"], num_proc=4)\n",
        "\n",
        "# 5️⃣ Convert to PyTorch tensors and create a DatasetDict\n",
        "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "print(\"✅ Dataset loaded and tokenized.\")\n",
        "print(f\"Training examples: {len(tokenized_datasets['train'])}\")\n",
        "print(f\"Validation examples: {len(tokenized_datasets['validation'])}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Create DataLoaders with a data collator\n",
        "# ------------------------------------------------------------\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Data collator pads to the longest sequence in the batch and masks\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # causal LM – no masked language modeling\n",
        ")\n",
        "\n",
        "# Batch size that fits a single 24GB GPU with fp16\n",
        "batch_size = 4\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    drop_last=True,  # drop incomplete batch for stable gradients\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "print(\"✅ DataLoaders ready for fine‑tuning.\")\n",
        "print(f\"Training batches per epoch: {len(train_loader)}\")\n",
        "print(f\"Validation batches per epoch: {len(val_loader)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which library is commonly used for efficient fine‑tuning of large language models?\", [\"Hugging Face Transformers\",\"PEFT\",\"PyTorch\",\"TensorFlow\"], 1, \"PEFT (Parameter‑Efficient Fine‑Tuning) provides LoRA, QLoRA, and other lightweight adapters that enable fine‑tuning large models with minimal GPU memory.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the recommended batch size for fine‑tuning GPT‑Oss‑20B on a single 24GB GPU?\", [\"1\",\"4\",\"8\",\"16\"], 0, \"Due to the 20B parameter size, a batch size of 1 (or 2 with gradient accumulation) is typically the safest choice on a 24GB GPU.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:38:49.442Z",
      "title": "Deploying and Fine‑Tuning GPT‑Oss‑20B in Jupyter: A Practitioner’s Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:38:49.451Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}