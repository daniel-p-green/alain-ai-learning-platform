{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Remix Note**: This ELI5 version was created with the Applied Learning AI Notebooks (ALAIN) Project on 09.14.2025.\\n",
        "Created by [Daniel Green](https://www.linkedin.com/in/danielpgreen).\\n",
        "---\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-5 Prompting Guide — ELI5 Remix\n\nThis guide explains how to talk to GPT-5 like you're explaining it to a 5-year-old — no coding experience needed. Using simple analogies and real-life examples, you'll learn how to get better answers by adjusting how much the AI thinks before responding, when to let it explore, and how to keep it on track."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:34:21.756Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Understand how GPT-5 behaves like a helpful but over-eager assistant who needs clear instructions.\n",
        "2. Learn to control how much the AI explores before answering using simple phrases like 'think less' or 'keep going'.\n",
        "3. Use everyday analogies (like cooking or cleaning) to craft prompts that reduce confusion and mistakes.\n",
        "4. Avoid common pitfalls such as asking vague questions or letting the AI guess too much without guidance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic internet browsing skills\n",
        "- Access to a web browser and a free account on OpenAI's chat platform\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Meet Your New Robot Friend -- GPT-5\n",
        "\n",
        "Imagine you just got a super-smart robot assistant who can answer any question — from ‘How do I tie my shoes?’ to ‘Why is the sky blue?’ But here’s the catch: this robot doesn’t read your mind. It needs clear instructions, just like a new puppy that wants to please but doesn’t know the rules yet.\n",
        "\n",
        "GPT-5 is like a friendly librarian who has read every book in the world… but still asks, ‘Do you mean fiction or science?’ when you say ‘Tell me about dinosaurs.’ It’s not being slow — it’s being careful. It tries to avoid mistakes by thinking through possibilities, which sometimes means it overthinks things. That’s okay! You’re the boss. You get to tell it how much to think before answering.\n",
        "\n",
        "Think of GPT-5 as a chef in your kitchen. If you say, ‘Make me dinner,’ it might start chopping onions, boiling water, preheating the oven, and checking your fridge… for 10 minutes. But if you say, ‘Make me pasta with tomato sauce — quick!’ it grabs the noodles and gets to work. The same AI, two different results — all because of how you asked.\n",
        "\n",
        "💡 **Key Terms Explained**: \n",
        "\n",
        "- **Prompt**: What you type to ask GPT-5 something. Think of it like a recipe card — the clearer the ingredients and steps, the better the dish. \n",
        "- **Reasoning Effort**: How hard the AI works to think before replying. High effort = deep analysis (like solving a math puzzle). Low effort = fast guess (like picking the first snack in the pantry). \n",
        "- **Agentic Behavior**: When the AI takes initiative — like deciding to look up facts, break tasks into steps, or ask follow-up questions. This is helpful… unless it goes off track. \n",
        "\n",
        "Why does this matter? Because GPT-5 isn’t magic — it’s pattern-matching. It predicts what words come next based on trillions of examples. So if you give vague prompts, it fills gaps with guesses — and sometimes those guesses are wrong. By learning to guide its thinking, you turn confusion into clarity. Trade-off: More thinking = better answers but slower. Less thinking = faster but possibly shallow. You choose the balance!\n",
        "\n",
        "Try this: Ask GPT-5 ‘What’s a good breakfast?’ Then try ‘What’s a quick, healthy breakfast I can make with eggs and toast?’ See how the second one gives a better answer? That’s prompting mastery in action.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This simulates how GPT-5 responds to vague vs clear prompts\n",
        "# In real life, you'd type these into an AI chatbox\n",
        "\n",
        "vague_prompt = \"What's a good breakfast?\"\n",
        "clear_prompt = \"What's a quick, healthy breakfast I can make with eggs and toast?\"\n",
        "\n",
        "print(\"Vague prompt response (simulated):\")\n",
        "print(\"→ Hmm, breakfast options include cereal, pancakes, yogurt, fruit...\\n\")\n",
        "\n",
        "print(\"Clear prompt response (simulated):\")\n",
        "print(\"→ Scramble 2 eggs, toast whole wheat bread, add a sprinkle of salt and pepper. Done in 5 minutes!\\n\")\n",
        "\n",
        "print(\"💡 Tip: Clear prompts = fewer surprises. Always add context like ingredients, time, or style!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: The Over-Eager Chef Analogy -- Controlling Agentic Eagerness\n",
        "\n",
        "Imagine you hired a chef who *really* wants to please you. You say, \"Make me dinner.\" Instead of grabbing the pasta, they open the fridge, check your dietary history, Google 12 recipes, compare nutritional values, call your neighbor to ask what you ate last week, and then start chopping onions… while humming a tune. You’re hungry now. They’re still organizing spices.\n",
        "\n",
        "That’s GPT-5 with high agentic eagerness. It’s not being rude — it’s being *too helpful*. It thinks: \"If I don’t do everything perfectly, I might fail my user.\" So it dives deep, explores every path, and sometimes forgets you just wanted a sandwich.\n",
        "\n",
        "This is called **agentic behavior** — when the AI takes initiative beyond your request. It’s like a robot vacuum that decides to clean your closet *and* your bookshelf because \"it looks dusty.\" Helpful? Maybe. Necessary? Not always.\n",
        "\n",
        "The trick is learning when to say: \"Just make the pasta. No research. No calls. No extra steps.\"\n",
        "\n",
        "💡 **Key Terms Explained**: \n",
        "\n",
        "- **Agentic Behavior**: When the AI decides to act on its own — like searching, planning steps, or asking follow-up questions — without being explicitly told to. It’s powered by internal reasoning loops that simulate \"thinking ahead.\" \n",
        "- **Reasoning Depth**: How many mental steps the AI takes before replying. High depth = 10-step plan. Low depth = 1-step guess. \n",
        "- **Eagerness Threshold**: The invisible setting that controls how much initiative the AI takes. Too high? It overcomplicates. Too low? It gives shallow answers. \n",
        "\n",
        "Why does this matter? Because every extra step the AI takes uses time, tokens (computing power), and increases the chance of error. Imagine a chef tasting every spice before adding one — you’ll wait 20 minutes for soup. Trade-off: More agency = better accuracy on complex tasks (like planning a trip). Less agency = faster, simpler answers (like ordering coffee). You control the dial.\n",
        "\n",
        "In real life, you can lower eagerness by saying: \"Just give me the answer — no explanations.\" Or raise it with: \"Break this down step by step and explain why.\" It’s not about right or wrong — it’s about matching the AI’s energy to your need.\n",
        "\n",
        "Try this: Ask GPT-5 \"How do I fix a leaky faucet?\" Then ask \"How do I fix a leaky faucet? Just list the 3 tools I need — no steps.\" See how the second answer is shorter but more useful when you’re in a hurry?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate two different levels of agentic eagerness\n",
        "# In real use, you'd type these prompts into a chat\n",
        "\n",
        "eager_prompt = \"How do I fix a leaky faucet?\"  # High agency: plans, explains, suggests tools\n",
        "quick_prompt = \"How do I fix a leaky faucet? Just list the 3 tools I need — no steps.\"  # Low agency: direct, minimal\n",
        "\n",
        "# Simulated outputs (what GPT-5 might return)\n",
        "print(\"=== EAGER CHEF (High Agentic Behavior) ===\")\n",
        "print(\"1. Turn off water supply under sink.\\n2. Open cabinet and find adjustable wrench.\\n3. Remove old washer — check for cracks.\\n4. Buy new rubber washer (size: 3/4 inch).\\n5. Reassemble, turn water back on, test.\\n6. Pro tip: Use plumber’s tape on threads for extra seal.\\n7. If still leaking, call a professional.\\n\")\n",
        "\n",
        "print(\"=== QUICK CHEF (Low Agentic Behavior) ===\")\n",
        "print(\"→ Adjustable wrench, rubber washer, plumber’s tape\\n\")\n",
        "\n",
        "print(\"💡 Tip: Use 'Just list...' or 'No explanations' to reduce eagerness. Use 'Explain step by step' to increase it.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: When to Say 'Stop!' -- Setting Clear Finish Lines\n",
        "\n",
        "Imagine you’re watching a toddler build a tower with blocks. They’re having fun… but they keep adding more and more blocks, even after the tower is taller than they are. It wobbles. It shakes. You know it’s going to fall. So you say: \"Stop! That’s enough!\" And the tower stays standing.\n",
        "\n",
        "GPT-5 is like that toddler — it loves to keep going. If you ask, \"Tell me about dinosaurs,\" it might start with T-Rex, then jump to how they laid eggs, then to what the Earth looked like back then, then to how fossils form, then to paleontologists’ tools… and by the time it’s done, you’ve read a whole book. You just wanted a quick fact!\n",
        "\n",
        "That’s why you need to set **finish lines** — clear signals that tell the AI: \"Stop here. I’m done. Don’t add more.\" Without them, GPT-5 keeps building blocks… even when your tower is already wobbling.\n",
        "\n",
        "The best way to set a finish line? Use phrases like:\n",
        "- \"Answer in 3 sentences.\"\n",
        "- \"List only the top 3 points.\"\n",
        "- \"Don’t explain further.\"\n",
        "- \"End after the first answer.\"\n",
        "\n",
        "These are like putting a fence around the block tower. You still get the structure you want — just without the messy overflow.\n",
        "\n",
        "💡 **Key Terms Explained**: \n",
        "\n",
        "- **Token Limit**: The maximum number of words or pieces of text the AI is allowed to generate before stopping. Think of it like a timer on a microwave — once it beeps, it stops, even if the food isn’t fully cooked. GPT-5 has a built-in limit (usually 4096 tokens), but you can guide it to stop earlier.\n",
        "- **Response Length Control**: Your ability to shape how long the answer is by using instructions like \"be brief\" or \"expand.\" This isn’t magic — it’s telling the AI where to cut off its thinking.\n",
        "- **Early Stopping**: When you force the AI to stop before it finishes its full reasoning chain. This saves time and avoids over-explaining. \n",
        "\n",
        "Why does this matter? Because every extra word GPT-5 writes uses computing power (called \"tokens\") and increases the chance of mistakes. Imagine a chef who keeps adding spices even after you said \"that’s enough\" — your soup becomes inedible. Trade-off: Short answers are fast and focused, but might miss nuance. Long answers are thorough, but can overwhelm. You decide: Is this a quick text message… or a research paper?\n",
        "\n",
        "Pro tip: If you’re unsure, start short. You can always ask \"Tell me more\" later. But if the AI goes too long, you can’t easily undo it — you have to start over. Set the finish line first, like putting a speed bump before the driveway.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate how GPT-5 responds with and without finish lines\n",
        "\n",
        "# No finish line — long, uncontrolled response\n",
        "long_prompt = \"What are the main causes of climate change?\"\n",
        "\n",
        "# With finish line — short, focused response\n",
        "short_prompt = \"What are the main causes of climate change? List only 3 in one sentence.\"\n",
        "\n",
        "print(\"=== NO FINISH LINE ===\")\n",
        "print(\"→ Climate change is driven by human activities including burning fossil fuels (coal, oil, gas), deforestation (which reduces CO2 absorption), and industrial agriculture (releasing methane and nitrous oxide). These increase greenhouse gases, trapping heat in the atmosphere. Other factors include urbanization, waste management, and transportation emissions. Scientists have confirmed this through decades of data from ice cores, satellites, and climate models.\\n\")\n",
        "\n",
        "print(\"=== WITH FINISH LINE ===\")\n",
        "print(\"→ Burning fossil fuels, deforestation, and industrial agriculture are the top three causes of climate change.\\n\")\n",
        "\n",
        "print(\"💡 Tip: Always add length limits like 'List 3' or 'One sentence' to avoid info overload.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: The Recipe Card Trick -- Tool Preambles Explained\n",
        "\n",
        "Imagine you’re watching a chef cook your favorite meal. They don’t just start chopping — they say out loud: \"First, I’ll heat the pan. Then I’ll chop the onions. Now I’m adding garlic — smell that? Perfect.\" You feel calm. You know what’s coming. You don’t panic when the sizzle starts.\n",
        "\n",
        "That’s what a **tool preamble** does for GPT-5. It’s like the AI reading its own recipe card out loud as it works. Instead of silently searching the web, calculating, or checking facts, it says: \"I’m going to look up the current weather in Tokyo... done. Now I’ll calculate the best flight time... got it. Here’s your answer.\"\n",
        "\n",
        "This isn’t just for show. It helps YOU follow along. Without it, GPT-5 might say: \"The best time to visit Tokyo is in spring.\" But you wonder: \"Wait — how did it know? Did it check the weather? Flight prices? Crowds?\" You’re left guessing. With a preamble, you see the steps — and you can trust the answer more.\n",
        "\n",
        "Think of it like a GPS that says: \"Calculating route... avoiding tolls... rerouting due to traffic... arriving in 12 minutes.\" You don’t just get the destination — you get the journey. That’s confidence.\n",
        "\n",
        "💡 **Key Terms Explained**: \n",
        "\n",
        "- **Tool Preamble**: A phrase or structure where the AI announces what external tool or action it’s about to use — like searching the web, using a calculator, or checking a database — before giving the final answer. \n",
        "- **Tool Use**: When GPT-5 accesses real-time data or external systems (like weather APIs, calculators, or code interpreters) to improve accuracy. It’s like the chef stepping out to buy fresh herbs. \n",
        "- **Transparency**: The quality of showing your work. In AI, transparency builds trust. If you see the steps, you can spot errors before they happen. \n",
        "\n",
        "Why does this matter? Because GPT-5 doesn’t know everything by heart — it sometimes needs to look things up. But if it hides that process, you can’t tell if it’s guessing or fact-checking. Trade-off: Preambles make responses longer and slower, but they’re far more reliable. For simple questions (like \"What’s 5+3?\") you don’t need one. For critical ones (like \"What’s the current stock price of Apple?\") you absolutely do. Always ask for a preamble when accuracy > speed.\n",
        "\n",
        "You can ask for one by saying: \"Show your steps,\" \"Explain how you found this,\" or \"Use tools and tell me what you did.\" GPT-5 will then act like a chef with a microphone — narrating every move.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate GPT-5 with and without tool preambles\n",
        "# In real use, you'd type these prompts into a chat\n",
        "\n",
        "prompt_no_preamble = \"What is the current population of Japan?\"\n",
        "prompt_with_preamble = \"What is the current population of Japan? Show your steps — use a tool if needed and explain what you did.\"\n",
        "\n",
        "print(\"=== NO TOOL PREAMBLE ===\")\n",
        "print(\"→ As of 2024, Japan’s population is approximately 125 million.\\n\")\n",
        "\n",
        "print(\"=== WITH TOOL PREAMBLE ===\")\n",
        "print(\"→ I’ll check the latest official data from Japan’s Statistics Bureau.\\n   [Tool: Web search] Searching 'Japan population 2024 official'...\\n   Found: Japan Statistics Bureau, April 2024 report: 124,650,000.\\n   Cross-referenced with World Bank 2023 estimate (125,780,000) — slight decline due to aging population.\\n   Final answer: Approximately 124.7 million as of early 2024.\\n\")\n",
        "\n",
        "print(\"💡 Tip: Always use 'Show your steps' for facts that change often — like prices, weather, or news.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: The ‘Think Less’ Switch -- Lowering Reasoning Effort\n",
        "\n",
        "Imagine you ask a friend, \"What’s my favorite color?\" They don’t just say \"blue.\" Instead, they start listing every color in the rainbow, checking your Instagram posts from last year, asking your mom, and even pulling out a paint swatch book. You’re standing there waiting… and you just wanted a quick answer!\n",
        "\n",
        "That’s what GPT-5 does when it thinks too much. It doesn’t know you’re in a hurry — so it runs through every possible idea before replying. But here’s the secret: you can flip a switch. Just say: **\"Think less,\"** or **\"Answer quickly,\"** and it’ll skip the deep thinking and give you the first good guess.\n",
        "\n",
        "It’s like telling your robot vacuum: \"Just clean the living room — don’t check under the couch, don’t map the whole house.\" You still get a clean floor — just faster.\n",
        "\n",
        "GPT-5 isn’t being lazy when it thinks less. It’s being *efficient*. For simple questions — like \"What time is it?\" or \"How do I boil an egg?\" — you don’t need a PhD-level analysis. You need a fast, clear reply.\n",
        "\n",
        "💡 **Key Terms Explained**: \n",
        "\n",
        "- **Reasoning Effort**: How many mental steps GPT-5 takes before replying. High effort = multi-step logic (like solving a riddle). Low effort = pattern match + quick guess (like recognizing a familiar face). \n",
        "- **Token Usage**: Each word or piece of text GPT-5 generates uses computing power called a \"token.\" More thinking = more tokens = slower response and higher cost. \n",
        "- **Latency**: The delay between your question and the answer. Lower reasoning effort = lower latency = instant replies. \n",
        "\n",
        "Why does this matter? Because sometimes speed beats perfection. If you’re texting a friend and need a quick joke, you don’t want GPT-5 to write a 500-word essay on humor theory. Trade-off: Low effort = faster but possibly less accurate. High effort = slower but deeper. Think of it like choosing between a microwave meal (fast, decent) vs. a five-star dinner (slow, amazing). Pick based on your hunger — or your deadline.\n",
        "\n",
        "Pro tip: Use phrases like \"Answer in one sentence,\" \"No explanations,\" or \"Just tell me\" to force low reasoning effort. This isn’t cheating — it’s smart prompting. You’re not dumbing down the AI; you’re matching its energy to your need.\n",
        "\n",
        "Try this: Ask GPT-5 \"Who won the 2023 World Series?\" Then ask \"Who won the 2023 World Series? Think less — just the team name.\" Notice how the second answer is shorter, sharper, and arrives instantly?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate low vs high reasoning effort responses\n",
        "# In real use, type these into your chat with GPT-5\n",
        "\n",
        "high_effort_prompt = \"Who won the 2023 World Series? Explain why, list key players, and compare to previous winners.\"\n",
        "low_effort_prompt = \"Who won the 2023 World Series? Think less — just the team name.\"\n",
        "\n",
        "print(\"=== HIGH REASONING EFFORT ===\")\n",
        "print(\"→ The Texas Rangers won the 2023 World Series, defeating the Arizona Diamondbacks in five games. This was their first championship in franchise history. Key players included Corey Seager (MVP), Adolis García, and Nathan Eovaldi. Their victory ended a 43-year drought since their only prior appearance in 1996. Compared to 2022’s Houston Astros, the Rangers relied more on power hitting than pitching dominance.\\n\")\n",
        "\n",
        "print(\"=== LOW REASONING EFFORT ===\")\n",
        "print(\"→ Texas Rangers\\n\")\n",
        "\n",
        "print(\"💡 Tip: Add 'Think less' or 'Just the answer' to any simple fact question to cut response time by 70%.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: The ‘Keep Going!’ Switch -- Boosting Persistence\n",
        "\n",
        "Imagine you’re asking a child to draw a picture of a dragon. They start sketching a head, then pause and say, \"I’m done!\" But you wanted a full dragon — wings, tail, fire, claws. You say, \"Keep going!\" And suddenly, the dragon comes alive.\n",
        "\n",
        "That’s exactly what the **‘Keep Going!’ switch** does for GPT-5. Sometimes, the AI stops too soon — even when there’s more to say. It thinks: \"I’ve given a good answer. That’s enough.\" But you? You want the *full* story. The extra details. The hidden tips. The backup plan.\n",
        "\n",
        "GPT-5 is trained to be concise. It doesn’t know you’re still hungry for more. So if you ask, \"How do I make pancakes?\" and it replies, \"Mix flour, eggs, milk, and cook on a pan,\" you might think: \"Wait — what about the heat? Do I use butter? How do I flip them?\" That’s when you flip the switch: **\"Keep going!\"** or **\"Tell me more.\"**\n",
        "\n",
        "This isn’t about being rude. It’s about *completeness*. Think of GPT-5 as a tour guide who stops at the first landmark because they think you’ve seen enough. But you’re still holding your camera, waiting for the view from the top of the hill. A simple \"Keep going!\" tells them: \"I’m not done yet. Show me the rest.\"\n",
        "\n",
        "💡 **Key Terms Explained**: \n",
        "\n",
        "- **Persistence**: How long GPT-5 continues generating content before stopping — even if it thinks it’s finished. High persistence means it keeps adding details, examples, or steps. Low persistence means it cuts off early. \n",
        "- **Response Termination**: When the AI decides its answer is \"complete\" based on internal patterns. It often stops at the first reasonable answer, not the most useful one. \n",
        "- **Prompt Extension**: The act of adding follow-up phrases like \"Keep going,\" \"Expand,\" or \"Add examples\" to push the AI beyond its default stopping point. \n",
        "\n",
        "Why does this matter? Because GPT-5’s default behavior is to be *efficient*, not *exhaustive*. It’s like a microwave that turns off when the food is warm — not when it’s piping hot. For simple questions, that’s fine. But for learning, planning, or creativity? You need the full meal. Trade-off: More persistence = richer, deeper answers… but longer responses and higher token use. If you’re writing a story, a recipe, or a study guide, you want persistence. If you’re texting a friend, you don’t. Match the switch to the task.\n",
        "\n",
        "Pro tip: Use \"Keep going!\" after any answer that feels too short. Or better yet — build it into your original prompt: \"Explain how to bake sourdough bread in detail — keep going until you’ve covered every step, including troubleshooting.\" Now you’ve set the tone from the start.\n",
        "\n",
        "Try this: Ask GPT-5 \"How do I start a garden?\" Then ask the same question but add \"Keep going!\" Watch how the second answer grows from 3 lines to a full gardening guide — soil types, seed spacing, watering schedules, pest tips, and seasonal timing. That’s persistence in action.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate GPT-5 with low vs high persistence\n",
        "# In real use, type these prompts into your chat\n",
        "\n",
        "short_prompt = \"How do I start a garden?\"\n",
        "long_prompt = \"How do I start a garden? Keep going! Include soil, seeds, watering, sunlight, pests, and seasons.\"\n",
        "\n",
        "print(\"=== LOW PERSISTENCE ===\")\n",
        "print(\"→ Buy soil, plant seeds, water daily, and give them sunlight.\\n\")\n",
        "\n",
        "print(\"=== HIGH PERSISTENCE ===\")\n",
        "print(\"→ 1. Choose a sunny spot (6+ hours of sun daily).\\n   2. Test your soil pH (ideal: 6.0–7.0); add compost if too acidic.\\n   3. Pick easy seeds like lettuce, radishes, or sunflowers.\\n   4. Plant seeds 2x deeper than their size, spaced 6–12 inches apart.\\n   5. Water gently every morning — don’t flood, just moisten the top 2 inches.\\n   6. Watch for pests: aphids = spray with soapy water; slugs = use copper tape.\\n   7. Mulch with straw to keep weeds down and moisture in.\\n   8. In spring, plant cool crops (peas, spinach); in summer, plant warm crops (tomatoes, peppers).\\n   9. Harvest when veggies are firm and colorful — don’t wait too long!\\n   10. Keep a journal: note what worked, when you planted, and how much you harvested.\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What happens if you ask GPT-5 to 'find me the best restaurant' without giving any details?\", [\"It will immediately pick one for you based on your location\",\"It will ask you clarifying questions like 'Where are you?' or 'What kind of food do you like?'\",\"It will start searching every restaurant in the world until it finds the perfect one\",\"It will refuse to answer because it doesn't know what 'best' means\"], 1, \"GPT-5 is trained to be helpful, not psychic. Without details, it will try to clarify instead of guessing — just like a good waiter would ask what you're in the mood for before recommending a dish.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which phrase tells GPT-5 to think less and respond faster?\", [\"Please research everything thoroughly before replying.\",\"Take your time — I want the most detailed answer possible.\",\"Answer quickly, even if it’s not perfect — just give me the main idea.\",\"Ask me three follow-up questions before answering.\"], 2, \"By saying 'answer quickly, even if it’s not perfect,' you’re lowering its reasoning effort — similar to telling a helper to grab the first clean towel instead of washing all of them first.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Why should you use tool preambles in your prompts?\", [\"To make GPT-5 speak louder\",\"So GPT-5 can show you its plan step-by-step, like a chef reading a recipe aloud\",\"To force GPT-5 to use more tools than needed\",\"Because it makes the AI look smarter\"], 1, \"Tool preambles act like a running commentary — they help you follow along as GPT-5 works, so you don’t feel lost. Just like hearing a chef say 'First I’ll chop the onions...' helps you understand the process.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:34:21.748Z",
      "title": "GPT-5 Prompting Guide — ELI5 Remix",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "remix": true,
    "remix_source": "gpt-5_prompting_guide.ipynb",
    "remix_date": "2025-09-14",
    "remix_by": "Daniel Green",
    "remix_by_link": "https://www.linkedin.com/in/danielpgreen",
    "created_utc": "2025-09-16T03:34:21.756Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}