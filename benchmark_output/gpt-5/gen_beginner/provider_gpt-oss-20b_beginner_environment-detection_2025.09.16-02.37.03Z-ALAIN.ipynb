{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with GPT‑OSS‑20B: A Beginner’s Guide\n\nThis lesson introduces the GPT‑OSS‑20B language model to absolute beginners. Using simple analogies and step‑by‑step instructions, learners will learn how to set up the environment, run the model in a notebook, and experiment with basic prompts—all without writing complex code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T02:37:03.128Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what GPT‑OSS‑20B is and how it works in everyday terms.\n",
        "2. Show how to install and configure the required libraries, including ipywidgets.\n",
        "3. Demonstrate how to load the model and generate text in a Jupyter notebook.\n",
        "4. Identify common pitfalls and how to avoid them when working with large language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic familiarity with Jupyter notebooks (opening a notebook, running a cell).\n",
        "- A computer with internet access and at least 8 GB of RAM (recommended 16 GB).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers==4.40.0 torch==2.2.0 accelerate==0.28.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers==4.40.0\",\"torch==2.2.0\",\"accelerate==0.28.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Meet the Model – What is GPT‑OSS‑20B?\n",
        "\n",
        "Imagine a gigantic library that has read every book, article, and conversation ever written. When you ask it a question, it doesn’t look up a single answer; instead, it *writes* a new paragraph that feels like it could have come from any of those sources. That’s what GPT‑OSS‑20B does, but in a computer.\n",
        "\n",
        "- **GPT** stands for *Generative Pre‑trained Transformer*. Think of it as a super‑smart robot that has learned grammar, facts, and even a bit of humor by reading a huge amount of text.\n",
        "- **OSS** means *Open‑Source Software*, so the code and the model weights are freely available for anyone to use and modify.\n",
        "- **20B** refers to the number of *parameters*—the tiny knobs the model turns to decide what word comes next. 20 billion is like having 20 billion tiny decision points, which is why the model can generate surprisingly coherent text.\n",
        "\n",
        "### Why 20 B? Trade‑offs in size\n",
        "Large models can produce more nuanced and context‑aware responses, but they also need more memory and compute power. A 20 B model typically requires a GPU with at least 8 GB of VRAM to run smoothly. If you try to run it on a machine with less memory, you’ll hit out‑of‑memory errors and the model will crash. That’s why we recommend a 16 GB RAM laptop or a cloud GPU instance.\n",
        "\n",
        "### Key terms explained\n",
        "- **Parameters**: The internal weights the model learns during training. More parameters usually mean better performance but higher resource usage.\n",
        "- **Transformer**: A neural network architecture that excels at processing sequences (like sentences) by paying attention to all parts of the input simultaneously.\n",
        "- **Pre‑trained**: The model has already been trained on a massive dataset before you use it, so you can start generating text right away.\n",
        "\n",
        "### Quick sanity check\n",
        "Below is a tiny snippet that just prints a friendly greeting. It’s not the model itself, but it shows how you can run Python code in a notebook.\n",
        "\n",
        "> **⚠️ Warning**: The real GPT‑OSS‑20B model is huge. Loading it in a notebook will take time and memory. The code below is only for demonstration.\n",
        "\n",
        "```python\n",
        "# Simple demo: print a greeting\n",
        "print('Hello, world!')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Importing the transformers library (make sure you installed it beforehand)\n",
        "# We set a random seed for reproducibility of any stochastic processes\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "print('Seeds set. CUDA available:', torch.cuda.is_available())\n",
        "``\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setting Up Your Notebook Environment\n",
        "\n",
        "Before we can ask GPT‑OSS‑20B to write a poem or explain a concept, we need to make sure the notebook has all the right tools installed and knows where to find the model files. Think of this like preparing a kitchen: you need the right utensils, the right ingredients, and a clean workspace before you can start cooking.\n",
        "\n",
        "### 1️⃣ Install the required libraries\n",
        "The four main packages we’ll use are:\n",
        "\n",
        "- **ipywidgets** – gives us interactive sliders, buttons, and text boxes.\n",
        "- **transformers** – the Hugging Face library that loads and runs the model.\n",
        "- **torch** – the deep‑learning backend that powers the model.\n",
        "- **accelerate** – helps us run the model on CPU or GPU efficiently.\n",
        "\n",
        "We’ll install a specific, stable version of each to avoid surprises.\n",
        "\n",
        "### 2️⃣ Create a dedicated folder for the model\n",
        "We’ll store the model weights and configuration in a folder called `~/gpt-oss-20b`. The environment variable `GPT_OSS_20B_HOME` tells the code where to look for these files.\n",
        "\n",
        "### 3️⃣ Verify your hardware\n",
        "GPT‑OSS‑20B is a 20‑billion‑parameter model. Running it on a CPU will be slow, and on a GPU with less than 8 GB of VRAM you’ll hit out‑of‑memory errors. The code below checks whether CUDA (NVIDIA GPU support) is available.\n",
        "\n",
        "### Extra explanatory paragraph – key terms and trade‑offs\n",
        "- **Environment variable**: a named value that programs can read to find out where to look for files or how to behave. Think of it as a signpost that points to the model’s home.\n",
        "- **CUDA**: a parallel computing platform that lets PyTorch use NVIDIA GPUs. If CUDA isn’t available, the model will fall back to CPU, which is much slower.\n",
        "- **VRAM**: the memory on a GPU. Large models need a lot of VRAM; otherwise they can’t load all the parameters at once.\n",
        "- **Trade‑offs**: Installing the full 20B model gives you the best text quality, but it also requires a powerful GPU and a lot of disk space. If you’re on a laptop with 8 GB of VRAM, you might consider using a smaller model or running inference on a cloud instance.\n",
        "\n",
        "### Quick sanity check\n",
        "Below is a short code snippet that installs the libraries, sets up the folder, and prints a quick CUDA status. Run it in a single cell.\n",
        "\n",
        "> **⚠️ Warning**: The `pip install` commands will download several hundred megabytes of data. Make sure you have a stable internet connection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages (run once)\n",
        "# If you already have them installed, you can skip this cell.\n",
        "!pip install --quiet ipywidgets>=8.0.0 transformers==4.40.0 torch==2.2.0 accelerate==0.28.0\n",
        "\n",
        "# Create the model directory and set the environment variable\n",
        "import os, sys\n",
        "home_dir = os.path.expanduser(\"~\")\n",
        "model_dir = os.path.join(home_dir, \"gpt-oss-20b\")\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.environ[\"GPT_OSS_20B_HOME\"] = model_dir\n",
        "print(f\"Model directory set to: {model_dir}\")\n",
        "\n",
        "# Verify CUDA availability\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"VRAM:\", round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2), \"GB\")\n",
        "else:\n",
        "    print(\"Running on CPU – expect slower inference.\")\n",
        "\n",
        "# Optional: clone the model repository if not already present\n",
        "repo_url = \"https://github.com/huggingface/transformers.git\"\n",
        "if not os.listdir(model_dir):\n",
        "    print(\"Cloning model repository…\")\n",
        "    !git clone --depth 1 {repo_url} {model_dir}\n",
        "else:\n",
        "    print(\"Model directory already contains files – skipping clone.\")\n",
        "\n",
        "# Set a reproducible seed for any future random operations\n",
        "import random, numpy as np\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "print(\"Seeds set. Ready to load GPT‑OSS‑20B.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the Model – The ‘Hello World’ of LLMs\n",
        "\n",
        "Imagine you have a gigantic library (the model) and a librarian (the tokenizer) that knows how to turn your questions into a format the library can understand. In this step we’ll bring the librarian and the library into our notebook so we can ask a simple question and see the answer.\n",
        "\n",
        "### 1️⃣ What we’re actually doing\n",
        "1. **Load the tokenizer** – the piece of code that turns plain text into numbers the model can read.\n",
        "2. **Load the model weights** – the 20 billion knobs that decide what word comes next.\n",
        "3. **Move everything to the right device** – GPU if available, otherwise CPU.\n",
        "4. **Run a tiny inference** – ask the model a question and print the reply.\n",
        "\n",
        "### 2️⃣ Why this is the “Hello World” of LLMs\n",
        "Just like printing “Hello, world!” in a new programming language, loading a large language model is the first step that shows everything is wired correctly. If the model loads and gives a coherent answer, you know your environment, dependencies, and hardware are all set.\n",
        "\n",
        "### 3️⃣ Extra explanatory paragraph – key terms and trade‑offs\n",
        "- **Tokenizer**: A mapping from words or sub‑words to integer IDs. Think of it as a dictionary that the model uses to read and write.\n",
        "- **Model weights**: The 20 billion parameters that were learned during training. They’re stored in a file that can be several gigabytes.\n",
        "- **Device**: The hardware (CPU or GPU) where the tensors live. GPUs are faster but need enough VRAM; CPUs are slower but always available.\n",
        "- **Half‑precision (fp16)**: A way to store numbers using 16 bits instead of 32, cutting memory usage in half with a small loss in precision. It’s a common trade‑off for large models.\n",
        "- **Generation**: The process of turning input tokens into output tokens. It can be tuned with temperature, max length, etc.\n",
        "\n",
        "### 4️⃣ Quick sanity check\n",
        "Below is a short code snippet that loads the tokenizer and model, runs a single prompt, and prints the result. It’s intentionally tiny so you can copy‑paste it into a single cell and run it.\n",
        "\n",
        "> **⚠️ Warning**: Loading the full 20 B model will take a few minutes and may require 8 GB+ of GPU memory. If you hit an out‑of‑memory error, try running on CPU or using a smaller model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Import libraries and set reproducible seeds\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Reproducibility: same random numbers each run\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# 2️⃣ Define the model name and device\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# 3️⃣ Load tokenizer (fast tokenizer is usually faster)\n",
        "print(\"Loading tokenizer…\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "except Exception as e:\n",
        "    print(\"Error loading tokenizer:\", e)\n",
        "    raise\n",
        "\n",
        "# 4️⃣ Load model weights – use fp16 if GPU available to save memory\n",
        "print(\"Loading model…\")\n",
        "model_kwargs = {\"torch_dtype\": torch.float16} if torch.cuda.is_available() else {}\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# 5️⃣ Run a tiny inference\n",
        "prompt = \"What is the capital of France?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\nPrompt:\", prompt)\n",
        "print(\"\\nResponse:\", generated_text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Building a Simple Prompt Interface with ipywidgets\n",
        "\n",
        "In the previous step we saw how to ask the model a single question from code.  Now we’ll turn that into a little *chat window* that lives inside the notebook, so you can type any prompt you like and see the answer instantly.\n",
        "\n",
        "### Why ipywidgets?\n",
        "Think of ipywidgets as a set of building blocks that let you add sliders, buttons, and text boxes to a notebook without writing a web page.  It’s like having a toolbox that lets you assemble a tiny interactive app right inside the cell you’re already working in.\n",
        "\n",
        "### The big picture\n",
        "1. **Create UI elements** – a text area for the prompt, a button to trigger generation, and an output area to show the reply.\n",
        "2. **Hook them together** – write a small function that takes the text from the prompt box, feeds it to the model, and writes the result to the output area.\n",
        "3. **Run the interface** – the user can now type any question, click *Generate*, and watch the model answer.\n",
        "\n",
        "### Extra explanatory paragraph – key terms and trade‑offs\n",
        "- **ipywidgets**: A Python library that provides interactive widgets for Jupyter.  It runs in the browser but communicates with the kernel via JavaScript.\n",
        "- **Output widget**: A special widget that can display arbitrary HTML or plain text.  It’s useful for showing model responses without cluttering the notebook.\n",
        "- **Event handling**: When the user clicks the button, an *event* is fired and a callback function runs.  This is how the UI stays responsive.\n",
        "- **Trade‑offs**: Using ipywidgets keeps everything in the notebook, which is great for quick experiments.  However, the interface is limited to what widgets provide; for a full‑blown chat app you’d eventually move to a web framework like Streamlit or Gradio.\n",
        "\n",
        "### Quick sanity check\n",
        "Below is a short code snippet that builds the interface.  It assumes the `model` and `tokenizer` objects from Step 3 are already loaded in the kernel.  If you’re starting fresh, run the loading code from Step 3 first.\n",
        "\n",
        "> **⚠️ Warning**: The button will trigger a full forward pass through the 20 B model, which can take several seconds and will use a chunk of GPU memory.  Keep an eye on your GPU usage in the system monitor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Import widgets and set up the UI\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# 2️⃣ Create the prompt text area, generate button, and output area\n",
        "prompt_box = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Type your question here…',\n",
        "    description='Prompt:',\n",
        "    layout=widgets.Layout(width='100%', height='80px')\n",
        ")\n",
        "\n",
        "generate_btn = widgets.Button(\n",
        "    description='Generate',\n",
        "    button_style='success',\n",
        "    tooltip='Click to ask the model',\n",
        "    icon='paper-plane'\n",
        ")\n",
        "\n",
        "output_area = widgets.Output(layout=widgets.Layout(border='1px solid #ddd', padding='10px'))\n",
        "\n",
        "# 3️⃣ Define the callback that runs when the button is clicked\n",
        "@widgets.interactive_output\n",
        "def generate_response(prompt: str = prompt_box.value):\n",
        "    # Clear previous output\n",
        "    output_area.clear_output()\n",
        "    with output_area:\n",
        "        print(f\"\\n**Prompt:** {prompt}\\n\")\n",
        "        # Tokenize and generate – use GPU if available\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            generated = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                do_sample=True\n",
        "            )\n",
        "        response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "        print(\"**Response:**\", response)\n",
        "\n",
        "# 4️⃣ Wire the button to the callback\n",
        "generate_btn.on_click(lambda _: generate_response(prompt=prompt_box.value))\n",
        "\n",
        "# 5️⃣ Display the UI components\n",
        "ui = widgets.VBox([prompt_box, generate_btn, output_area])\n",
        "display(ui)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generating Text – From Prompt to Response\n",
        "\n",
        "Generating text with GPT‑OSS‑20B is like asking a very clever friend to write a story. You give them a starting sentence (the *prompt*) and they finish it for you. The way they finish it depends on a few knobs you can turn:\n",
        "\n",
        "- **Temperature** – how wildly the friend can deviate from the most obvious next word. A low temperature (≈0.2) makes the reply safe and predictable; a high temperature (≈1.0) makes it more creative but also more random.\n",
        "- **Top‑p (nucleus sampling)** – instead of picking the single most likely word, the friend looks at the smallest set of words that together make up *p* percent of the probability mass. This keeps the reply coherent while still allowing some surprise.\n",
        "- **Max new tokens** – the maximum length of the reply. Think of it as setting a word‑limit for your friend.\n",
        "\n",
        "### Extra explanatory paragraph – key terms and trade‑offs\n",
        "- **Sampling**: The process of choosing the next word from a probability distribution. Deterministic generation (e.g., `do_sample=False`) always picks the highest‑probability word, which can lead to bland, repetitive text. Stochastic sampling (e.g., `do_sample=True`) introduces variety but can also produce nonsensical outputs if the temperature is too high.\n",
        "- **Beam search**: A deterministic alternative that keeps multiple candidate sequences in parallel. It can improve quality for short responses but is memory‑heavy and slower for long generations.\n",
        "- **Trade‑offs**: Higher temperature and larger `max_new_tokens` increase GPU memory usage and inference time. Lower temperature and shorter outputs are faster and safer but may feel robotic. Choosing the right balance depends on the task: creative writing vs. factual answering.\n",
        "\n",
        "Below we’ll show how to experiment with these knobs in a single, easy‑to‑copy code cell. The code uses the `model` and `tokenizer` objects you loaded in Step 3, so make sure they’re still in the kernel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Set up reproducible seeds for generation\n",
        "import random, numpy as np, torch\n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# 2️⃣ Define a helper function that runs generation with custom settings\n",
        "\n",
        "def generate_text(prompt, temperature=0.7, top_p=0.9, max_new_tokens=60, do_sample=True):\n",
        "    \"\"\"Generate a response from GPT‑OSS‑20B.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt : str\n",
        "        The user‑supplied prompt.\n",
        "    temperature : float\n",
        "        Controls randomness; lower = more deterministic.\n",
        "    top_p : float\n",
        "        Nucleus sampling threshold.\n",
        "    max_new_tokens : int\n",
        "        Maximum number of tokens to generate.\n",
        "    do_sample : bool\n",
        "        Whether to sample or use greedy decoding.\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    # Run generation\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=do_sample,\n",
        "            pad_token_id=tokenizer.eos_token_id,  # avoid warning on long outputs\n",
        "        )\n",
        "\n",
        "    # Decode and return\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# 3️⃣ Example prompts with different settings\n",
        "prompts = [\n",
        "    \"Explain quantum computing in simple terms.\",\n",
        "    \"Write a short poem about autumn leaves.\",\n",
        "    \"Generate a recipe for vegan lasagna.\"\n",
        "]\n",
        "\n",
        "# 4️⃣ Run each prompt with two different temperature settings\n",
        "for i, p in enumerate(prompts, 1):\n",
        "    print(f\"\\n--- Prompt {i}: {p}\\n\")\n",
        "    for temp in [0.3, 1.0]:\n",
        "        print(f\"Temperature={temp} →\")\n",
        "        print(generate_text(p, temperature=temp, top_p=0.95, max_new_tokens=80))\n",
        "        print(\"\\n---\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Common Pitfalls and How to Avoid Them\n",
        "\n",
        "When you’re working with a 20‑billion‑parameter model, a few small mistakes can turn a smooth experiment into a frustrating crash.  Think of the model as a giant, very heavy backpack: if you forget to strap it on correctly, it will wobble, spill its contents, or even break the floor.  Below we list the most frequent missteps, explain why they happen, and show you the right way to do it.\n",
        "\n",
        "### 1️⃣ Forgetting to move everything to the right device\n",
        "If you load the model on the GPU but forget to move the input tensors to the same device, PyTorch will silently copy data back and forth, eating time and memory.  The result?  Slower inference and, on a GPU with limited VRAM, an out‑of‑memory (OOM) error.\n",
        "\n",
        "### 2️⃣ Using the default 32‑bit precision on a large model\n",
        "A 20 B model in float32 needs roughly 80 GB of memory – far beyond what most GPUs can hold.  Even with a 16 GB GPU, you’ll hit OOM unless you switch to half‑precision (fp16) or use a quantized version.  The trade‑off is a tiny loss in numerical precision, which is usually negligible for text generation.\n",
        "\n",
        "### 3️⃣ Not setting the model to evaluation mode\n",
        "During training the model keeps track of gradients, but during inference you don’t need them.  Leaving the model in training mode (`model.train()`) keeps gradient buffers alive and can double memory usage.  Always call `model.eval()` before generating.\n",
        "\n",
        "### 4️⃣ Forgetting `torch.no_grad()` during inference\n",
        "Without `torch.no_grad()`, PyTorch records every operation for back‑propagation, which again bloats memory.  Wrap your generation code in a `with torch.no_grad():` block.\n",
        "\n",
        "### 5️⃣ Ignoring the `pad_token_id` warning\n",
        "When the generated text exceeds the model’s context window, the tokenizer may complain about missing `pad_token_id`.  Supplying it prevents a noisy warning and ensures consistent decoding.\n",
        "\n",
        "### 6️⃣ Using too high a temperature or `max_new_tokens`\n",
        "A temperature above 1.2 can produce gibberish, while a very large `max_new_tokens` can exhaust GPU memory.  Start with moderate values (temperature ≈ 0.7, max_new_tokens ≈ 50) and adjust only if needed.\n",
        "\n",
        "### Key terms and trade‑offs\n",
        "- **Device**: The hardware (CPU or GPU) where tensors live.  GPUs are faster but limited by VRAM.\n",
        "- **Precision (fp32 vs fp16)**: fp32 uses 32 bits per number; fp16 uses 16 bits, cutting memory usage in half at a small precision cost.\n",
        "- **Evaluation mode**: `model.eval()` disables dropout and gradient buffers, saving memory.\n",
        "- **Gradient tracking**: `torch.no_grad()` tells PyTorch not to record operations for back‑propagation.\n",
        "- **Pad token**: A special token that tells the tokenizer how to pad sequences; required for some generation methods.\n",
        "- **Temperature**: Controls randomness; lower values → deterministic, higher values → creative.\n",
        "- **Max new tokens**: Caps the length of the generated text.\n",
        "\n",
        "By keeping these points in mind, you’ll avoid the most common headaches and keep your experiments running smoothly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Reproducible seeds for all random operations\n",
        "import random, numpy as np, torch\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# 2️⃣ Load the model correctly – GPU + fp16, eval mode, no_grad\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Load model with fp16 if GPU available, otherwise fp32\n",
        "print(\"Loading model…\")\n",
        "model_kwargs = {\"torch_dtype\": torch.float16} if torch.cuda.is_available() else {}\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)\n",
        "model.to(DEVICE)\n",
        "model.eval()  # 3️⃣ Switch to evaluation mode\n",
        "\n",
        "# 4️⃣ Example prompt\n",
        "prompt = \"Explain the concept of a black hole in simple terms.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "# 5️⃣ Generate with proper context handling\n",
        "with torch.no_grad():  # 4️⃣ Disable gradient tracking\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=60,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,  # 5️⃣ Avoid pad_token_id warning\n",
        "    )\n",
        "\n",
        "# 6️⃣ Decode and display\n",
        "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\n**Prompt:**\", prompt)\n",
        "print(\"\\n**Response:**\", response)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following is NOT a recommended step when setting up GPT‑OSS‑20B?\", [\"Install ipywidgets>=8.0.0\",\"Set the environment variable GPT_OSS_20B_HOME\",\"Use a GPU with less than 4 GB VRAM\",\"Clone the model repository into the home directory\"], 2, \"GPT‑OSS‑20B requires a GPU with at least 8 GB VRAM for smooth inference; using less than 4 GB can lead to out‑of‑memory errors.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Quick check 2: Basic understanding\", [\"A\",\"B\",\"C\",\"D\"], 0, \"Review the outline section to find the correct answer.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:37:03.121Z",
      "title": "Getting Started with GPT‑OSS‑20B: A Beginner’s Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T02:37:03.128Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}