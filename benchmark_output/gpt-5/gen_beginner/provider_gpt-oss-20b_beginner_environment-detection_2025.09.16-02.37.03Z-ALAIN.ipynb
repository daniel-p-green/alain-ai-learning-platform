{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîß Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('üìù Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîê Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'üîè Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '‚àÖ'\\n    return v[:3] + '‚Ä¶' + v[-2:] if len(v) > 6 else '‚Ä¢‚Ä¢‚Ä¢'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üåê ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('‚úÖ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('‚ö†Ô∏è Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîé Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('‚ö†Ô∏è Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('‚úÖ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('‚ö†Ô∏è Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) ‚Äî 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with GPT‚ÄëOSS‚Äë20B: A Beginner‚Äôs Guide\n\nThis lesson introduces the GPT‚ÄëOSS‚Äë20B language model to absolute beginners. Using simple analogies and step‚Äëby‚Äëstep instructions, learners will learn how to set up the environment, run the model in a notebook, and experiment with basic prompts‚Äîall without writing complex code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ‚è±Ô∏è Estimated time to complete: 36‚Äì60 minutes (rough).  ",
        "\n> üïí Created (UTC): 2025-09-16T02:37:03.128Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what GPT‚ÄëOSS‚Äë20B is and how it works in everyday terms.\n",
        "2. Show how to install and configure the required libraries, including ipywidgets.\n",
        "3. Demonstrate how to load the model and generate text in a Jupyter notebook.\n",
        "4. Identify common pitfalls and how to avoid them when working with large language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic familiarity with Jupyter notebooks (opening a notebook, running a cell).\n",
        "- A computer with internet access and at least 8‚ÄØGB of RAM (recommended 16‚ÄØGB).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers==4.40.0 torch==2.2.0 accelerate==0.28.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers==4.40.0\",\"torch==2.2.0\",\"accelerate==0.28.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('‚úÖ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Meet the Model ‚Äì What is GPT‚ÄëOSS‚Äë20B?\n",
        "\n",
        "Imagine a gigantic library that has read every book, article, and conversation ever written. When you ask it a question, it doesn‚Äôt look up a single answer; instead, it *writes* a new paragraph that feels like it could have come from any of those sources. That‚Äôs what GPT‚ÄëOSS‚Äë20B does, but in a computer.\n",
        "\n",
        "- **GPT** stands for *Generative Pre‚Äëtrained Transformer*. Think of it as a super‚Äësmart robot that has learned grammar, facts, and even a bit of humor by reading a huge amount of text.\n",
        "- **OSS** means *Open‚ÄëSource Software*, so the code and the model weights are freely available for anyone to use and modify.\n",
        "- **20B** refers to the number of *parameters*‚Äîthe tiny knobs the model turns to decide what word comes next. 20‚ÄØbillion is like having 20‚ÄØbillion tiny decision points, which is why the model can generate surprisingly coherent text.\n",
        "\n",
        "### Why 20‚ÄØB? Trade‚Äëoffs in size\n",
        "Large models can produce more nuanced and context‚Äëaware responses, but they also need more memory and compute power. A 20‚ÄØB model typically requires a GPU with at least 8‚ÄØGB of VRAM to run smoothly. If you try to run it on a machine with less memory, you‚Äôll hit out‚Äëof‚Äëmemory errors and the model will crash. That‚Äôs why we recommend a 16‚ÄØGB RAM laptop or a cloud GPU instance.\n",
        "\n",
        "### Key terms explained\n",
        "- **Parameters**: The internal weights the model learns during training. More parameters usually mean better performance but higher resource usage.\n",
        "- **Transformer**: A neural network architecture that excels at processing sequences (like sentences) by paying attention to all parts of the input simultaneously.\n",
        "- **Pre‚Äëtrained**: The model has already been trained on a massive dataset before you use it, so you can start generating text right away.\n",
        "\n",
        "### Quick sanity check\n",
        "Below is a tiny snippet that just prints a friendly greeting. It‚Äôs not the model itself, but it shows how you can run Python code in a notebook.\n",
        "\n",
        "> **‚ö†Ô∏è Warning**: The real GPT‚ÄëOSS‚Äë20B model is huge. Loading it in a notebook will take time and memory. The code below is only for demonstration.\n",
        "\n",
        "```python\n",
        "# Simple demo: print a greeting\n",
        "print('Hello, world!')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Importing the transformers library (make sure you installed it beforehand)\n",
        "# We set a random seed for reproducibility of any stochastic processes\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "print('Seeds set. CUDA available:', torch.cuda.is_available())\n",
        "``\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setting Up Your Notebook Environment\n",
        "\n",
        "Before we can ask GPT‚ÄëOSS‚Äë20B to write a poem or explain a concept, we need to make sure the notebook has all the right tools installed and knows where to find the model files. Think of this like preparing a kitchen: you need the right utensils, the right ingredients, and a clean workspace before you can start cooking.\n",
        "\n",
        "### 1Ô∏è‚É£ Install the required libraries\n",
        "The four main packages we‚Äôll use are:\n",
        "\n",
        "- **ipywidgets** ‚Äì gives us interactive sliders, buttons, and text boxes.\n",
        "- **transformers** ‚Äì the Hugging Face library that loads and runs the model.\n",
        "- **torch** ‚Äì the deep‚Äëlearning backend that powers the model.\n",
        "- **accelerate** ‚Äì helps us run the model on CPU or GPU efficiently.\n",
        "\n",
        "We‚Äôll install a specific, stable version of each to avoid surprises.\n",
        "\n",
        "### 2Ô∏è‚É£ Create a dedicated folder for the model\n",
        "We‚Äôll store the model weights and configuration in a folder called `~/gpt-oss-20b`. The environment variable `GPT_OSS_20B_HOME` tells the code where to look for these files.\n",
        "\n",
        "### 3Ô∏è‚É£ Verify your hardware\n",
        "GPT‚ÄëOSS‚Äë20B is a 20‚Äëbillion‚Äëparameter model. Running it on a CPU will be slow, and on a GPU with less than 8‚ÄØGB of VRAM you‚Äôll hit out‚Äëof‚Äëmemory errors. The code below checks whether CUDA (NVIDIA GPU support) is available.\n",
        "\n",
        "### Extra explanatory paragraph ‚Äì key terms and trade‚Äëoffs\n",
        "- **Environment variable**: a named value that programs can read to find out where to look for files or how to behave. Think of it as a signpost that points to the model‚Äôs home.\n",
        "- **CUDA**: a parallel computing platform that lets PyTorch use NVIDIA GPUs. If CUDA isn‚Äôt available, the model will fall back to CPU, which is much slower.\n",
        "- **VRAM**: the memory on a GPU. Large models need a lot of VRAM; otherwise they can‚Äôt load all the parameters at once.\n",
        "- **Trade‚Äëoffs**: Installing the full 20B model gives you the best text quality, but it also requires a powerful GPU and a lot of disk space. If you‚Äôre on a laptop with 8‚ÄØGB of VRAM, you might consider using a smaller model or running inference on a cloud instance.\n",
        "\n",
        "### Quick sanity check\n",
        "Below is a short code snippet that installs the libraries, sets up the folder, and prints a quick CUDA status. Run it in a single cell.\n",
        "\n",
        "> **‚ö†Ô∏è Warning**: The `pip install` commands will download several hundred megabytes of data. Make sure you have a stable internet connection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages (run once)\n",
        "# If you already have them installed, you can skip this cell.\n",
        "!pip install --quiet ipywidgets>=8.0.0 transformers==4.40.0 torch==2.2.0 accelerate==0.28.0\n",
        "\n",
        "# Create the model directory and set the environment variable\n",
        "import os, sys\n",
        "home_dir = os.path.expanduser(\"~\")\n",
        "model_dir = os.path.join(home_dir, \"gpt-oss-20b\")\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.environ[\"GPT_OSS_20B_HOME\"] = model_dir\n",
        "print(f\"Model directory set to: {model_dir}\")\n",
        "\n",
        "# Verify CUDA availability\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"VRAM:\", round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2), \"GB\")\n",
        "else:\n",
        "    print(\"Running on CPU ‚Äì expect slower inference.\")\n",
        "\n",
        "# Optional: clone the model repository if not already present\n",
        "repo_url = \"https://github.com/huggingface/transformers.git\"\n",
        "if not os.listdir(model_dir):\n",
        "    print(\"Cloning model repository‚Ä¶\")\n",
        "    !git clone --depth 1 {repo_url} {model_dir}\n",
        "else:\n",
        "    print(\"Model directory already contains files ‚Äì skipping clone.\")\n",
        "\n",
        "# Set a reproducible seed for any future random operations\n",
        "import random, numpy as np\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "print(\"Seeds set. Ready to load GPT‚ÄëOSS‚Äë20B.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the Model ‚Äì The ‚ÄòHello World‚Äô of LLMs\n",
        "\n",
        "Imagine you have a gigantic library (the model) and a librarian (the tokenizer) that knows how to turn your questions into a format the library can understand. In this step we‚Äôll bring the librarian and the library into our notebook so we can ask a simple question and see the answer.\n",
        "\n",
        "### 1Ô∏è‚É£ What we‚Äôre actually doing\n",
        "1. **Load the tokenizer** ‚Äì the piece of code that turns plain text into numbers the model can read.\n",
        "2. **Load the model weights** ‚Äì the 20‚ÄØbillion knobs that decide what word comes next.\n",
        "3. **Move everything to the right device** ‚Äì GPU if available, otherwise CPU.\n",
        "4. **Run a tiny inference** ‚Äì ask the model a question and print the reply.\n",
        "\n",
        "### 2Ô∏è‚É£ Why this is the ‚ÄúHello World‚Äù of LLMs\n",
        "Just like printing ‚ÄúHello, world!‚Äù in a new programming language, loading a large language model is the first step that shows everything is wired correctly. If the model loads and gives a coherent answer, you know your environment, dependencies, and hardware are all set.\n",
        "\n",
        "### 3Ô∏è‚É£ Extra explanatory paragraph ‚Äì key terms and trade‚Äëoffs\n",
        "- **Tokenizer**: A mapping from words or sub‚Äëwords to integer IDs. Think of it as a dictionary that the model uses to read and write.\n",
        "- **Model weights**: The 20‚ÄØbillion parameters that were learned during training. They‚Äôre stored in a file that can be several gigabytes.\n",
        "- **Device**: The hardware (CPU or GPU) where the tensors live. GPUs are faster but need enough VRAM; CPUs are slower but always available.\n",
        "- **Half‚Äëprecision (fp16)**: A way to store numbers using 16 bits instead of 32, cutting memory usage in half with a small loss in precision. It‚Äôs a common trade‚Äëoff for large models.\n",
        "- **Generation**: The process of turning input tokens into output tokens. It can be tuned with temperature, max length, etc.\n",
        "\n",
        "### 4Ô∏è‚É£ Quick sanity check\n",
        "Below is a short code snippet that loads the tokenizer and model, runs a single prompt, and prints the result. It‚Äôs intentionally tiny so you can copy‚Äëpaste it into a single cell and run it.\n",
        "\n",
        "> **‚ö†Ô∏è Warning**: Loading the full 20‚ÄØB model will take a few minutes and may require 8‚ÄØGB+ of GPU memory. If you hit an out‚Äëof‚Äëmemory error, try running on CPU or using a smaller model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Import libraries and set reproducible seeds\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Reproducibility: same random numbers each run\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# 2Ô∏è‚É£ Define the model name and device\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# 3Ô∏è‚É£ Load tokenizer (fast tokenizer is usually faster)\n",
        "print(\"Loading tokenizer‚Ä¶\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "except Exception as e:\n",
        "    print(\"Error loading tokenizer:\", e)\n",
        "    raise\n",
        "\n",
        "# 4Ô∏è‚É£ Load model weights ‚Äì use fp16 if GPU available to save memory\n",
        "print(\"Loading model‚Ä¶\")\n",
        "model_kwargs = {\"torch_dtype\": torch.float16} if torch.cuda.is_available() else {}\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# 5Ô∏è‚É£ Run a tiny inference\n",
        "prompt = \"What is the capital of France?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\nPrompt:\", prompt)\n",
        "print(\"\\nResponse:\", generated_text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Building a Simple Prompt Interface with ipywidgets\n",
        "\n",
        "In the previous step we saw how to ask the model a single question from code.  Now we‚Äôll turn that into a little *chat window* that lives inside the notebook, so you can type any prompt you like and see the answer instantly.\n",
        "\n",
        "### Why ipywidgets?\n",
        "Think of ipywidgets as a set of building blocks that let you add sliders, buttons, and text boxes to a notebook without writing a web page.  It‚Äôs like having a toolbox that lets you assemble a tiny interactive app right inside the cell you‚Äôre already working in.\n",
        "\n",
        "### The big picture\n",
        "1. **Create UI elements** ‚Äì a text area for the prompt, a button to trigger generation, and an output area to show the reply.\n",
        "2. **Hook them together** ‚Äì write a small function that takes the text from the prompt box, feeds it to the model, and writes the result to the output area.\n",
        "3. **Run the interface** ‚Äì the user can now type any question, click *Generate*, and watch the model answer.\n",
        "\n",
        "### Extra explanatory paragraph ‚Äì key terms and trade‚Äëoffs\n",
        "- **ipywidgets**: A Python library that provides interactive widgets for Jupyter.  It runs in the browser but communicates with the kernel via JavaScript.\n",
        "- **Output widget**: A special widget that can display arbitrary HTML or plain text.  It‚Äôs useful for showing model responses without cluttering the notebook.\n",
        "- **Event handling**: When the user clicks the button, an *event* is fired and a callback function runs.  This is how the UI stays responsive.\n",
        "- **Trade‚Äëoffs**: Using ipywidgets keeps everything in the notebook, which is great for quick experiments.  However, the interface is limited to what widgets provide; for a full‚Äëblown chat app you‚Äôd eventually move to a web framework like Streamlit or Gradio.\n",
        "\n",
        "### Quick sanity check\n",
        "Below is a short code snippet that builds the interface.  It assumes the `model` and `tokenizer` objects from Step‚ÄØ3 are already loaded in the kernel.  If you‚Äôre starting fresh, run the loading code from Step‚ÄØ3 first.\n",
        "\n",
        "> **‚ö†Ô∏è Warning**: The button will trigger a full forward pass through the 20‚ÄØB model, which can take several seconds and will use a chunk of GPU memory.  Keep an eye on your GPU usage in the system monitor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Import widgets and set up the UI\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# 2Ô∏è‚É£ Create the prompt text area, generate button, and output area\n",
        "prompt_box = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Type your question here‚Ä¶',\n",
        "    description='Prompt:',\n",
        "    layout=widgets.Layout(width='100%', height='80px')\n",
        ")\n",
        "\n",
        "generate_btn = widgets.Button(\n",
        "    description='Generate',\n",
        "    button_style='success',\n",
        "    tooltip='Click to ask the model',\n",
        "    icon='paper-plane'\n",
        ")\n",
        "\n",
        "output_area = widgets.Output(layout=widgets.Layout(border='1px solid #ddd', padding='10px'))\n",
        "\n",
        "# 3Ô∏è‚É£ Define the callback that runs when the button is clicked\n",
        "@widgets.interactive_output\n",
        "def generate_response(prompt: str = prompt_box.value):\n",
        "    # Clear previous output\n",
        "    output_area.clear_output()\n",
        "    with output_area:\n",
        "        print(f\"\\n**Prompt:** {prompt}\\n\")\n",
        "        # Tokenize and generate ‚Äì use GPU if available\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            generated = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                do_sample=True\n",
        "            )\n",
        "        response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "        print(\"**Response:**\", response)\n",
        "\n",
        "# 4Ô∏è‚É£ Wire the button to the callback\n",
        "generate_btn.on_click(lambda _: generate_response(prompt=prompt_box.value))\n",
        "\n",
        "# 5Ô∏è‚É£ Display the UI components\n",
        "ui = widgets.VBox([prompt_box, generate_btn, output_area])\n",
        "display(ui)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generating Text ‚Äì From Prompt to Response\n",
        "\n",
        "Generating text with GPT‚ÄëOSS‚Äë20B is like asking a very clever friend to write a story. You give them a starting sentence (the *prompt*) and they finish it for you. The way they finish it depends on a few knobs you can turn:\n",
        "\n",
        "- **Temperature** ‚Äì how wildly the friend can deviate from the most obvious next word. A low temperature (‚âà0.2) makes the reply safe and predictable; a high temperature (‚âà1.0) makes it more creative but also more random.\n",
        "- **Top‚Äëp (nucleus sampling)** ‚Äì instead of picking the single most likely word, the friend looks at the smallest set of words that together make up *p* percent of the probability mass. This keeps the reply coherent while still allowing some surprise.\n",
        "- **Max new tokens** ‚Äì the maximum length of the reply. Think of it as setting a word‚Äëlimit for your friend.\n",
        "\n",
        "### Extra explanatory paragraph ‚Äì key terms and trade‚Äëoffs\n",
        "- **Sampling**: The process of choosing the next word from a probability distribution. Deterministic generation (e.g., `do_sample=False`) always picks the highest‚Äëprobability word, which can lead to bland, repetitive text. Stochastic sampling (e.g., `do_sample=True`) introduces variety but can also produce nonsensical outputs if the temperature is too high.\n",
        "- **Beam search**: A deterministic alternative that keeps multiple candidate sequences in parallel. It can improve quality for short responses but is memory‚Äëheavy and slower for long generations.\n",
        "- **Trade‚Äëoffs**: Higher temperature and larger `max_new_tokens` increase GPU memory usage and inference time. Lower temperature and shorter outputs are faster and safer but may feel robotic. Choosing the right balance depends on the task: creative writing vs. factual answering.\n",
        "\n",
        "Below we‚Äôll show how to experiment with these knobs in a single, easy‚Äëto‚Äëcopy code cell. The code uses the `model` and `tokenizer` objects you loaded in Step‚ÄØ3, so make sure they‚Äôre still in the kernel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Set up reproducible seeds for generation\n",
        "import random, numpy as np, torch\n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# 2Ô∏è‚É£ Define a helper function that runs generation with custom settings\n",
        "\n",
        "def generate_text(prompt, temperature=0.7, top_p=0.9, max_new_tokens=60, do_sample=True):\n",
        "    \"\"\"Generate a response from GPT‚ÄëOSS‚Äë20B.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt : str\n",
        "        The user‚Äësupplied prompt.\n",
        "    temperature : float\n",
        "        Controls randomness; lower = more deterministic.\n",
        "    top_p : float\n",
        "        Nucleus sampling threshold.\n",
        "    max_new_tokens : int\n",
        "        Maximum number of tokens to generate.\n",
        "    do_sample : bool\n",
        "        Whether to sample or use greedy decoding.\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    # Run generation\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=do_sample,\n",
        "            pad_token_id=tokenizer.eos_token_id,  # avoid warning on long outputs\n",
        "        )\n",
        "\n",
        "    # Decode and return\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# 3Ô∏è‚É£ Example prompts with different settings\n",
        "prompts = [\n",
        "    \"Explain quantum computing in simple terms.\",\n",
        "    \"Write a short poem about autumn leaves.\",\n",
        "    \"Generate a recipe for vegan lasagna.\"\n",
        "]\n",
        "\n",
        "# 4Ô∏è‚É£ Run each prompt with two different temperature settings\n",
        "for i, p in enumerate(prompts, 1):\n",
        "    print(f\"\\n--- Prompt {i}: {p}\\n\")\n",
        "    for temp in [0.3, 1.0]:\n",
        "        print(f\"Temperature={temp} ‚Üí\")\n",
        "        print(generate_text(p, temperature=temp, top_p=0.95, max_new_tokens=80))\n",
        "        print(\"\\n---\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Common Pitfalls and How to Avoid Them\n",
        "\n",
        "When you‚Äôre working with a 20‚Äëbillion‚Äëparameter model, a few small mistakes can turn a smooth experiment into a frustrating crash.  Think of the model as a giant, very heavy backpack: if you forget to strap it on correctly, it will wobble, spill its contents, or even break the floor.  Below we list the most frequent missteps, explain why they happen, and show you the right way to do it.\n",
        "\n",
        "### 1Ô∏è‚É£ Forgetting to move everything to the right device\n",
        "If you load the model on the GPU but forget to move the input tensors to the same device, PyTorch will silently copy data back and forth, eating time and memory.  The result?  Slower inference and, on a GPU with limited VRAM, an out‚Äëof‚Äëmemory (OOM) error.\n",
        "\n",
        "### 2Ô∏è‚É£ Using the default 32‚Äëbit precision on a large model\n",
        "A 20‚ÄØB model in float32 needs roughly 80‚ÄØGB of memory ‚Äì far beyond what most GPUs can hold.  Even with a 16‚ÄØGB GPU, you‚Äôll hit OOM unless you switch to half‚Äëprecision (fp16) or use a quantized version.  The trade‚Äëoff is a tiny loss in numerical precision, which is usually negligible for text generation.\n",
        "\n",
        "### 3Ô∏è‚É£ Not setting the model to evaluation mode\n",
        "During training the model keeps track of gradients, but during inference you don‚Äôt need them.  Leaving the model in training mode (`model.train()`) keeps gradient buffers alive and can double memory usage.  Always call `model.eval()` before generating.\n",
        "\n",
        "### 4Ô∏è‚É£ Forgetting `torch.no_grad()` during inference\n",
        "Without `torch.no_grad()`, PyTorch records every operation for back‚Äëpropagation, which again bloats memory.  Wrap your generation code in a `with torch.no_grad():` block.\n",
        "\n",
        "### 5Ô∏è‚É£ Ignoring the `pad_token_id` warning\n",
        "When the generated text exceeds the model‚Äôs context window, the tokenizer may complain about missing `pad_token_id`.  Supplying it prevents a noisy warning and ensures consistent decoding.\n",
        "\n",
        "### 6Ô∏è‚É£ Using too high a temperature or `max_new_tokens`\n",
        "A temperature above 1.2 can produce gibberish, while a very large `max_new_tokens` can exhaust GPU memory.  Start with moderate values (temperature‚ÄØ‚âà‚ÄØ0.7, max_new_tokens‚ÄØ‚âà‚ÄØ50) and adjust only if needed.\n",
        "\n",
        "### Key terms and trade‚Äëoffs\n",
        "- **Device**: The hardware (CPU or GPU) where tensors live.  GPUs are faster but limited by VRAM.\n",
        "- **Precision (fp32 vs fp16)**: fp32 uses 32 bits per number; fp16 uses 16 bits, cutting memory usage in half at a small precision cost.\n",
        "- **Evaluation mode**: `model.eval()` disables dropout and gradient buffers, saving memory.\n",
        "- **Gradient tracking**: `torch.no_grad()` tells PyTorch not to record operations for back‚Äëpropagation.\n",
        "- **Pad token**: A special token that tells the tokenizer how to pad sequences; required for some generation methods.\n",
        "- **Temperature**: Controls randomness; lower values ‚Üí deterministic, higher values ‚Üí creative.\n",
        "- **Max new tokens**: Caps the length of the generated text.\n",
        "\n",
        "By keeping these points in mind, you‚Äôll avoid the most common headaches and keep your experiments running smoothly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Reproducible seeds for all random operations\n",
        "import random, numpy as np, torch\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# 2Ô∏è‚É£ Load the model correctly ‚Äì GPU + fp16, eval mode, no_grad\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer‚Ä¶\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Load model with fp16 if GPU available, otherwise fp32\n",
        "print(\"Loading model‚Ä¶\")\n",
        "model_kwargs = {\"torch_dtype\": torch.float16} if torch.cuda.is_available() else {}\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)\n",
        "model.to(DEVICE)\n",
        "model.eval()  # 3Ô∏è‚É£ Switch to evaluation mode\n",
        "\n",
        "# 4Ô∏è‚É£ Example prompt\n",
        "prompt = \"Explain the concept of a black hole in simple terms.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "# 5Ô∏è‚É£ Generate with proper context handling\n",
        "with torch.no_grad():  # 4Ô∏è‚É£ Disable gradient tracking\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=60,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,  # 5Ô∏è‚É£ Avoid pad_token_id warning\n",
        "    )\n",
        "\n",
        "# 6Ô∏è‚É£ Decode and display\n",
        "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\n**Prompt:**\", prompt)\n",
        "print(\"\\n**Response:**\", response)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>‚ö†Ô∏è Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>‚úÖ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>‚ùå Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following is NOT a recommended step when setting up GPT‚ÄëOSS‚Äë20B?\", [\"Install ipywidgets>=8.0.0\",\"Set the environment variable GPT_OSS_20B_HOME\",\"Use a GPU with less than 4‚ÄØGB VRAM\",\"Clone the model repository into the home directory\"], 2, \"GPT‚ÄëOSS‚Äë20B requires a GPU with at least 8‚ÄØGB VRAM for smooth inference; using less than 4‚ÄØGB can lead to out‚Äëof‚Äëmemory errors.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Quick check 2: Basic understanding\", [\"A\",\"B\",\"C\",\"D\"], 0, \"Review the outline section to find the correct answer.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:37:03.121Z",
      "title": "Getting Started with GPT‚ÄëOSS‚Äë20B: A Beginner‚Äôs Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T02:37:03.128Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36‚Äì60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}