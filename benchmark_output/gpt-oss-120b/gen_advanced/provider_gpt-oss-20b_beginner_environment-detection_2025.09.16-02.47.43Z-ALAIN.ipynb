{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîß Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('üìù Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîê Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'üîè Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '‚àÖ'\\n    return v[:3] + '‚Ä¶' + v[-2:] if len(v) > 6 else '‚Ä¢‚Ä¢‚Ä¢'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üåê ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('‚úÖ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('‚ö†Ô∏è Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîé Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('‚ö†Ô∏è Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('‚úÖ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('‚ö†Ô∏è Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) ‚Äî 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced GPT‚ÄëOSS‚Äë20B: Deep Dive into Architecture, Optimization, and Deployment\n\nThis notebook guides advanced practitioners through the intricacies of the GPT‚ÄëOSS‚Äë20B model, covering model loading, prompt engineering, performance profiling, quantization, distributed training, and deployment. It balances theory with hands‚Äëon code, emphasizing trade‚Äëoffs and expert considerations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ‚è±Ô∏è Estimated time to complete: 36‚Äì60 minutes (rough).  ",
        "\n> üïí Created (UTC): 2025-09-16T02:47:43.131Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain the architectural choices behind GPT‚ÄëOSS‚Äë20B and their impact on performance.\n",
        "2. Demonstrate advanced prompt engineering techniques for high‚Äëquality generation.\n",
        "3. Profile and optimize inference using quantization, LoRA, and distributed inference.\n",
        "4. Deploy a fine‚Äëtuned GPT‚ÄëOSS‚Äë20B model as a scalable API service.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+\n",
        "- Basic knowledge of PyTorch and Hugging Face Transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.40.0 accelerate>=0.28.0 bitsandbytes>=0.43.0 datasets>=2.20.0 torch>=2.2.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.40.0\",\"accelerate>=0.28.0\",\"bitsandbytes>=0.43.0\",\"datasets>=2.20.0\",\"torch>=2.2.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('‚úÖ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Setup & Model Retrieval\n",
        "\n",
        "Imagine you‚Äôre building a giant LEGO set. The instructions (our code) tell you which pieces (packages) you need, how to assemble them, and where to find the final model. In this notebook, we‚Äôll first make sure our machine has the right LEGO bricks, then pull the GPT‚ÄëOSS‚Äë20B model from Hugging Face‚Äôs online store.\n",
        "\n",
        "### Why do we need these specific packages?\n",
        "- **transformers** ‚Äì the library that knows how to talk to GPT‚ÄëOSS‚Äë20B.\n",
        "- **accelerate** ‚Äì lets us run the model on one or many GPUs without writing low‚Äëlevel CUDA code.\n",
        "- **bitsandbytes** ‚Äì gives us fast 4‚Äëbit quantization to shrink the model.\n",
        "- **datasets** ‚Äì a convenient way to load and preprocess data.\n",
        "- **torch** ‚Äì the deep‚Äëlearning engine.\n",
        "- **ipywidgets** ‚Äì optional, but useful for interactive demos.\n",
        "\n",
        "### Key terms explained\n",
        "- **Quantization**: converting 32‚Äëbit floating‚Äëpoint weights to 4‚Äëbit integers. Think of it like compressing a high‚Äëresolution photo to a smaller file size; you lose a bit of detail but the file becomes much lighter.\n",
        "- **Seed**: a starting number for random number generators. Setting a seed guarantees that the same random choices (e.g., weight initialization) happen every run, so results are reproducible.\n",
        "- **HF_TOKEN**: your personal Hugging Face authentication token. It unlocks private models and ensures you‚Äôre credited for downloads.\n",
        "\n",
        "### Trade‚Äëoffs\n",
        "Using 4‚Äëbit quantization cuts GPU memory by ~80‚ÄØ% but can slightly increase perplexity (the model‚Äôs error rate). If you‚Äôre running on a single GPU with limited VRAM, the memory savings outweigh the small loss in accuracy. If you have ample memory and need the absolute best quality, you might skip quantization.\n",
        "\n",
        "### Quick sanity check\n",
        "Before we dive into code, make sure you have a CUDA‚Äëenabled GPU (CUDA‚ÄØ12+ recommended) and that your environment variables are set:\n",
        "\n",
        "```bash\n",
        "export HF_TOKEN=YOUR_HF_TOKEN_HERE\n",
        "```\n",
        "\n",
        "Replace `YOUR_HF_TOKEN_HERE` with the token you copied from your Hugging Face account.\n",
        "\n",
        "---\n",
        "\n",
        "### What will happen in the code cells?\n",
        "1. **Install** the required packages, handling any import errors.\n",
        "2. **Import** the libraries, set a global random seed, and load the GPT‚ÄëOSS‚Äë20B model with 4‚Äëbit quantization.\n",
        "\n",
        "Let‚Äôs get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Install required packages\n",
        "# We use a try/except block so that if a package is already installed, we skip re‚Äëinstalling it.\n",
        "# This keeps the notebook idempotent.\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "packages = [\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"accelerate>=0.28.0\",\n",
        "    \"bitsandbytes>=0.43.0\",\n",
        "    \"datasets>=2.20.0\",\n",
        "    \"torch>=2.2.0\",\n",
        "    \"ipywidgets>=8.0.0\"\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        __import__(pkg.split('>=')[0])\n",
        "        print(f\"{pkg} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pkg}...\")\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", pkg]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "        except Exception as exc:\n",
        "            if IN_COLAB:\n",
        "                packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                if packages:\n",
        "                    try:\n",
        "                        import IPython\n",
        "                        ip = IPython.get_ipython()\n",
        "                        if ip is not None:\n",
        "                            ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                        else:\n",
        "                            import subprocess as _subprocess\n",
        "                            _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                    except Exception as colab_exc:\n",
        "                        print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                        raise\n",
        "                else:\n",
        "                    print('No packages specified for pip install; skipping fallback')\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "# Enable ipywidgets extension for interactive widgets (if running in Jupyter)\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"jupyter\", \"nbextension\", \"enable\", \"--py\", \"widgetsnbextension\"])\n",
        "except Exception as e:\n",
        "    print(\"Could not enable widgetsnbextension (might be running in a non‚ÄëJupyter environment).\", e)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Import, set seed, and load the model\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1Ô∏è‚É£ Set a global random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2Ô∏è‚É£ Verify that the HF_TOKEN is available\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise EnvironmentError(\"HF_TOKEN environment variable not set. Please export your Hugging Face token.\")\n",
        "\n",
        "# 3Ô∏è‚É£ Load tokenizer and model with 4‚Äëbit quantization via bitsandbytes\n",
        "MODEL_NAME = \"EleutherAI/gpt-oss-20b\"\n",
        "\n",
        "print(\"Loading tokenizer...\", end=\" \")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(\"done\")\n",
        "\n",
        "print(\"Loading model (4‚Äëbit quantized)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,          # use float16 for intermediate ops\n",
        "    load_in_4bit=True,                 # enable 4‚Äëbit quantization\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",                # automatically place layers on available GPUs\n",
        "    use_auth_token=HF_TOKEN\n",
        ")\n",
        "print(\"model loaded successfully! GPU memory usage: {:.2f} GB\".format(\n",
        "    torch.cuda.memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n",
        "))\n",
        "\n",
        "# Quick sanity check: generate a short sentence\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "generated = model.generate(**inputs, max_new_tokens=20)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Understanding GPT‚ÄëOSS‚Äë20B Architecture\n",
        "\n",
        "Imagine a giant, multi‚Äëlayered sandwich where each layer is a tiny kitchen that knows how to transform the ingredients (the input tokens) into a richer, more flavorful dish. In GPT‚ÄëOSS‚Äë20B, each layer is a *transformer block* that performs two main tasks:\n",
        "\n",
        "1. **Self‚ÄëAttention** ‚Äì the kitchen checks every ingredient against every other ingredient to decide how much each one should influence the others. Think of it as a group of chefs tasting each other‚Äôs dishes and adjusting seasoning accordingly.\n",
        "2. **Feed‚ÄëForward Network (FFN)** ‚Äì after the tasting, each chef adds a final seasoning (a small neural net) to each ingredient.\n",
        "\n",
        "These blocks are stacked 24 times, creating a deep, 20‚Äëbillion‚Äëparameter model. The 24 layers are like 24 kitchens stacked on top of each other, each building on the flavor profile created by the previous one.\n",
        "\n",
        "### Architectural Highlights\n",
        "- **24 Transformer layers** ‚Äì depth gives the model the ability to capture long‚Äërange dependencies.\n",
        "- **16 attention heads per layer** ‚Äì each head learns a different way to look at the input, like having 16 chefs each focusing on a distinct flavor profile.\n",
        "- **Hidden size 1,280** ‚Äì the width of the internal representation; larger width means more expressive power but higher memory cost.\n",
        "- **Intermediate size 5,120** ‚Äì the size of the FFN‚Äôs hidden layer; this is where the ‚Äúseasoning‚Äù happens.\n",
        "- **Positional embeddings** ‚Äì a way to tell the model where each token sits in the sequence, similar to giving each chef a numbered plate.\n",
        "- **LayerNorm + Residual connections** ‚Äì keep the signal stable and allow gradients to flow easily, like a safety net for the chefs.\n",
        "\n",
        "### Key Terms & Trade‚Äëoffs\n",
        "- **Self‚ÄëAttention**: Computes a weighted sum of all tokens for each token, allowing the model to capture relationships regardless of distance. It‚Äôs computationally expensive (O(n¬≤) in sequence length) but essential for language understanding.\n",
        "- **Feed‚ÄëForward Network (FFN)**: A two‚Äëlayer MLP applied independently to each token. It adds non‚Äëlinearity and expands the representation.\n",
        "- **LayerNorm**: Normalizes activations to stabilize training; it adds a small computational overhead but improves convergence.\n",
        "- **Residual Connection**: Adds the input of a layer to its output, helping gradients flow and preventing vanishing gradients.\n",
        "- **Parameter Count**: 20‚ÄØB parameters ‚âà 80‚ÄØGB of FP16 memory. Quantization (4‚Äëbit) reduces this to ~16‚ÄØGB but may slightly degrade perplexity.\n",
        "\n",
        "**Why 24 layers and 16 heads?**\n",
        "- Depth (layers) allows the model to build hierarchical representations; each layer can capture increasingly abstract patterns.\n",
        "- Width (hidden size) and number of heads control the capacity to represent diverse patterns. More heads mean the model can attend to more relationships in parallel, but each head is smaller, so the overall parameter count stays manageable.\n",
        "- The chosen configuration balances GPU memory constraints (‚âà20‚ÄØGB FP16) with the ability to model complex language tasks.\n",
        "\n",
        "### Quick Code Peek\n",
        "Below we load the model‚Äôs configuration and print a concise summary. This is a great way to verify that the architecture matches the documentation and to inspect any custom settings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Inspect GPT‚ÄëOSS‚Äë20B architecture\n",
        "# We import only the config to avoid loading the huge weights.\n",
        "# This keeps the notebook lightweight and fast.\n",
        "\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# Load the configuration for EleutherAI/gpt-oss-20b\n",
        "config = AutoConfig.from_pretrained(\"EleutherAI/gpt-oss-20b\")\n",
        "\n",
        "# Print key hyperparameters\n",
        "print(\"Model name:                 \", config._name_or_path)\n",
        "print(\"Number of layers (n_layers):\", config.n_layer)\n",
        "print(\"Hidden size (n_embd):       \", config.n_embd)\n",
        "print(\"Intermediate size (n_inner):\", config.n_inner)\n",
        "print(\"Number of attention heads:  \", config.n_head)\n",
        "print(\"Total parameters:           \", config.num_parameters())\n",
        "\n",
        "# Show a compact summary of the transformer blocks\n",
        "print(\"\\nTransformer block summary: \")\n",
        "for i in range(config.n_layer):\n",
        "    print(f\"  Layer {i+1:02d}:  {config.n_head} heads, hidden {config.n_embd}, FFN {config.n_inner}\")\n",
        "\n",
        "# Optional: compute memory footprint for FP16 (approximate)\n",
        "# 1 parameter = 2 bytes in FP16\n",
        "bytes_per_param = 2\n",
        "mem_gb = config.num_parameters() * bytes_per_param / (1024 ** 3)\n",
        "print(f\"\\nApprox. FP16 memory: {mem_gb:.2f} GB\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What We Learned\n",
        "- The model has **24 layers** with **16 attention heads** each.\n",
        "- Hidden size is **1,280** and the FFN expands to **5,120**.\n",
        "- Total parameter count is ~20‚ÄØB, which translates to ~40‚ÄØGB in FP32 and ~20‚ÄØGB in FP16.\n",
        "- 4‚Äëbit quantization cuts that down to ~16‚ÄØGB, making it feasible on a single RTX‚Äë3090.\n",
        "\n",
        "Feel free to tweak the `config` object (e.g., `config.n_layer = 12`) to experiment with smaller variants, but remember that changing these values will require re‚Äëtraining or fine‚Äëtuning from scratch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the Model with Hugging Face\n",
        "\n",
        "### Why do we need a special loading routine?\n",
        "Think of the GPT‚ÄëOSS‚Äë20B model as a gigantic library of books. Each book (layer) is stored on a different shelf (GPU). When you ask the model to generate text, you need to fetch the right books, read them, and then put them back. Hugging Face‚Äôs `AutoModelForCausalLM` is the librarian that knows how to locate, load, and hand out these books efficiently.\n",
        "\n",
        "### The `from_pretrained` magic\n",
        "```python\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-oss-20b\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "```\n",
        "- **`torch_dtype=torch.float16`** ‚Äì tells PyTorch to keep intermediate activations in half‚Äëprecision, which cuts memory by ~50‚ÄØ% compared to full 32‚Äëbit.\n",
        "- **`load_in_4bit=True`** ‚Äì activates Bitsandbytes‚Äô 4‚Äëbit quantization. Imagine compressing a 32‚Äëbit number to a 4‚Äëbit nibble; you lose a little detail but the file shrinks dramatically.\n",
        "- **`device_map=\"auto\"`** ‚Äì automatically shards the model across all available GPUs, so you don‚Äôt have to manually assign layers.\n",
        "- **`use_auth_token`** ‚Äì required for large or private models; it authenticates your Hugging Face account.\n",
        "\n",
        "### What happens under the hood?\n",
        "1. **Download** the model weights from the Hugging Face hub.\n",
        "2. **Quantize** the weights to 4‚Äëbit using Bitsandbytes.\n",
        "3. **Shard** the quantized tensors across GPUs.\n",
        "4. **Wrap** everything in a `CausalLM` head that can take token IDs and produce logits.\n",
        "\n",
        "### Key terms & trade‚Äëoffs\n",
        "- **Quantization**: Converting 32‚Äëbit floating‚Äëpoint weights to lower‚Äëbit integers. It reduces memory and speeds up inference but can slightly hurt perplexity.\n",
        "- **Device map**: A strategy for distributing tensors across devices. `\"auto\"` is convenient but may not balance load perfectly on heterogeneous GPUs.\n",
        "- **Half‚Äëprecision (FP16)**: A compromise between speed and numerical stability. Some models still benefit from full FP32, but for GPT‚ÄëOSS‚Äë20B FP16 is usually safe.\n",
        "- **HF_TOKEN**: Your personal Hugging Face authentication token. Keep it secret; it grants access to large models and private repos.\n",
        "\n",
        "### Why 4‚Äëbit + FP16?\n",
        "The 20‚ÄëB model would normally need ~20‚ÄØGB of VRAM in FP16. With 4‚Äëbit quantization, that drops to ~4‚ÄØGB, making it runnable on a single RTX‚Äë3090. The trade‚Äëoff is a marginal increase in perplexity (‚âà1‚Äì2‚ÄØ%) and a tiny drop in generation quality. For most inference workloads, the speed‚Äëmemory gain outweighs this cost.\n",
        "\n",
        "### Quick sanity check\n",
        "After loading, we‚Äôll generate a short sentence to confirm everything works:\n",
        "```python\n",
        "prompt = \"The future of AI is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "generated = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
        "```\n",
        "If you see a coherent continuation, the model is ready for the next steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Load tokenizer and model with 4‚Äëbit quantization\n",
        "# Import required libraries\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Ensure reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Verify HF_TOKEN\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise EnvironmentError(\"HF_TOKEN not found. Export your Hugging Face token before running this cell.\")\n",
        "\n",
        "# Load tokenizer\n",
        "MODEL_NAME = \"EleutherAI/gpt-oss-20b\"\n",
        "print(\"Loading tokenizer...\", end=\" \")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(\"done\")\n",
        "\n",
        "# Load model with 4‚Äëbit quantization and automatic device mapping\n",
        "print(\"Loading model (4‚Äëbit, FP16)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "print(\"model loaded! GPU memory used: {:.2f} GB\".format(\n",
        "    torch.cuda.memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n",
        "))\n",
        "\n",
        "# Quick generation test\n",
        "prompt = \"The future of AI is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "generated = model.generate(**inputs, max_new_tokens=20)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Tokenization & Prompt Engineering\n",
        "\n",
        "### 1Ô∏è‚É£ What is tokenization?  \n",
        "Think of a sentence as a long string of characters that a computer can‚Äôt understand directly. Tokenization is the process of chopping that string into *tokens*‚Äîsmall, meaningful units that the model can work with.  \n",
        "\n",
        "- **Word‚Äëlevel** tokenizers split on spaces, but they struggle with rare words.  \n",
        "- **Sub‚Äëword** tokenizers (like BPE or WordPiece) break words into smaller pieces, so even unseen words can be represented.  \n",
        "- **Byte‚Äëpair encoding (BPE)** starts with individual characters and repeatedly merges the most frequent pairs, creating a vocabulary that balances coverage and size.\n",
        "\n",
        "In GPT‚ÄëOSS‚Äë20B we use a BPE‚Äëstyle tokenizer that maps each token to an integer ID.  The model sees a sequence of IDs, not raw text.\n",
        "\n",
        "### 2Ô∏è‚É£ Prompt engineering  \n",
        "A *prompt* is the text you give the model to steer its output.  It‚Äôs like giving a chef a recipe: the more precise you are, the more likely the dish will match your taste.  Prompt engineering is the art of crafting that recipe.\n",
        "\n",
        "- **Prefix prompts**: ‚ÄúTranslate the following sentence to French: ‚Ä¶‚Äù  \n",
        "- **Instruction prompts**: ‚ÄúYou are a helpful assistant. Answer the question.‚Äù  \n",
        "- **Few‚Äëshot prompts**: Provide a few examples before the target input.\n",
        "\n",
        "The goal is to reduce ambiguity, guide the model‚Äôs internal attention, and control the style or format of the output.\n",
        "\n",
        "### 3Ô∏è‚É£ Sampling strategies & key terms  \n",
        "When the model finishes the prompt, it must decide which token to output next.  Several knobs let you trade off creativity vs. determinism:\n",
        "\n",
        "| Term | What it does | Typical values | Trade‚Äëoff |\n",
        "|------|--------------|----------------|-----------|\n",
        "| **Temperature** | Scales logits before softmax. Lower ‚Üí more deterministic, higher ‚Üí more random. | 0 (deterministic) ‚Äì 1.5 (very creative) | 0 ‚Üí safe but dull; 1+ ‚Üí diverse but may hallucinate. |\n",
        "| **Top‚Äëp (nucleus sampling)** | Keeps the smallest set of tokens whose cumulative probability ‚â• p. | 0.8‚Äì0.95 | 0.8 ‚Üí more focused; 0.95 ‚Üí more diverse. |\n",
        "| **Repetition penalty** | Penalizes tokens that have already appeared. | 1.0 (none) ‚Äì 1.5 | 1.0 ‚Üí loops; >1 ‚Üí discourages repetition. |\n",
        "| **Length penalty** | Adjusts score based on sequence length (used in beam search). | 0.8‚Äì1.2 | 0.8 ‚Üí favors shorter outputs; 1.2 ‚Üí encourages longer. |\n",
        "| **Beam width** | Number of parallel hypotheses kept during generation. | 1‚Äì10 | 1 ‚Üí greedy; >1 ‚Üí more exhaustive search. |\n",
        "\n",
        "### 4Ô∏è‚É£ Why these knobs matter  \n",
        "- **Deterministic generation** (temperature‚ÄØ=‚ÄØ0) is great for debugging or when you need reproducible results.  \n",
        "- **Higher temperature** or **top‚Äëp** introduces variety, useful for creative writing or sampling many candidates.  \n",
        "- **Repetition penalty** is essential for long‚Äëform generation; without it the model can get stuck in loops.  \n",
        "- **Beam search** with a moderate width (e.g., 5) often yields higher‚Äëquality text than greedy decoding, but at the cost of speed.\n",
        "\n",
        "Choosing the right combination depends on the task: a chatbot may favor low temperature for safety, while a story generator may use higher temperature for flair.\n",
        "\n",
        "### 5Ô∏è‚É£ Extra explanatory paragraph: key terms & rationale/trade‚Äëoffs\n",
        "**Token** ‚Äì the smallest unit the model processes; it‚Äôs an integer ID that maps to a sub‚Äëword or character.  Tokens are the building blocks of the input and output sequences.  \n",
        "\n",
        "**Tokenizer** ‚Äì the algorithm that turns raw text into tokens and back.  It must be consistent between training and inference; otherwise the model will misinterpret the prompt.  \n",
        "\n",
        "**Prompt** ‚Äì the text you feed to the model to elicit a desired response.  A well‚Äëcrafted prompt reduces ambiguity and guides the model‚Äôs attention.  \n",
        "\n",
        "**Sampling strategy** ‚Äì the method the model uses to pick the next token.  Deterministic strategies (temperature‚ÄØ=‚ÄØ0, greedy) guarantee reproducibility but can produce bland or repetitive text.  Stochastic strategies (temperature‚ÄØ>‚ÄØ0, top‚Äëp) increase diversity but risk incoherence or hallucinations.  \n",
        "\n",
        "**Trade‚Äëoffs** ‚Äì Every knob balances speed, memory, and output quality.  For instance, a larger beam width improves quality but slows generation and consumes more GPU memory.  Lower temperature reduces randomness but may lead to repetitive or overly safe responses.  The art of prompt engineering is to find the sweet spot for your specific use case.\n",
        "\n",
        "---\n",
        "\n",
        "### 6Ô∏è‚É£ Quick hands‚Äëon demo\n",
        "Below we‚Äôll load the tokenizer and model, then generate the same prompt with different sampling settings to see how the output changes.  The code is intentionally short (‚â§30 lines) and fully reproducible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tokenization & Prompt Engineering demo\n",
        "# ------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Imports & reproducibility\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "SEED = 123\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# 2Ô∏è‚É£ Load tokenizer & model (reuse if already loaded)\n",
        "MODEL_NAME = \"EleutherAI/gpt-oss-20b\"\n",
        "print(\"Loading tokenizer‚Ä¶\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\"Loading model (4‚Äëbit, FP16)‚Ä¶\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Helper to generate text\n",
        "def generate(prompt, **gen_kwargs):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**inputs, **gen_kwargs)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# 4Ô∏è‚É£ Base prompt\n",
        "prompt = \"Explain the concept of tokenization in simple terms:\"\n",
        "\n",
        "# 5Ô∏è‚É£ Different sampling strategies\n",
        "print(\"\\nDeterministic (temperature=0):\")\n",
        "print(generate(prompt, temperature=0, max_new_tokens=50))\n",
        "\n",
        "print(\"\\nTemperature 0.8 (more creative):\")\n",
        "print(generate(prompt, temperature=0.8, max_new_tokens=50))\n",
        "\n",
        "print(\"\\nTop‚Äëp 0.9 (nucleus sampling):\")\n",
        "print(generate(prompt, temperature=0.8, top_p=0.9, max_new_tokens=50))\n",
        "\n",
        "print(\"\\nRepetition penalty 1.2 (avoid loops):\")\n",
        "print(generate(prompt, temperature=0.8, repetition_penalty=1.2, max_new_tokens=50))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Baseline Generation & Evaluation\n",
        "\n",
        "### Why do we need a baseline?\n",
        "Think of the GPT‚ÄëOSS‚Äë20B model as a chef who can cook an endless variety of dishes. Before we start tweaking ingredients (prompt engineering, temperature, etc.), we want a *reference plate* that shows what the chef normally produces. This reference plate is our **baseline** ‚Äì it lets us measure how much a change improves or hurts the final dish.\n",
        "\n",
        "### What we‚Äôll do in this section\n",
        "1. **Generate a small set of baseline responses** for a handful of prompts.\n",
        "2. **Quantitatively evaluate** those responses using BLEU and ROUGE, two classic metrics from machine‚Äëtranslation research.\n",
        "3. **Add a quick human‚Äëin‚Äëthe‚Äëloop sanity check** so we can see if the numbers match what a person would think.\n",
        "\n",
        "### Key terms & trade‚Äëoffs\n",
        "- **BLEU (Bilingual Evaluation Understudy)** ‚Äì a precision‚Äëoriented metric that counts how many *n‚Äëgram* overlaps exist between the generated text and a reference. It‚Äôs fast and easy to compute but can be overly harsh on creative language and doesn‚Äôt capture meaning.\n",
        "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** ‚Äì a family of recall‚Äëbased metrics (ROUGE‚ÄëL, ROUGE‚ÄëN, ROUGE‚ÄëS) that measure how much of the reference content appears in the generated text. It tends to reward longer outputs and can be more forgiving than BLEU.\n",
        "- **Human‚Äëin‚Äëthe‚Äëloop** ‚Äì a manual scoring step where a human annotator rates the quality (fluency, relevance, factuality). It‚Äôs the gold standard but expensive and slow.\n",
        "- **Trade‚Äëoffs** ‚Äì BLEU is quick but may penalize novel phrasing; ROUGE is more tolerant but can over‚Äëreward verbosity. Human scores are accurate but noisy and costly. In practice, we combine all three to get a balanced view.\n",
        "\n",
        "### Why baseline matters\n",
        "If you tweak temperature from 0.7 to 1.2 and BLEU drops from 0.45 to 0.38, you know the change hurt precision. If ROUGE jumps, maybe the model is adding more content but also more hallucinations. Human scores help confirm whether the numbers reflect real quality.\n",
        "\n",
        "---\n",
        "\n",
        "### Quick sanity check\n",
        "Below we‚Äôll generate five responses for each prompt, compute BLEU/ROUGE against a short reference, and print the results. The code is split into two cells for clarity and reproducibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Generate baseline responses\n",
        "# ------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Imports & reproducibility\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2Ô∏è‚É£ Load tokenizer & model (reuse if already loaded)\n",
        "MODEL_NAME = \"EleutherAI/gpt-oss-20b\"\n",
        "print(\"Loading tokenizer‚Ä¶\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\"Loading model (4‚Äëbit, FP16)‚Ä¶\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Prompts and reference answers (short, hand‚Äëcrafted)\n",
        "prompts = [\n",
        "    \"Explain the concept of tokenization in simple terms.\",\n",
        "    \"What is the difference between supervised and unsupervised learning?\",\n",
        "    \"Describe the process of photosynthesis.\",\n",
        "]\n",
        "\n",
        "references = [\n",
        "    \"Tokenization is the process of breaking text into smaller pieces called tokens, which the model can understand.\",\n",
        "    \"Supervised learning uses labeled data, while unsupervised learning finds patterns in unlabeled data.\",\n",
        "    \"Photosynthesis is how plants convert sunlight into energy, producing glucose and oxygen.\",\n",
        "]\n",
        "\n",
        "# 4Ô∏è‚É£ Generate 5 samples per prompt\n",
        "generated_outputs = []\n",
        "for idx, prompt in enumerate(prompts):\n",
        "    print(f\"\\nPrompt {idx+1}: {prompt}\")\n",
        "    for i in range(5):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            do_sample=True,\n",
        "        )\n",
        "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        generated_outputs.append(text)\n",
        "        print(f\"  Sample {i+1}: {text}\")\n",
        "\n",
        "print(\"\\nBaseline generation complete.\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Compute BLEU & ROUGE, add human‚Äëin‚Äëthe‚Äëloop placeholder\n",
        "# ------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Install & import evaluation library (if not already installed)\n",
        "try:\n",
        "    import evaluate\n",
        "except ImportError:\n",
        "    import subprocess, sys\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"evaluate==0.4.2\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "    import evaluate\n",
        "\n",
        "# 2Ô∏è‚É£ Load metrics\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# 3Ô∏è‚É£ Prepare references and predictions for each prompt\n",
        "# The evaluate library expects lists of tokenized sentences\n",
        "predictions = []\n",
        "reference_lists = []\n",
        "\n",
        "# Re‚Äëuse the prompts and references from the previous cell\n",
        "for ref in references:\n",
        "    reference_lists.append([ref.split()])  # list of token lists\n",
        "\n",
        "# Flatten generated_outputs into a list of token lists\n",
        "for gen in generated_outputs:\n",
        "    predictions.append(gen.split())\n",
        "\n",
        "# 4Ô∏è‚É£ Compute metrics\n",
        "bleu_score = bleu.compute(predictions=predictions, references=reference_lists)\n",
        "rouge_score = rouge.compute(predictions=predictions, references=reference_lists)\n",
        "\n",
        "print(\"\\n=== Automatic Evaluation ===\")\n",
        "print(f\"BLEU score: {bleu_score['bleu']:.4f}\")\n",
        "print(f\"ROUGE‚ÄëL F1: {rouge_score['rougeL_fmeasure']:.4f}\")\n",
        "print(f\"ROUGE‚Äë1 F1: {rouge_score['rouge1_fmeasure']:.4f}\")\n",
        "print(f\"ROUGE‚Äë2 F1: {rouge_score['rouge2_fmeasure']:.4f}\")\n",
        "\n",
        "# 5Ô∏è‚É£ Human‚Äëin‚Äëthe‚Äëloop placeholder\n",
        "print(\"\\n=== Human Evaluation (manual) ===\")\n",
        "print(\"Please read each generated sample and rate it on a 1‚Äì5 scale for: fluency, relevance, factuality.\")\n",
        "print(\"(In a real workflow, you would collect these scores in a spreadsheet or annotation tool.)\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Profiling Inference Performance\n",
        "\n",
        "When you run a large language model like GPT‚ÄëOSS‚Äë20B, you‚Äôre essentially asking a giant factory to produce text. Just like a factory, you want to know which machines (layers, attention heads, matrix multiplications) are the bottlenecks, how much memory each part consumes, and how long the whole process takes. \n",
        "\n",
        "In this section we‚Äôll use **PyTorch‚Äôs built‚Äëin profiler** to answer those questions. Think of the profiler as a stopwatch that also records how many workers (threads) are busy, how much memory is being used, and which parts of the code are the slowest. With that data you can decide whether to:\n",
        "\n",
        "* Move a layer to a faster GPU, \n",
        "* Reduce the batch size, \n",
        "* Quantize a sub‚Äëmodule, or \n",
        "* Parallelize across multiple GPUs.\n",
        "\n",
        "### Key terms & trade‚Äëoffs\n",
        "- **CPU vs GPU**: The CPU is like a single worker that can do many different tasks, but it‚Äôs slower for matrix math. The GPU is a team of workers that can do the same math in parallel, but it has limited memory.\n",
        "- **Latency**: The time it takes to generate a single token. Lower latency means a snappier user experience.\n",
        "- **Throughput**: How many tokens you can generate per second. Higher throughput is important for batch inference or serving many users.\n",
        "- **Memory footprint**: The amount of VRAM used while the model is running. If you exceed VRAM, the GPU will swap to slower system RAM or crash.\n",
        "- **Profiler overhead**: Profiling adds a small amount of extra work (e.g., recording timestamps). For very short runs the overhead can dominate, so we‚Äôll run a few warm‚Äëup steps before measuring.\n",
        "\n",
        "**Why profiling matters**: Without it you might spend hours tweaking hyper‚Äëparameters only to discover that the real bottleneck is a single attention head that is not on the fastest GPU. Profiling gives you a data‚Äëdriven map of where to focus your optimization efforts.\n",
        "\n",
        "### What we‚Äôll do\n",
        "1. Load the model (if not already loaded) and set a deterministic seed.\n",
        "2. Warm‚Äëup the model with a few dummy generations.\n",
        "3. Run the profiler while generating a short prompt.\n",
        "4. Print a concise summary of the most time‚Äëconsuming operations and memory usage.\n",
        "\n",
        "Feel free to adjust the `max_new_tokens` or `batch_size` to match your own workload.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Setup ‚Äì imports, seed, and model loading (re‚Äëuse if already loaded)\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1Ô∏è‚É£ Set a global seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2Ô∏è‚É£ Load tokenizer & model if not already present\n",
        "MODEL_NAME = \"EleutherAI/gpt-oss-20b\"\n",
        "try:\n",
        "    tokenizer\n",
        "except NameError:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "try:\n",
        "    model\n",
        "except NameError:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "        use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        "    )\n",
        "\n",
        "print(\"Model and tokenizer ready ‚Äì GPU memory used: {:.2f} GB\".format(\n",
        "    torch.cuda.memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n",
        "))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Profiling inference with torch.profiler\n",
        "# ------------------------------------------------------------\n",
        "import torch.profiler\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "# 3Ô∏è‚É£ Warm‚Äëup: run a few generations without profiling\n",
        "warmup_prompt = \"Hello world\"\n",
        "for _ in range(3):\n",
        "    inputs = tokenizer(warmup_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    model.generate(**inputs, max_new_tokens=10)\n",
        "\n",
        "# 4Ô∏è‚É£ Define a simple generation function to profile\n",
        "\n",
        "def generate_once(prompt, max_new_tokens=20):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with record_function(\"model_inference\"):\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# 5Ô∏è‚É£ Run profiler\n",
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
        "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"runs/profiler_oss20b\"),\n",
        "    record_shapes=True,\n",
        "    profile_memory=True,\n",
        "    with_stack=True,\n",
        ") as prof:\n",
        "    for step in range(5):  # 5 steps to get a stable profile\n",
        "        generate_once(\"The quick brown fox jumps over the lazy dog.\", max_new_tokens=15)\n",
        "        prof.step()\n",
        "\n",
        "# 6Ô∏è‚É£ Print a concise summary\n",
        "print(\"\\n=== Profiling Summary ===\")\n",
        "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n",
        "print(\"\\nMemory usage (GPU): {:.2f} MB\".format(\n",
        "    torch.cuda.memory_allocated() / (1024 ** 2) if torch.cuda.is_available() else 0\n",
        "))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>‚ö†Ô∏è Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>‚úÖ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>‚ùå Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following best describes the trade‚Äëoff when applying 4‚Äëbit quantization to GPT‚ÄëOSS‚Äë20B?\", [\"Increased memory usage with higher accuracy\",\"Reduced memory usage with a slight drop in accuracy\",\"No change in memory usage but improved speed\",\"Significant accuracy loss with no memory benefit\"], 1, \"4‚Äëbit quantization reduces memory footprint by ~80% but can slightly degrade model perplexity.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary advantage of using LoRA for fine‚Äëtuning large language models?\", [\"It eliminates the need for GPU memory\",\"It adds a large number of trainable parameters\",\"It keeps the majority of the model frozen while adding few trainable weights\",\"It converts the model to a smaller architecture\"], 2, \"LoRA introduces a small set of trainable rank‚Äëdecomposition matrices, keeping most of the pre‚Äëtrained weights frozen.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:47:43.122Z",
      "title": "Advanced GPT‚ÄëOSS‚Äë20B: Deep Dive into Architecture, Optimization, and Deployment",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T02:47:43.131Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36‚Äì60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}