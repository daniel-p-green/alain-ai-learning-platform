{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced GPT‑OSS‑20B: Deep Dive into Architecture, Optimization, and Deployment\n\nThis notebook guides advanced practitioners through the intricacies of the GPT‑OSS‑20B model, covering model loading, prompt engineering, performance profiling, quantization, distributed training, and deployment. It balances theory with hands‑on code, emphasizing trade‑offs and expert considerations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T02:47:43.131Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain the architectural choices behind GPT‑OSS‑20B and their impact on performance.\n",
        "2. Demonstrate advanced prompt engineering techniques for high‑quality generation.\n",
        "3. Profile and optimize inference using quantization, LoRA, and distributed inference.\n",
        "4. Deploy a fine‑tuned GPT‑OSS‑20B model as a scalable API service.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+\n",
        "- Basic knowledge of PyTorch and Hugging Face Transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.40.0 accelerate>=0.28.0 bitsandbytes>=0.43.0 datasets>=2.20.0 torch>=2.2.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.40.0\",\"accelerate>=0.28.0\",\"bitsandbytes>=0.43.0\",\"datasets>=2.20.0\",\"torch>=2.2.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Setup & Model Retrieval\n",
        "\n",
        "Imagine you’re building a giant LEGO set. The instructions (our code) tell you which pieces (packages) you need, how to assemble them, and where to find the final model. In this notebook, we’ll first make sure our machine has the right LEGO bricks, then pull the GPT‑OSS‑20B model from Hugging Face’s online store.\n",
        "\n",
        "### Why do we need these specific packages?\n",
        "- **transformers** – the library that knows how to talk to GPT‑OSS‑20B.\n",
        "- **accelerate** – lets us run the model on one or many GPUs without writing low‑level CUDA code.\n",
        "- **bitsandbytes** – gives us fast 4‑bit quantization to shrink the model.\n",
        "- **datasets** – a convenient way to load and preprocess data.\n",
        "- **torch** – the deep‑learning engine.\n",
        "- **ipywidgets** – optional, but useful for interactive demos.\n",
        "\n",
        "### Key terms explained\n",
        "- **Quantization**: converting 32‑bit floating‑point weights to 4‑bit integers. Think of it like compressing a high‑resolution photo to a smaller file size; you lose a bit of detail but the file becomes much lighter.\n",
        "- **Seed**: a starting number for random number generators. Setting a seed guarantees that the same random choices (e.g., weight initialization) happen every run, so results are reproducible.\n",
        "- **HF_TOKEN**: your personal Hugging Face authentication token. It unlocks private models and ensures you’re credited for downloads.\n",
        "\n",
        "### Trade‑offs\n",
        "Using 4‑bit quantization cuts GPU memory by ~80 % but can slightly increase perplexity (the model’s error rate). If you’re running on a single GPU with limited VRAM, the memory savings outweigh the small loss in accuracy. If you have ample memory and need the absolute best quality, you might skip quantization.\n",
        "\n",
        "### Quick sanity check\n",
        "Before we dive into code, make sure you have a CUDA‑enabled GPU (CUDA 12+ recommended) and that your environment variables are set:\n",
        "\n",
        "```bash\n",
        "export HF_TOKEN=YOUR_HF_TOKEN_HERE\n",
        "```\n",
        "\n",
        "Replace `YOUR_HF_TOKEN_HERE` with the token you copied from your Hugging Face account.\n",
        "\n",
        "---\n",
        "\n",
        "### What will happen in the code cells?\n",
        "1. **Install** the required packages, handling any import errors.\n",
        "2. **Import** the libraries, set a global random seed, and load the GPT‑OSS‑20B model with 4‑bit quantization.\n",
        "\n",
        "Let’s get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Install required packages\n",
        "# We use a try/except block so that if a package is already installed, we skip re‑installing it.\n",
        "# This keeps the notebook idempotent.\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "packages = [\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"accelerate>=0.28.0\",\n",
        "    \"bitsandbytes>=0.43.0\",\n",
        "    \"datasets>=2.20.0\",\n",
        "    \"torch>=2.2.0\",\n",
        "    \"ipywidgets>=8.0.0\"\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        __import__(pkg.split('>=')[0])\n",
        "        print(f\"{pkg} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pkg}...\")\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", pkg]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "        except Exception as exc:\n",
        "            if IN_COLAB:\n",
        "                packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                if packages:\n",
        "                    try:\n",
        "                        import IPython\n",
        "                        ip = IPython.get_ipython()\n",
        "                        if ip is not None:\n",
        "                            ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                        else:\n",
        "                            import subprocess as _subprocess\n",
        "                            _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                    except Exception as colab_exc:\n",
        "                        print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                        raise\n",
        "                else:\n",
        "                    print('No packages specified for pip install; skipping fallback')\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "# Enable ipywidgets extension for interactive widgets (if running in Jupyter)\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"jupyter\", \"nbextension\", \"enable\", \"--py\", \"widgetsnbextension\"])\n",
        "except Exception as e:\n",
        "    print(\"Could not enable widgetsnbextension (might be running in a non‑Jupyter environment).\", e)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Import, set seed, and load the model\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1️⃣ Set a global random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2️⃣ Verify that the HF_TOKEN is available\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise EnvironmentError(\"HF_TOKEN environment variable not set. Please export your Hugging Face token.\")\n",
        "\n",
        "# 3️⃣ Load tokenizer and model with 4‑bit quantization via bitsandbytes\n",
        "MODEL_NAME = \"EleutherAI/gpt-oss-20b\"\n",
        "\n",
        "print(\"Loading tokenizer...\", end=\" \")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(\"done\")\n",
        "\n",
        "print(\"Loading model (4‑bit quantized)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,          # use float16 for intermediate ops\n",
        "    load_in_4bit=True,                 # enable 4‑bit quantization\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",                # automatically place layers on available GPUs\n",
        "    use_auth_token=HF_TOKEN\n",
        ")\n",
        "print(\"model loaded successfully! GPU memory usage: {:.2f} GB\".format(\n",
        "    torch.cuda.memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n",
        "))\n",
        "\n",
        "# Quick sanity check: generate a short sentence\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "generated = model.generate(**inputs, max_new_tokens=20)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Understanding GPT‑OSS‑20B Architecture\n",
        "\n",
        "Imagine a giant, multi‑layered sandwich where each layer is a tiny kitchen that knows how to transform the ingredients (the input tokens) into a richer, more flavorful dish. In GPT‑OSS‑20B, each layer is a *transformer block* that performs two main tasks:\n",
        "\n",
        "1. **Self‑Attention** – the kitchen checks every ingredient against every other ingredient to decide how much each one should influence the others. Think of it as a group of chefs tasting each other’s dishes and adjusting seasoning accordingly.\n",
        "2. **Feed‑Forward Network (FFN)** – after the tasting, each chef adds a final seasoning (a small neural net) to each ingredient.\n",
        "\n",
        "These blocks are stacked 24 times, creating a deep, 20‑billion‑parameter model. The 24 layers are like 24 kitchens stacked on top of each other, each building on the flavor profile created by the previous one.\n",
        "\n",
        "### Architectural Highlights\n",
        "- **24 Transformer layers** – depth gives the model the ability to capture long‑range dependencies.\n",
        "- **16 attention heads per layer** – each head learns a different way to look at the input, like having 16 chefs each focusing on a distinct flavor profile.\n",
        "- **Hidden size 1,280** – the width of the internal representation; larger width means more expressive power but higher memory cost.\n",
        "- **Intermediate size 5,120** – the size of the FFN’s hidden layer; this is where the “seasoning” happens.\n",
        "- **Positional embeddings** – a way to tell the model where each token sits in the sequence, similar to giving each chef a numbered plate.\n",
        "- **LayerNorm + Residual connections** – keep the signal stable and allow gradients to flow easily, like a safety net for the chefs.\n",
        "\n",
        "### Key Terms & Trade‑offs\n",
        "- **Self‑Attention**: Computes a weighted sum of all tokens for each token, allowing the model to capture relationships regardless of distance. It’s computationally expensive (O(n²) in sequence length) but essential for language understanding.\n",
        "- **Feed‑Forward Network (FFN)**: A two‑layer MLP applied independently to each token. It adds non‑linearity and expands the representation.\n",
        "- **LayerNorm**: Normalizes activations to stabilize training; it adds a small computational overhead but improves convergence.\n",
        "- **Residual Connection**: Adds the input of a layer to its output, helping gradients flow and preventing vanishing gradients.\n",
        "- **Parameter Count**: 20 B parameters ≈ 80 GB of FP16 memory. Quantization (4‑bit) reduces this to ~16 GB but may slightly degrade perplexity.\n",
        "\n",
        "**Why 24 layers and 16 heads?**\n",
        "- Depth (layers) allows the model to build hierarchical representations; each layer can capture increasingly abstract patterns.\n",
        "- Width (hidden size) and number of heads control the capacity to represent diverse patterns. More heads mean the model can attend to more relationships in parallel, but each head is smaller, so the overall parameter count stays manageable.\n",
        "- The chosen configuration balances GPU memory constraints (≈20 GB FP16) with the ability to model complex language tasks.\n",
        "\n",
        "### Quick Code Peek\n",
        "Below we load the model’s configuration and print a concise summary. This is a great way to verify that the architecture matches the documentation and to inspect any custom settings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Inspect GPT‑OSS‑20B architecture\n",
        "# We import only the config to avoid loading the huge weights.\n",
        "# This keeps the notebook lightweight and fast.\n",
        "\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# Load the configuration for EleutherAI/gpt-oss-20b\n",
        "config = AutoConfig.from_pretrained(\"EleutherAI/gpt-oss-20b\")\n",
        "\n",
        "# Print key hyperparameters\n",
        "print(\"Model name:                 \", config._name_or_path)\n",
        "print(\"Number of layers (n_layers):\", config.n_layer)\n",
        "print(\"Hidden size (n_embd):       \", config.n_embd)\n",
        "print(\"Intermediate size (n_inner):\", config.n_inner)\n",
        "print(\"Number of attention heads:  \", config.n_head)\n",
        "print(\"Total parameters:           \", config.num_parameters())\n",
        "\n",
        "# Show a compact summary of the transformer blocks\n",
        "print(\"\\nTransformer block summary: \")\n",
        "for i in range(config.n_layer):\n",
        "    print(f\"  Layer {i+1:02d}:  {config.n_head} heads, hidden {config.n_embd}, FFN {config.n_inner}\")\n",
        "\n",
        "# Optional: compute memory footprint for FP16 (approximate)\n",
        "# 1 parameter = 2 bytes in FP16\n",
        "bytes_per_param = 2\n",
        "mem_gb = config.num_parameters() * bytes_per_param / (1024 ** 3)\n",
        "print(f\"\\nApprox. FP16 memory: {mem_gb:.2f} GB\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What We Learned\n",
        "- The model has **24 layers** with **16 attention heads** each.\n",
        "- Hidden size is **1,280** and the FFN expands to **5,120**.\n",
        "- Total parameter count is ~20 B, which translates to ~40 GB in FP32 and ~20 GB in FP16.\n",
        "- 4‑bit quantization cuts that down to ~16 GB, making it feasible on a single RTX‑3090.\n",
        "\n",
        "Feel free to tweak the `config` object (e.g., `config.n_layer = 12`) to experiment with smaller variants, but remember that changing these values will require re‑training or fine‑tuning from scratch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the Model with Hugging Face\n",
        "\n",
        "### Why do we need a special loading routine?\n",
        "Think of the GPT‑OSS‑20B model as a gigantic library of books. Each book (layer) is stored on a different shelf (GPU). When you ask the model to generate text, you need to fetch the right books, read them, and then put them back. Hugging Face’s `AutoModelForCausalLM` is the librarian that knows how to locate, load, and hand out these books efficiently.\n",
        "\n",
        "### The `from_pretrained` magic\n",
        "```python\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-oss-20b\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "```\n",
        "- **`torch_dtype=torch.float16`** – tells PyTorch to keep intermediate activations in half‑precision, which cuts memory by ~50 % compared to full 32‑bit.\n",
        "- **`load_in_4bit=True`** – activates Bitsandbytes’ 4‑bit quantization. Imagine compressing a 32‑bit number to a 4‑bit nibble; you lose a little detail but the file shrinks dramatically.\n",
        "- **`device_map=\"auto\"`** – automatically shards the model across all available GPUs, so you don’t have to manually assign layers.\n",
        "- **`use_auth_token`** – required for large or private models; it authenticates your Hugging Face account.\n",
        "\n",
        "### What happens under the hood?\n",
        "1. **Download** the model weights from the Hugging Face hub.\n",
        "2. **Quantize** the weights to 4‑bit using Bitsandbytes.\n",
        "3. **Shard** the quantized tensors across GPUs.\n",
        "4. **Wrap** everything in a `CausalLM` head that can take token IDs and produce logits.\n",
        "\n",
        "### Key terms & trade‑offs\n",
        "- **Quantization**: Converting 32‑bit floating‑point weights to lower‑bit integers. It reduces memory and speeds up inference but can slightly hurt perplexity.\n",
        "- **Device map**: A strategy for distributing tensors across devices. `\"auto\"` is convenient but may not balance load perfectly on heterogeneous GPUs.\n",
        "- **Half‑precision (FP16)**: A compromise between speed and numerical stability. Some models still benefit from full FP32, but for GPT‑OSS‑20B FP16 is usually safe.\n",
        "- **HF_TOKEN**: Your personal Hugging Face authentication token. Keep it secret; it grants access to large models and private repos.\n",
        "\n",
        "### Why 4‑bit + FP16?\n",
        "The 20‑B model would normally need ~20 GB of VRAM in FP16. With 4‑bit quantization, that drops to ~4 GB, making it runnable on a single RTX‑3090. The trade‑off is a marginal increase in perplexity (≈1–2 %) and a tiny drop in generation quality. For most inference workloads, the speed‑memory gain outweighs this cost.\n",
        "\n",
        "### Quick sanity check\n",
        "After loading, we’ll generate a short sentence to confirm everything works:\n",
        "```python\n",
        "prompt = \"The future of AI is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "generated = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
        "```\n",
        "If you see a coherent continuation, the model is ready for the next steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Load tokenizer and model with 4‑bit quantization\n",
        "# Import required libraries\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Ensure reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Verify HF_TOKEN\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise EnvironmentError(\"HF_TOKEN not found. Export your Hugging Face token before running this cell.\")\n",
        "\n",
        "# Load tokenizer\n",
        "MODEL_NAME = \"EleutherAI/gpt-oss-20b\"\n",
        "print(\"Loading tokenizer...\", end=\" \")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(\"done\")\n",
        "\n",
        "# Load model with 4‑bit quantization and automatic device mapping\n",
        "print(\"Loading model (4‑bit, FP16)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "print(\"model loaded! GPU memory used: {:.2f} GB\".format(\n",
        "    torch.cuda.memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n",
        "))\n",
        "\n",
        "# Quick generation test\n",
        "prompt = \"The future of AI is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "generated = model.generate(**inputs, max_new_tokens=20)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Tokenization & Prompt Engineering\n",
        "\n",
        "### 1️⃣ What is tokenization?  \n",
        "Think of a sentence as a long string of characters that a computer can’t understand directly. Tokenization is the process of chopping that string into *tokens*—small, meaningful units that the model can work with.  \n",
        "\n",
        "- **Word‑level** tokenizers split on spaces, but they struggle with rare words.  \n",
        "- **Sub‑word** tokenizers (like BPE or WordPiece) break words into smaller pieces, so even unseen words can be represented.  \n",
        "- **Byte‑pair encoding (BPE)** starts with individual characters and repeatedly merges the most frequent pairs, creating a vocabulary that balances coverage and size.\n",
        "\n",
        "In GPT‑OSS‑20B we use a BPE‑style tokenizer that maps each token to an integer ID.  The model sees a sequence of IDs, not raw text.\n",
        "\n",
        "### 2️⃣ Prompt engineering  \n",
        "A *prompt* is the text you give the model to steer its output.  It’s like giving a chef a recipe: the more precise you are, the more likely the dish will match your taste.  Prompt engineering is the art of crafting that recipe.\n",
        "\n",
        "- **Prefix prompts**: “Translate the following sentence to French: …”  \n",
        "- **Instruction prompts**: “You are a helpful assistant. Answer the question.”  \n",
        "- **Few‑shot prompts**: Provide a few examples before the target input.\n",
        "\n",
        "The goal is to reduce ambiguity, guide the model’s internal attention, and control the style or format of the output.\n",
        "\n",
        "### 3️⃣ Sampling strategies & key terms  \n",
        "When the model finishes the prompt, it must decide which token to output next.  Several knobs let you trade off creativity vs. determinism:\n",
        "\n",
        "| Term | What it does | Typical values | Trade‑off |\n",
        "|------|--------------|----------------|-----------|\n",
        "| **Temperature** | Scales logits before softmax. Lower → more deterministic, higher → more random. | 0 (deterministic) – 1.5 (very creative) | 0 → safe but dull; 1+ → diverse but may hallucinate. |\n",
        "| **Top‑p (nucleus sampling)** | Keeps the smallest set of tokens whose cumulative probability ≥ p. | 0.8–0.95 | 0.8 → more focused; 0.95 → more diverse. |\n",
        "| **Repetition penalty** | Penalizes tokens that have already appeared. | 1.0 (none) – 1.5 | 1.0 → loops; >1 → discourages repetition. |\n",
        "| **Length penalty** | Adjusts score based on sequence length (used in beam search). | 0.8–1.2 | 0.8 → favors shorter outputs; 1.2 → encourages longer. |\n",
        "| **Beam width** | Number of parallel hypotheses kept during generation. | 1–10 | 1 → greedy; >1 → more exhaustive search. |\n",
        "\n",
        "### 4️⃣ Why these knobs matter  \n",
        "- **Deterministic generation** (temperature = 0) is great for debugging or when you need reproducible results.  \n",
        "- **Higher temperature** or **top‑p** introduces variety, useful for creative writing or sampling many candidates.  \n",
        "- **Repetition penalty** is essential for long‑form generation; without it the model can get stuck in loops.  \n",
        "- **Beam search** with a moderate width (e.g., 5) often yields higher‑quality text than greedy decoding, but at the cost of speed.\n",
        "\n",
        "Choosing the right combination depends on the task: a chatbot may favor low temperature for safety, while a story generator may use higher temperature for flair.\n",
        "\n",
        "### 5️⃣ Extra explanatory paragraph: key terms & rationale/trade‑offs\n",
        "**Token** – the smallest unit the model processes; it’s an integer ID that maps to a sub‑word or character.  Tokens are the building blocks of the input and output sequences.  \n",
        "\n",
        "**Tokenizer** – the algorithm that turns raw text into tokens and back.  It must be consistent between training and inference; otherwise the model will misinterpret the prompt.  \n",
        "\n",
        "**Prompt** – the text you feed to the model to elicit a desired response.  A well‑crafted prompt reduces ambiguity and guides the model’s attention.  \n",
        "\n",
        "**Sampling strategy** – the method the model uses to pick the next token.  Deterministic strategies (temperature = 0, greedy) guarantee reproducibility but can produce bland or repetitive text.  Stochastic strategies (temperature > 0, top‑p) increase diversity but risk incoherence or hallucinations.  \n",
        "\n",
        "**Trade‑offs** – Every knob balances speed, memory, and output quality.  For instance, a larger beam width improves quality but slows generation and consumes more GPU memory.  Lower temperature reduces randomness but may lead to repetitive or overly safe responses.  The art of prompt engineering is to find the sweet spot for your specific use case.\n",
        "\n",
        "---\n",
        "\n",
        "### 6️⃣ Quick hands‑on demo\n",
        "Below we’ll load the tokenizer and model, then generate the same prompt with different sampling settings to see how the output changes.  The code is intentionally short (≤30 lines) and fully reproducible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tokenization & Prompt Engineering demo\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣ Imports & reproducibility\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "SEED = 123\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# 2️⃣ Load tokenizer & model (reuse if already loaded)\n",
        "MODEL_NAME = \"EleutherAI/gpt-oss-20b\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\"Loading model (4‑bit, FP16)…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        ")\n",
        "\n",
        "# 3️⃣ Helper to generate text\n",
        "def generate(prompt, **gen_kwargs):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**inputs, **gen_kwargs)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# 4️⃣ Base prompt\n",
        "prompt = \"Explain the concept of tokenization in simple terms:\"\n",
        "\n",
        "# 5️⃣ Different sampling strategies\n",
        "print(\"\\nDeterministic (temperature=0):\")\n",
        "print(generate(prompt, temperature=0, max_new_tokens=50))\n",
        "\n",
        "print(\"\\nTemperature 0.8 (more creative):\")\n",
        "print(generate(prompt, temperature=0.8, max_new_tokens=50))\n",
        "\n",
        "print(\"\\nTop‑p 0.9 (nucleus sampling):\")\n",
        "print(generate(prompt, temperature=0.8, top_p=0.9, max_new_tokens=50))\n",
        "\n",
        "print(\"\\nRepetition penalty 1.2 (avoid loops):\")\n",
        "print(generate(prompt, temperature=0.8, repetition_penalty=1.2, max_new_tokens=50))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Baseline Generation & Evaluation\n",
        "\n",
        "### Why do we need a baseline?\n",
        "Think of the GPT‑OSS‑20B model as a chef who can cook an endless variety of dishes. Before we start tweaking ingredients (prompt engineering, temperature, etc.), we want a *reference plate* that shows what the chef normally produces. This reference plate is our **baseline** – it lets us measure how much a change improves or hurts the final dish.\n",
        "\n",
        "### What we’ll do in this section\n",
        "1. **Generate a small set of baseline responses** for a handful of prompts.\n",
        "2. **Quantitatively evaluate** those responses using BLEU and ROUGE, two classic metrics from machine‑translation research.\n",
        "3. **Add a quick human‑in‑the‑loop sanity check** so we can see if the numbers match what a person would think.\n",
        "\n",
        "### Key terms & trade‑offs\n",
        "- **BLEU (Bilingual Evaluation Understudy)** – a precision‑oriented metric that counts how many *n‑gram* overlaps exist between the generated text and a reference. It’s fast and easy to compute but can be overly harsh on creative language and doesn’t capture meaning.\n",
        "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** – a family of recall‑based metrics (ROUGE‑L, ROUGE‑N, ROUGE‑S) that measure how much of the reference content appears in the generated text. It tends to reward longer outputs and can be more forgiving than BLEU.\n",
        "- **Human‑in‑the‑loop** – a manual scoring step where a human annotator rates the quality (fluency, relevance, factuality). It’s the gold standard but expensive and slow.\n",
        "- **Trade‑offs** – BLEU is quick but may penalize novel phrasing; ROUGE is more tolerant but can over‑reward verbosity. Human scores are accurate but noisy and costly. In practice, we combine all three to get a balanced view.\n",
        "\n",
        "### Why baseline matters\n",
        "If you tweak temperature from 0.7 to 1.2 and BLEU drops from 0.45 to 0.38, you know the change hurt precision. If ROUGE jumps, maybe the model is adding more content but also more hallucinations. Human scores help confirm whether the numbers reflect real quality.\n",
        "\n",
        "---\n",
        "\n",
        "### Quick sanity check\n",
        "Below we’ll generate five responses for each prompt, compute BLEU/ROUGE against a short reference, and print the results. The code is split into two cells for clarity and reproducibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Generate baseline responses\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣ Imports & reproducibility\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2️⃣ Load tokenizer & model (reuse if already loaded)\n",
        "MODEL_NAME = \"EleutherAI/gpt-oss-20b\"\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\"Loading model (4‑bit, FP16)…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        ")\n",
        "\n",
        "# 3️⃣ Prompts and reference answers (short, hand‑crafted)\n",
        "prompts = [\n",
        "    \"Explain the concept of tokenization in simple terms.\",\n",
        "    \"What is the difference between supervised and unsupervised learning?\",\n",
        "    \"Describe the process of photosynthesis.\",\n",
        "]\n",
        "\n",
        "references = [\n",
        "    \"Tokenization is the process of breaking text into smaller pieces called tokens, which the model can understand.\",\n",
        "    \"Supervised learning uses labeled data, while unsupervised learning finds patterns in unlabeled data.\",\n",
        "    \"Photosynthesis is how plants convert sunlight into energy, producing glucose and oxygen.\",\n",
        "]\n",
        "\n",
        "# 4️⃣ Generate 5 samples per prompt\n",
        "generated_outputs = []\n",
        "for idx, prompt in enumerate(prompts):\n",
        "    print(f\"\\nPrompt {idx+1}: {prompt}\")\n",
        "    for i in range(5):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            do_sample=True,\n",
        "        )\n",
        "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        generated_outputs.append(text)\n",
        "        print(f\"  Sample {i+1}: {text}\")\n",
        "\n",
        "print(\"\\nBaseline generation complete.\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Compute BLEU & ROUGE, add human‑in‑the‑loop placeholder\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣ Install & import evaluation library (if not already installed)\n",
        "try:\n",
        "    import evaluate\n",
        "except ImportError:\n",
        "    import subprocess, sys\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"evaluate==0.4.2\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "    import evaluate\n",
        "\n",
        "# 2️⃣ Load metrics\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# 3️⃣ Prepare references and predictions for each prompt\n",
        "# The evaluate library expects lists of tokenized sentences\n",
        "predictions = []\n",
        "reference_lists = []\n",
        "\n",
        "# Re‑use the prompts and references from the previous cell\n",
        "for ref in references:\n",
        "    reference_lists.append([ref.split()])  # list of token lists\n",
        "\n",
        "# Flatten generated_outputs into a list of token lists\n",
        "for gen in generated_outputs:\n",
        "    predictions.append(gen.split())\n",
        "\n",
        "# 4️⃣ Compute metrics\n",
        "bleu_score = bleu.compute(predictions=predictions, references=reference_lists)\n",
        "rouge_score = rouge.compute(predictions=predictions, references=reference_lists)\n",
        "\n",
        "print(\"\\n=== Automatic Evaluation ===\")\n",
        "print(f\"BLEU score: {bleu_score['bleu']:.4f}\")\n",
        "print(f\"ROUGE‑L F1: {rouge_score['rougeL_fmeasure']:.4f}\")\n",
        "print(f\"ROUGE‑1 F1: {rouge_score['rouge1_fmeasure']:.4f}\")\n",
        "print(f\"ROUGE‑2 F1: {rouge_score['rouge2_fmeasure']:.4f}\")\n",
        "\n",
        "# 5️⃣ Human‑in‑the‑loop placeholder\n",
        "print(\"\\n=== Human Evaluation (manual) ===\")\n",
        "print(\"Please read each generated sample and rate it on a 1–5 scale for: fluency, relevance, factuality.\")\n",
        "print(\"(In a real workflow, you would collect these scores in a spreadsheet or annotation tool.)\")\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Profiling Inference Performance\n",
        "\n",
        "When you run a large language model like GPT‑OSS‑20B, you’re essentially asking a giant factory to produce text. Just like a factory, you want to know which machines (layers, attention heads, matrix multiplications) are the bottlenecks, how much memory each part consumes, and how long the whole process takes. \n",
        "\n",
        "In this section we’ll use **PyTorch’s built‑in profiler** to answer those questions. Think of the profiler as a stopwatch that also records how many workers (threads) are busy, how much memory is being used, and which parts of the code are the slowest. With that data you can decide whether to:\n",
        "\n",
        "* Move a layer to a faster GPU, \n",
        "* Reduce the batch size, \n",
        "* Quantize a sub‑module, or \n",
        "* Parallelize across multiple GPUs.\n",
        "\n",
        "### Key terms & trade‑offs\n",
        "- **CPU vs GPU**: The CPU is like a single worker that can do many different tasks, but it’s slower for matrix math. The GPU is a team of workers that can do the same math in parallel, but it has limited memory.\n",
        "- **Latency**: The time it takes to generate a single token. Lower latency means a snappier user experience.\n",
        "- **Throughput**: How many tokens you can generate per second. Higher throughput is important for batch inference or serving many users.\n",
        "- **Memory footprint**: The amount of VRAM used while the model is running. If you exceed VRAM, the GPU will swap to slower system RAM or crash.\n",
        "- **Profiler overhead**: Profiling adds a small amount of extra work (e.g., recording timestamps). For very short runs the overhead can dominate, so we’ll run a few warm‑up steps before measuring.\n",
        "\n",
        "**Why profiling matters**: Without it you might spend hours tweaking hyper‑parameters only to discover that the real bottleneck is a single attention head that is not on the fastest GPU. Profiling gives you a data‑driven map of where to focus your optimization efforts.\n",
        "\n",
        "### What we’ll do\n",
        "1. Load the model (if not already loaded) and set a deterministic seed.\n",
        "2. Warm‑up the model with a few dummy generations.\n",
        "3. Run the profiler while generating a short prompt.\n",
        "4. Print a concise summary of the most time‑consuming operations and memory usage.\n",
        "\n",
        "Feel free to adjust the `max_new_tokens` or `batch_size` to match your own workload.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Setup – imports, seed, and model loading (re‑use if already loaded)\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1️⃣ Set a global seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2️⃣ Load tokenizer & model if not already present\n",
        "MODEL_NAME = \"EleutherAI/gpt-oss-20b\"\n",
        "try:\n",
        "    tokenizer\n",
        "except NameError:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "try:\n",
        "    model\n",
        "except NameError:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "        use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        "    )\n",
        "\n",
        "print(\"Model and tokenizer ready – GPU memory used: {:.2f} GB\".format(\n",
        "    torch.cuda.memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n",
        "))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Profiling inference with torch.profiler\n",
        "# ------------------------------------------------------------\n",
        "import torch.profiler\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "# 3️⃣ Warm‑up: run a few generations without profiling\n",
        "warmup_prompt = \"Hello world\"\n",
        "for _ in range(3):\n",
        "    inputs = tokenizer(warmup_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    model.generate(**inputs, max_new_tokens=10)\n",
        "\n",
        "# 4️⃣ Define a simple generation function to profile\n",
        "\n",
        "def generate_once(prompt, max_new_tokens=20):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with record_function(\"model_inference\"):\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# 5️⃣ Run profiler\n",
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
        "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"runs/profiler_oss20b\"),\n",
        "    record_shapes=True,\n",
        "    profile_memory=True,\n",
        "    with_stack=True,\n",
        ") as prof:\n",
        "    for step in range(5):  # 5 steps to get a stable profile\n",
        "        generate_once(\"The quick brown fox jumps over the lazy dog.\", max_new_tokens=15)\n",
        "        prof.step()\n",
        "\n",
        "# 6️⃣ Print a concise summary\n",
        "print(\"\\n=== Profiling Summary ===\")\n",
        "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n",
        "print(\"\\nMemory usage (GPU): {:.2f} MB\".format(\n",
        "    torch.cuda.memory_allocated() / (1024 ** 2) if torch.cuda.is_available() else 0\n",
        "))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following best describes the trade‑off when applying 4‑bit quantization to GPT‑OSS‑20B?\", [\"Increased memory usage with higher accuracy\",\"Reduced memory usage with a slight drop in accuracy\",\"No change in memory usage but improved speed\",\"Significant accuracy loss with no memory benefit\"], 1, \"4‑bit quantization reduces memory footprint by ~80% but can slightly degrade model perplexity.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary advantage of using LoRA for fine‑tuning large language models?\", [\"It eliminates the need for GPU memory\",\"It adds a large number of trainable parameters\",\"It keeps the majority of the model frozen while adding few trainable weights\",\"It converts the model to a smaller architecture\"], 2, \"LoRA introduces a small set of trainable rank‑decomposition matrices, keeping most of the pre‑trained weights frozen.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:47:43.122Z",
      "title": "Advanced GPT‑OSS‑20B: Deep Dive into Architecture, Optimization, and Deployment",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T02:47:43.131Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}