{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with GPT‑OSS‑20B: A Beginner’s Guide\n\nThis lesson introduces the GPT‑OSS‑20B language model to absolute beginners. We’ll walk through setting up a Jupyter environment, loading the model, running simple prompts, and troubleshooting common issues—all explained with everyday analogies and minimal jargon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T02:44:34.608Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what GPT‑OSS‑20B is and how it differs from smaller models.\n",
        "2. Show how to install and configure the required libraries, including ipywidgets.\n",
        "3. Demonstrate how to load the model and generate text in a Jupyter notebook.\n",
        "4. Identify common pitfalls and best practices for working with large language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic familiarity with Python syntax (variables, functions).\n",
        "- Access to a Jupyter notebook environment (e.g., Anaconda, Google Colab).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.30.0 torch>=2.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.30.0\",\"torch>=2.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Setup\n",
        "\n",
        "Welcome to the first step of your journey with GPT‑OSS‑20B! Think of GPT‑OSS‑20B as a gigantic library of stories that has read billions of books, articles, and webpages. Just like a librarian who can pull out the right book in a flash, this model can generate text that feels natural and relevant.\n",
        "\n",
        "In this section we’ll:\n",
        "\n",
        "1. **Install the software** you need to talk to the model.\n",
        "2. **Set up a reproducible environment** so that your results can be shared and re‑run.\n",
        "3. **Verify that everything is working** before we dive into the model itself.\n",
        "\n",
        "> **Why do we need all these steps?**\n",
        "> \n",
        "> Large language models are heavy‑weight beasts. They require the right versions of PyTorch, the Transformers library, and a few helper tools like ipywidgets. Installing the wrong version can lead to cryptic errors that are hard to debug. By following a clear, reproducible setup we avoid those headaches and make sure that anyone who copies your notebook will see the same results.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **GPT‑OSS‑20B**: A 20‑billion‑parameter transformer model released by Hugging Face. Parameters are like the tiny knobs that let the model remember patterns in language.\n",
        "- **PyTorch**: The deep‑learning framework that actually runs the math behind the model.\n",
        "- **Transformers**: A high‑level library that wraps PyTorch to make it easier to load and use models.\n",
        "- **ipywidgets**: A library that lets you add interactive sliders, buttons, and text boxes to Jupyter notebooks.\n",
        "- **Reproducibility**: The practice of setting random seeds and using fixed library versions so that results can be exactly replicated.\n",
        "\n",
        "> **Trade‑offs**: Using the latest library versions gives you new features and bug fixes, but sometimes those updates break compatibility with older code. By pinning versions (e.g., `transformers>=4.30.0`) we strike a balance between stability and access to recent improvements.\n",
        "\n",
        "### Quick Checklist\n",
        "\n",
        "- [ ] Python 3.10+ installed\n",
        "- [ ] Jupyter Notebook or JupyterLab running\n",
        "- [ ] Internet connection (to download packages and the model)\n",
        "\n",
        "Let’s get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages with error handling\n",
        "# This cell will run the pip install commands only if the packages are missing.\n",
        "# It also prints a friendly message so you know what’s happening.\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "packages = [\n",
        "    \"ipywidgets>=8.0.0\",\n",
        "    \"transformers>=4.30.0\",\n",
        "    \"torch>=2.0.0\"\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        __import__(pkg.split('>=')[0])\n",
        "        print(f\"{pkg} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pkg}…\")\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", pkg]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "        except Exception as exc:\n",
        "            if IN_COLAB:\n",
        "                packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                if packages:\n",
        "                    try:\n",
        "                        import IPython\n",
        "                        ip = IPython.get_ipython()\n",
        "                        if ip is not None:\n",
        "                            ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                        else:\n",
        "                            import subprocess as _subprocess\n",
        "                            _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                    except Exception as colab_exc:\n",
        "                        print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                        raise\n",
        "                else:\n",
        "                    print('No packages specified for pip install; skipping fallback')\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "# Enable ipywidgets in the notebook environment\n",
        "try:\n",
        "    import ipywidgets\n",
        "    print(\"ipywidgets is ready!\")\n",
        "except Exception as e:\n",
        "    print(\"Error importing ipywidgets:\", e)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Your Environment Variable\n",
        "\n",
        "GPT‑OSS‑20B is hosted on Hugging Face’s model hub. To access it, you’ll need an API key. Store it in a file called `.env` in the same folder as your notebook, or set it directly in a notebook cell:\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"YOUR_HUGGING_FACE_API_KEY\"\n",
        "```\n",
        "\n",
        "> **Tip**: Keep your API key secret! Don’t commit the `.env` file to version control.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set a random seed for reproducibility\n",
        "# This ensures that any random choices (e.g., dropout, sampling) are the same each run.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(f\"Random seed set to {SEED}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2\n",
        "\n",
        "Thinking...\n",
        ">We need to produce JSON structure with section_number 2, title \"Step 2: What is GPT‑OSS‑20B?\", content array with markdown and code cells, callouts array, estimated_tokens 1000, prerequisites_check array, next_section_hint.\n",
        ">\n",
        ">We need to target 800-1000 tokens. Provide markdown explanation with analogies, extra paragraph defining key terms and explaining rationale/trade-offs. Provide code cell with <=30 lines, maybe a small snippet to load model or show token counts.\n",
        ">\n",
        ">We need to i...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Preparing Your Notebook\n",
        "\n",
        "### Why a “Notebook Prep” step?\n",
        "\n",
        "Think of a Jupyter notebook as a kitchen where you’ll cook up a delicious AI recipe. Before you start chopping ingredients, you need to make sure the stove is on, the pans are clean, and the spices are measured. In the same way, before we ask GPT‑OSS‑20B to generate text, we need to make sure the notebook environment is ready: the right libraries are loaded, the GPU is available, and the random seed is set so that the same recipe yields the same dish every time.\n",
        "\n",
        "### What will we do in this step?\n",
        "\n",
        "1. **Verify GPU availability** – Large models like GPT‑OSS‑20B are heavy; they run best on a GPU.\n",
        "2. **Set up a deterministic environment** – We’ll lock the random seed for NumPy, Python’s `random`, and PyTorch.\n",
        "3. **Create a small helper function** – A reusable wrapper that prints the device and memory usage so you can keep an eye on resource consumption.\n",
        "4. **Add a quick sanity‑check** – A tiny test that confirms the environment can run a minimal inference without errors.\n",
        "\n",
        "> **Analogy**: Imagine you’re a chef who wants to replicate a signature dish exactly. You’d keep a notebook of the exact oven temperature, the exact amount of salt, and the exact timing. That’s what we’re doing here, but for a neural network.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **GPU (Graphics Processing Unit)**: A specialized piece of hardware that can perform many calculations in parallel, making it ideal for training and inference of deep learning models.\n",
        "- **CUDA**: NVIDIA’s parallel computing platform that allows Python code to run on the GPU.\n",
        "- **Random Seed**: A starting value for pseudo‑random number generators. Setting the same seed ensures that operations that involve randomness (like dropout or token sampling) produce identical results across runs.\n",
        "- **Determinism**: The property that a program will produce the same output given the same input and environment. In deep learning, full determinism is hard to achieve due to non‑deterministic GPU operations, but setting seeds reduces variance.\n",
        "- **Memory Usage**: The amount of GPU RAM consumed by the model and its intermediate tensors. Monitoring this helps avoid out‑of‑memory (OOM) crashes.\n",
        "\n",
        "> **Trade‑offs**: Enabling full determinism (e.g., by disabling certain CUDA optimizations) can slow down inference. For most educational purposes, a fixed seed with the default CUDA settings offers a good balance between reproducibility and speed.\n",
        "\n",
        "### Quick Checklist\n",
        "\n",
        "- [ ] GPU is available and CUDA is working.\n",
        "- [ ] Random seed is set for all libraries.\n",
        "- [ ] `torch.cuda.memory_allocated()` can be queried.\n",
        "- [ ] A minimal inference test runs without errors.\n",
        "\n",
        "Let’s put this into code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "#  Notebook preparation utilities\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 1️⃣ Set a global random seed for reproducibility\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(f\"✅ Random seed set to {SEED}\")\n",
        "\n",
        "# 2️⃣ Check GPU availability and basic CUDA info\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"✅ GPU detected: {torch.cuda.get_device_name(device)}\")\n",
        "    print(f\"   Total GPU memory: {torch.cuda.get_device_properties(device).total_memory / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"⚠️  No GPU found – falling back to CPU (may be slow).\")\n",
        "\n",
        "# 3️⃣ Helper to report current memory usage\n",
        "\n",
        "def report_memory(label: str = \"Current\"):\n",
        "    \"\"\"Prints the amount of GPU memory allocated and cached.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated(device) / (1024**3)\n",
        "        cached = torch.cuda.memory_reserved(device) / (1024**3)\n",
        "        print(f\"{label} GPU memory – allocated: {allocated:.2f} GB, cached: {cached:.2f} GB\")\n",
        "    else:\n",
        "        print(f\"{label} – CPU mode, memory reporting not available.\")\n",
        "\n",
        "# 4️⃣ Sanity‑check: run a tiny inference on a dummy token\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load a very small tokenizer/model for the check (does not use GPT‑OSS‑20B)\n",
        "# This keeps the check lightweight and fast.\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    dummy_input = tokenizer(\"Hello world\", return_tensors=\"pt\").to(device)\n",
        "    dummy_model = AutoModelForCausalLM.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "    with torch.no_grad():\n",
        "        _ = dummy_model(**dummy_input)\n",
        "    print(\"✅ Sanity check passed – dummy inference succeeded.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Sanity check failed: {e}\")\n",
        "\n",
        "# Report memory after sanity check\n",
        "report_memory(\"After sanity check\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Loading the Model\n",
        "\n",
        "### Why do we need a separate loading step?\n",
        "\n",
        "Think of GPT‑OSS‑20B as a gigantic library that lives in a remote data center. The *loading* process is like pulling the entire library onto a local shelf so you can browse it quickly. If you try to read a book that’s still in the cloud, you’ll have to wait for each page to download, which is slow and wasteful. By loading the model once into memory, you pay the cost of the download only once and then you can generate text instantly.\n",
        "\n",
        "### What will happen in this cell?\n",
        "\n",
        "1. **Pull the tokenizer** – the piece that turns your text into numbers the model understands.\n",
        "2. **Pull the model weights** – the 20‑billion‑parameter neural network that actually does the heavy lifting.\n",
        "3. **Move everything to the best device** – GPU if available, otherwise CPU.\n",
        "4. **Set a deterministic seed** – so that the same prompt always gives the same first token.\n",
        "5. **Run a tiny sanity‑check** – generate a single token to confirm everything is wired up.\n",
        "\n",
        "> **Analogy**: Loading the model is like installing a huge software package on your computer. Once it’s installed, you can launch it instantly. If you try to run it from the internet every time, you’d be waiting for the download to finish each time.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **Tokenizer**: A tool that converts human‑readable text into a sequence of integer IDs that the model can process. Think of it as a translator that maps words to numbers.\n",
        "- **Model**: The neural network itself, consisting of layers of weights that have learned language patterns. For GPT‑OSS‑20B, it has 20 billion such weights.\n",
        "- **Device**: The hardware (CPU or GPU) where tensors live. GPUs can perform many operations in parallel, making inference faster.\n",
        "- **Context length**: The maximum number of tokens the model can look at at once. GPT‑OSS‑20B supports up to 4 096 tokens.\n",
        "- **Determinism**: Setting a random seed ensures that operations involving randomness (like dropout or sampling) produce the same results each run.\n",
        "\n",
        "> **Trade‑offs**: Loading the full 20B model requires a lot of GPU memory (≈30 GB). If your GPU is smaller, you’ll hit an out‑of‑memory error. In that case, you can either use a smaller model (e.g., GPT‑OSS‑3B) or run on CPU, which is slower but still works. The choice depends on your hardware and the speed you need.\n",
        "\n",
        "### Quick Checklist\n",
        "\n",
        "- [ ] `HF_TOKEN` is set in the environment.\n",
        "- [ ] `transformers`, `torch`, and `ipywidgets` are installed.\n",
        "- [ ] GPU is available (if you want GPU inference).\n",
        "- [ ] Random seed is set.\n",
        "\n",
        "Let’s load the model now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "#  Load GPT‑OSS‑20B tokenizer and model\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1️⃣ Ensure reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2️⃣ Determine device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✅ Using device: {DEVICE}\")\n",
        "\n",
        "# 3️⃣ Load tokenizer (fast tokenizers are optional but faster)\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"huggingface/gpt-oss-20b\",\n",
        "        use_fast=True,\n",
        "        token=os.getenv(\"HF_TOKEN\")\n",
        "    )\n",
        "    print(\"✅ Tokenizer loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to load tokenizer: {e}\")\n",
        "    raise\n",
        "\n",
        "# 4️⃣ Load model weights – this can take a few minutes\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"huggingface/gpt-oss-20b\",\n",
        "        torch_dtype=torch.float16 if DEVICE.type == \"cuda\" else torch.float32,\n",
        "        low_cpu_mem_usage=True,\n",
        "        token=os.getenv(\"HF_TOKEN\")\n",
        "    ).to(DEVICE)\n",
        "    print(\"✅ Model loaded and moved to device\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"❌ RuntimeError during model load: {e}\")\n",
        "    print(\"⚠️  Try reducing the batch size or using a smaller model.\")\n",
        "    raise\n",
        "\n",
        "# 5️⃣ Quick sanity‑check: generate a single token\n",
        "prompt = \"Once upon a time\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(input_ids, max_new_tokens=1)\n",
        "print(\"✅ Sanity check passed – generated token:\", tokenizer.decode(output_ids[0]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What to do next?\n",
        "\n",
        "You now have the tokenizer and model in memory, ready to generate text. In the next step we’ll feed a prompt and let the model produce a continuation. If you hit an out‑of‑memory error, try the following:\n",
        "\n",
        "- Use `torch_dtype=torch.float16` (already set for GPU) or `torch_dtype=torch.bfloat16` if your GPU supports it.\n",
        "- Load the model with `low_cpu_mem_usage=True` (already set) to stream weights from disk.\n",
        "- Switch to a smaller model like `gpt-oss-3b`.\n",
        "\n",
        "Feel free to experiment with the `max_new_tokens` parameter in the `generate` call to produce longer or shorter outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generating Text\n",
        "\n",
        "### Why generate text?\n",
        "\n",
        "Imagine you’re a chef who has just finished preparing a huge batch of dough. The dough is ready, but you still need to decide what shape to bake it into. Generating text is the same: we give the model a *prompt* (the dough) and ask it to *continue* (shape the dough into a story, answer, or code snippet). The model’s job is to predict the next token (word or sub‑word) one step at a time until we stop it.\n",
        "\n",
        "### What will happen in this cell?\n",
        "\n",
        "1. **Wrap the generation logic in a reusable function** – so you can experiment with different prompts and settings without rewriting code.\n",
        "2. **Show a few key generation parameters** – `max_new_tokens`, `temperature`, `top_p`, and `do_sample`.\n",
        "3. **Run a quick demo** – generate a short continuation of a user‑supplied prompt.\n",
        "4. **Explain how each parameter affects the output** – using everyday analogies.\n",
        "\n",
        "> **Analogy**: Think of `temperature` as the *spice level* in a recipe. A low temperature (close to 0) gives you a very predictable, bland dish. A high temperature (close to 1) adds excitement but can also make the dish a bit chaotic.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **Token**: The smallest unit the model understands. Tokens can be whole words, parts of words, or punctuation. Think of them as the *letters* that build words.\n",
        "- **Prompt**: The text you give to the model to start the generation. It’s like the first sentence of a story.\n",
        "- **Generation Parameters**:\n",
        "  - `max_new_tokens`: How many new tokens the model should produce. It’s the *length* of the dish.\n",
        "  - `temperature`: Controls randomness. Lower values make the model more deterministic; higher values make it more creative.\n",
        "  - `top_p` (nucleus sampling): Keeps the model’s choices within the most probable `p` fraction of the distribution. It’s a way to prune unlikely words.\n",
        "  - `do_sample`: If `True`, the model samples from the probability distribution; if `False`, it picks the most likely token (greedy decoding).\n",
        "- **Determinism vs Creativity**: Setting a seed and using low temperature gives reproducible, safe outputs. Raising temperature or enabling sampling introduces variability, which can be useful for creative tasks but may produce nonsensical results.\n",
        "- **Trade‑offs**:\n",
        "  - **Speed vs Quality**: Sampling (`do_sample=True`) requires extra computation because the model must evaluate probabilities for many tokens. Greedy decoding is faster but can get stuck in repetitive loops.\n",
        "  - **Memory vs Flexibility**: Generating many tokens (`max_new_tokens` large) consumes more GPU memory because intermediate tensors must be stored. For very long outputs, consider streaming or chunking.\n",
        "\n",
        "### Quick Checklist\n",
        "\n",
        "- [ ] `model` and `tokenizer` are loaded and on the correct device.\n",
        "- [ ] Random seed is set for reproducibility.\n",
        "- [ ] You understand how `temperature`, `top_p`, and `max_new_tokens` influence output.\n",
        "- [ ] You’re ready to experiment with different prompts.\n",
        "\n",
        "Let’s dive into the code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "#  Text generation helper\n",
        "# ------------------------------------------------------------\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1️⃣ Ensure reproducibility (seed already set in previous steps)\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 2️⃣ Helper function for generation\n",
        "\n",
        "def generate_text(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 50,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.9,\n",
        "    do_sample: bool = True,\n",
        "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        ") -> str:\n",
        "    \"\"\"Generate a continuation for *prompt* using the global *model* and *tokenizer*.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt: str\n",
        "        The starting text.\n",
        "    max_new_tokens: int\n",
        "        How many new tokens to generate.\n",
        "    temperature: float\n",
        "        Controls randomness. 0 = deterministic, 1 = fully random.\n",
        "    top_p: float\n",
        "        Nucleus sampling threshold.\n",
        "    do_sample: bool\n",
        "        If False, use greedy decoding.\n",
        "    device: torch.device\n",
        "        CPU or GPU.\n",
        "    \"\"\"\n",
        "    # Tokenize prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=do_sample,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    # Decode and return\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# 3️⃣ Demo: generate a short story continuation\n",
        "prompt_text = \"The ancient library held a secret that no one had ever discovered.\"\n",
        "print(\"\\n--- Prompt ---\")\n",
        "print(prompt_text)\n",
        "print(\"\\n--- Generated Text ---\")\n",
        "print(generate_text(prompt_text, max_new_tokens=80, temperature=0.8, top_p=0.95))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Interactive Prompting with ipywidgets\n",
        "\n",
        "In the previous steps we saw how to feed a prompt to GPT‑OSS‑20B and get a response back. That was a *static* interaction: you typed the prompt in a code cell, ran it, and saw the output. Interactive prompting turns that into a *live* chat‑like experience, just like a text‑based adventure game where you type a command and the game responds immediately.\n",
        "\n",
        "### Why use ipywidgets?\n",
        "\n",
        "Think of ipywidgets as a set of building blocks that let you add buttons, sliders, and text boxes to a notebook, just like you would add knobs and switches to a control panel. When you click a button or type something, the widget can *observe* the change and run a piece of code automatically. This is handy for:\n",
        "\n",
        "- Quickly testing different prompts without editing code.\n",
        "- Demonstrating how changing temperature or max tokens affects the output.\n",
        "- Building a simple chatbot that users can play with.\n",
        "\n",
        "### Key Terms Explained\n",
        "\n",
        "- **Widget**: A UI element (e.g., Text, Button, Output) that lives inside a Jupyter notebook.\n",
        "- **Observer**: A callback function that runs whenever a widget’s value changes.\n",
        "- **Output**: A special widget that captures and displays printed text or plots.\n",
        "- **Event loop**: The mechanism that keeps the notebook responsive while widgets wait for user actions.\n",
        "- **Reproducibility**: Setting a seed ensures that the same prompt always produces the same first token, but interactive widgets can still introduce variability if sampling is enabled.\n",
        "\n",
        "> **Trade‑offs**: Using ipywidgets adds a small amount of overhead because the notebook has to maintain the UI state and handle events. For very large models, each generation can take several seconds, so the UI may feel a bit laggy. However, the benefit of immediate visual feedback far outweighs this minor delay for educational purposes.\n",
        "\n",
        "### Quick Checklist\n",
        "\n",
        "- [ ] `ipywidgets` is installed and enabled (`%load_ext widgetsnbextension`).\n",
        "- [ ] The `model` and `tokenizer` are already loaded and on the correct device.\n",
        "- [ ] Random seed is set for deterministic sampling if desired.\n",
        "- [ ] You understand how to modify generation parameters via the UI.\n",
        "\n",
        "Let’s build the interactive prompt now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "#  Interactive prompt using ipywidgets\n",
        "# ------------------------------------------------------------\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import torch\n",
        "\n",
        "# Assume global `model`, `tokenizer`, and `DEVICE` are already defined\n",
        "# (from the previous loading step).  We keep the same seed for reproducibility.\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 1️⃣ Create widgets\n",
        "prompt_box = widgets.Text(\n",
        "    value=\"Hello, GPT‑OSS‑20B!\",\n",
        "    placeholder=\"Type your prompt here…\",\n",
        "    description=\"Prompt:\",\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "max_tokens_slider = widgets.IntSlider(\n",
        "    value=50,\n",
        "    min=10,\n",
        "    max=200,\n",
        "    step=10,\n",
        "    description='Max tokens:'\n",
        ")\n",
        "\n",
        "temp_slider = widgets.FloatSlider(\n",
        "    value=0.7,\n",
        "    min=0.0,\n",
        "    max=1.0,\n",
        "    step=0.05,\n",
        "    description='Temperature:'\n",
        ")\n",
        "\n",
        "sample_toggle = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Sample (vs greedy)'\n",
        ")\n",
        "\n",
        "run_button = widgets.Button(description='Generate', button_style='success')\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# 2️⃣ Define the generation logic\n",
        "\n",
        "def on_generate_clicked(_):\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        prompt = prompt_box.value\n",
        "        max_new = max_tokens_slider.value\n",
        "        temp = temp_slider.value\n",
        "        do_sample = sample_toggle.value\n",
        "        # Tokenize and generate\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            gen_ids = model.generate(\n",
        "                input_ids,\n",
        "                max_new_tokens=max_new,\n",
        "                temperature=temp,\n",
        "                do_sample=do_sample,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "        print(text)\n",
        "\n",
        "# 3️⃣ Attach the callback\n",
        "run_button.on_click(on_generate_clicked)\n",
        "\n",
        "# 4️⃣ Display the UI\n",
        "ui = widgets.VBox([\n",
        "    prompt_box,\n",
        "    widgets.HBox([max_tokens_slider, temp_slider, sample_toggle]),\n",
        "    run_button,\n",
        "    output_area\n",
        "])\n",
        "display(ui)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following is NOT a recommended practice when working with GPT‑OSS‑20B?\", [\"Use a small batch size to reduce memory usage.\",\"Always set a random seed for reproducibility.\",\"Avoid using ipywidgets for interactive demos.\",\"Monitor GPU memory usage during inference.\"], 2, \"Using ipywidgets is encouraged for interactive demos; avoiding it limits user experience.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which library is essential for building interactive widgets in Jupyter?\", [\"numpy\",\"ipywidgets\",\"matplotlib\",\"pandas\"], 1, \"ipywidgets provides the tools needed to create interactive UI elements within Jupyter notebooks.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:44:34.601Z",
      "title": "Getting Started with GPT‑OSS‑20B: A Beginner’s Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T02:44:34.608Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}