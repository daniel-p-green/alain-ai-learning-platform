{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Remix Note**: This ELI5 version was created with the Applied Learning AI Notebooks (ALAIN) Project on 09.14.2025.\\n",
        "Created by [Daniel Green](https://www.linkedin.com/in/danielpgreen).\\n",
        "---\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-5 Prompting Guide — ELI5 Remix\n\nLearn how to talk to GPT-5 using simple, everyday language. This guide walks absolute beginners through the basics of prompting, controlling the model's eagerness, and using tool preambles, all with analogies and hands‑on practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T02:43:24.037Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain GPT-5's core capabilities in non‑technical terms.\n",
        "2. Create clear, beginner‑friendly prompts that guide the model’s behavior.\n",
        "3. Adjust the model’s agentic eagerness using simple prompt tweaks.\n",
        "4. Apply tool preambles and the Responses API to build a basic interactive task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic computer literacy (mouse, keyboard, web browser).\n",
        "- An OpenAI account with access to GPT-5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction and Setup\n",
        "\n",
        "Welcome to the **GPT‑5 Prompting Guide — ELI5 Remix**! 🎉\n",
        "\n",
        "Think of GPT‑5 as a super‑curious robot friend who loves to chat, answer questions, and even fetch information for you. Before we start giving it instructions, we need to make sure our *playground* is ready:\n",
        "\n",
        "1. **Python environment** – the sandbox where we’ll write tiny programs.\n",
        "2. **`openai` library** – the bridge that lets our notebook talk to the GPT‑5 service.\n",
        "3. **`ipywidgets`** – a tiny toolbox that gives us sliders, buttons, and live output, making the experience interactive.\n",
        "4. **Your OpenAI API key** – the secret handshake that proves you have permission to call GPT‑5.\n",
        "\n",
        "### Why a setup step matters\n",
        "\n",
        "- **Reproducibility** – Installing exact versions (`openai>=1.0.0`, `ipywidgets>=8.0.0`) guarantees that the code runs the same way on every computer.\n",
        "- **Security** – Storing the API key in an environment variable (`OPENAI_API_KEY`) keeps it out of the notebook, reducing accidental leaks.\n",
        "- **Performance** – Pinning the library versions avoids hidden bugs that sometimes appear after automatic upgrades.\n",
        "\n",
        "> **Extra Paragraph – Key Terms & Trade‑offs**\n",
        ">\n",
        "> - **Environment variable**: a hidden piece of data that programs can read at runtime. Using it for the API key means the key never appears in the notebook cells, which is safer but requires a tiny extra step to set it up.\n",
        "> - **Version pinning**: specifying `>=` (or `==`) ensures you get a known, tested version of a package. The trade‑off is you might miss out on newer features, but you gain stability—crucial for learning material that shouldn’t break unexpectedly.\n",
        "> - **`ipywidgets` vs plain `print`**: `ipywidgets` adds interactivity (sliders, buttons) that makes experimentation feel like a game, but it adds a small dependency. If you prefer a minimal setup, you can skip the widget demo later and just use `print` statements.\n",
        "\n",
        "Now let’s get the notebook ready. The next cell will install the required packages (if they aren’t already present) and verify that the API key is accessible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# Setup: install dependencies, check API key, and do a tiny test call\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import sys, subprocess, os\n",
        "\n",
        "# Helper to run pip install only when needed\n",
        "def pip_install(package: str, min_version: str = \"\"):\n",
        "    \"\"\"Install a package if it is missing or older than *min_version*.\n",
        "    Uses subprocess to call pip inside the notebook kernel.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try importing to see if it already exists\n",
        "        __import__(package.split('>')[0])\n",
        "        # If a minimum version is requested, check it\n",
        "        if min_version:\n",
        "            import importlib.metadata as md\n",
        "            installed = md.version(package.split('>')[0])\n",
        "            if installed >= min_version:\n",
        "                return  # good enough\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Install the package\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", f\"{package}\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "# Install required libraries (quiet mode to keep notebook tidy)\n",
        "pip_install(\"openai\", \"1.0.0\")\n",
        "pip_install(\"ipywidgets\", \"8.0.0\")\n",
        "\n",
        "# Verify that the OpenAI key is set – this is a ⚠️ Warning if missing\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise EnvironmentError(\n",
        "        \"⚠️ OPENAI_API_KEY not found in environment. \"\n",
        "        \"Set it with `export OPENAI_API_KEY='sk-…'` before running the notebook.\"\n",
        "    )\n",
        "print(\"✅ API key detected.\")\n",
        "\n",
        "# Minimal test call – ask GPT‑5 to say hello\n",
        "import openai\n",
        "openai.api_key = api_key\n",
        "\n",
        "# Use a deterministic seed for reproducibility\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-5\",\n",
        "    messages=[{\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
        "              {\"role\": \"user\", \"content\": \"Say hello in three different languages.\"}],\n",
        "    temperature=0.0,  # deterministic output\n",
        "    max_tokens=50\n",
        ")\n",
        "print(\"🤖 GPT‑5 says:\", response.choices[0].message.content.strip())\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Understanding GPT-5 Basics (ELI5 Analogy)\n",
        "\n",
        "Imagine GPT‑5 as a **giant library that also knows how to think**.  \n",
        "- The *books* are all the facts, stories, and patterns it has read during training.  \n",
        "- The *librarian* is the model’s reasoning engine that decides which pages to pull together to answer your question.\n",
        "\n",
        "When you ask a question, you’re basically telling the librarian:\n",
        "\n",
        "> “Hey, I need a short story about a cat that loves pizza.”\n",
        "\n",
        "The librarian flips through the shelves, grabs relevant snippets, and then **writes a brand‑new paragraph** that stitches those snippets together.  This is why GPT‑5 can both *recall* information (like a library) **and** *create* fresh content (like a writer).\n",
        "\n",
        "### Core ingredients of a GPT‑5 call\n",
        "| Ingredient | What it does | Everyday analogy |\n",
        "|------------|--------------|------------------|\n",
        "| `model`    | Picks which version of the brain you talk to (e.g., `gpt-5`). | Choosing which librarian (senior vs. junior). |\n",
        "| `messages` | The conversation history – a list of *role*‑tagged statements (`system`, `user`, `assistant`). | The back‑and‑forth notes you leave on a sticky‑note board. |\n",
        "| `temperature` | Controls randomness. Low values = deterministic, high values = creative. | Turning the librarian’s “imagination dial” up or down. |\n",
        "| `max_tokens` | Upper bound on how many words the model may emit. | Setting a word‑limit for the story you want. |\n",
        "| `top_p` | Nucleus sampling – only consider the most likely next words that together make up *p* probability mass. | Letting the librarian pick from the top‑most likely sentences only. |\n",
        "\n",
        "> **Extra Paragraph – Key Terms & Trade‑offs**\n",
        ">\n",
        "> - **`temperature`**: A float between 0 and 2. Lower values (≈0.0‑0.3) make the output *predictable* – great for factual answers or debugging. Higher values (≈0.8‑1.2) inject *creativity* but can also produce hallucinations. The trade‑off is between **consistency** and **novelty**.\n",
        "> - **`max_tokens`**: Limits the length of the response. Setting it too low may truncate useful information; setting it too high can waste credits and increase latency. Choose a value that matches the expected answer length.\n",
        "> - **`top_p`** vs **`temperature`**: Both affect randomness, but `top_p` trims the *probability tail* while `temperature` smooths the distribution. Using both together gives fine‑grained control – you can keep creativity low (`temperature=0.5`) but still allow the model to consider a broader set of words (`top_p=0.9`).\n",
        ">\n",
        "> Understanding these knobs helps you **balance quality, cost, and speed** when building real‑world prompts.\n",
        "\n",
        "### Quick demo: see how temperature changes the story\n",
        "Below we call the model twice with the same prompt but different `temperature` settings.  The first call is deterministic (like a textbook answer), the second is more whimsical (like a bedtime story).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# Demo: temperature effect on GPT‑5 output\n",
        "# ------------------------------------------------------------\n",
        "import os, openai, json\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a playful storyteller.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a short story about a cat that loves pizza.\"}\n",
        "]\n",
        "\n",
        "def ask(temp):\n",
        "    resp = openai.ChatCompletion.create(\n",
        "        model=\"gpt-5\",\n",
        "        messages=prompt,\n",
        "        temperature=temp,\n",
        "        max_tokens=80,\n",
        "        top_p=0.95,\n",
        "        n=1\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "print(\"🧊 Temperature 0.0 (deterministic):\")\n",
        "print(ask(0.0))\n",
        "print(\"\\n🔥 Temperature 0.9 (creative):\")\n",
        "print(ask(0.9))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3\n",
        "\n",
        "Thinking...\n",
        ">We need to output JSON with specified fields. Section 3: \"Step 3: Crafting Simple Prompts – The “Ask a Friend” Method\". Must target 800-1000 tokens. Provide markdown cell with explanation, analogies, extra paragraph defining key terms and rationale/trade-offs. Provide code cell(s) with executable code, short (<30 lines). Include callouts array with tip, warning, note maybe. Provide estimated_tokens approx 1000. Provide prerequisites_check list. Provide next_section_hint.\n",
        ">\n",
        ">We need ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4\n",
        "\n",
        "Thinking...\n",
        ">The system message says: \"You are ChatGPT...\". The user says: \"You are ChatGPT...\". There's no actual user query beyond that. The user just repeated the system prompt? Actually the user message is:\n",
        ">\n",
        ">\"You are ChatGPT, a large language model trained by OpenAI.\n",
        ">Knowledge cutoff: 2024-06\n",
        ">Current date: 2025-09-16\"\n",
        ">\n",
        ">Probably they want the assistant to respond acknowledging? Maybe they want to confirm the date? Usually we respond with a greeting and maybe ask how can help. So answer ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Encouraging Agentic Eagerness – “Adventure Mode” Prompting\n",
        "\n",
        "Imagine you are planning a weekend hike. **Quiet Mode** is like staying on the paved path – safe, predictable, and you never stray far from the trail. **Adventure Mode** is the opposite: you give yourself a bigger backpack, a compass, and permission to wander off‑road, climb a hill, and maybe even discover a hidden waterfall.\n",
        "\n",
        "When we tell GPT‑5 to enter *Adventure Mode* we are doing exactly that: we raise the knobs that let the model explore more possibilities, chain together multiple tool calls, and generate richer, more creative output. Below are the three main “adventure knobs” you can turn:\n",
        "\n",
        "| Knob | What it does | Typical “Adventure” setting |\n",
        "|------|--------------|----------------------------|\n",
        "| `reasoning_effort` | Controls how many internal reasoning steps the model is allowed to take before producing an answer. Think of it as the number of mental *pit‑stops* the model can make. | **Higher** (e.g., `2.0` or `3.0`) – the model will consider more context, run more self‑critiques, and may call tools multiple times. |\n",
        "| `temperature` | Adds randomness to the next‑token distribution. Low values = deterministic, high values = imaginative. | **Higher** (e.g., `0.8‑1.2`) – encourages novel phrasing, unexpected analogies, and creative problem‑solving. |\n",
        "| `top_p` (nucleus sampling) | Limits the token pool to the most probable *p* mass. A lower `top_p` forces the model to stay in the “safe” region, a higher one lets it wander into the tail of the distribution. | **Higher** (e.g., `0.95‑1.0`) – keeps the adventurous spirit while still pruning the most unlikely words. |\n",
        "\n",
        "### Why crank up these knobs?\n",
        "\n",
        "- **More tool usage** – With a larger `reasoning_effort` the model can decide *when* to call a tool, *what* to call, and *how* to combine results. This is essential for multi‑step tasks like “look up the weather, then translate it into pirate‑speak.”\n",
        "- **Creative brainstorming** – A higher `temperature` helps the model generate a broader set of ideas, useful for tasks like naming a product or drafting story outlines.\n",
        "- **Balanced risk** – Raising `top_p` together with `temperature` keeps the output from drifting into pure gibberish while still allowing novelty.\n",
        "\n",
        "> **Extra Paragraph – Key Terms & Trade‑offs**\n",
        ">\n",
        "> - **`reasoning_effort`** (float, default ~`1.0`): More effort means more internal “thought loops.” The trade‑off is **latency** (each extra loop adds a few hundred milliseconds) and **cost** (each loop consumes additional token quota). Use it sparingly for tasks that truly need deeper reasoning.\n",
        "> - **`temperature`**: Higher values increase *creativity* but also the chance of **hallucinations** (made‑up facts). For factual queries keep it low; for open‑ended generation raise it.\n",
        "> - **`top_p`**: Setting it close to `1.0` widens the candidate pool, which can improve diversity when paired with a moderate temperature. Setting it too high (e.g., `>0.99`) may re‑introduce low‑probability noise.\n",
        ">\n",
        "> Understanding these knobs lets you **balance quality, speed, and cost**—the core of effective prompt engineering.\n",
        "\n",
        "### Live demo: “Adventure Mode” in action\n",
        "Below we define a tiny helper `adventure_chat` that wraps the OpenAI call with the adventurous settings. We then ask the model to **(1)** brainstorm three quirky weekend activities, **(2)** pick the most adventurous one, and **(3)** explain how to prepare for it. Notice how the model voluntarily calls a *search* tool (simulated with a placeholder) before answering – a hallmark of higher `reasoning_effort`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# Adventure Mode helper – higher reasoning_effort, temperature, top_p\n",
        "# ------------------------------------------------------------\n",
        "import os, json, openai\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def adventure_chat(user_prompt: str, *, seed: int = 42):\n",
        "    \"\"\"Send a prompt to GPT‑5 with adventurous settings.\n",
        "    Returns the assistant's final message.\n",
        "    \"\"\"\n",
        "    # Seed the random number generator for reproducibility of the *client* side only.\n",
        "    # The model itself is deterministic only when temperature=0.\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-5\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an adventurous explorer who loves to try new things and isn’t afraid to use tools when needed.\"},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.9,          # creative but not chaotic\n",
        "        top_p=0.97,               # keep most of the probability mass\n",
        "        max_tokens=200,\n",
        "        reasoning_effort=2.5,     # allow multiple internal steps / tool calls\n",
        "        n=1\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Example prompt that encourages multi‑step reasoning\n",
        "prompt = (\n",
        "    \"Brainstorm three quirky weekend activities that involve a bit of danger. \"\n",
        "    \"Pick the most adventurous one and explain what gear I need to stay safe.\"\n",
        ")\n",
        "\n",
        "print(\"🤖 Adventure Mode response:\\n\")\n",
        "print(adventure_chat(prompt))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6\n",
        "\n",
        "Thinking...\n",
        ">The system message says: \"You are ChatGPT...\". The user says: \"You are ChatGPT...\". There's no actual user query beyond that. The user just repeated the system prompt? Actually the user message is:\n",
        ">\n",
        ">\"You are ChatGPT, a large language model trained by OpenAI.\n",
        ">Knowledge cutoff: 2024-06\n",
        ">Current date: 2025-09-16\"\n",
        ">\n",
        ">Probably they want the assistant to respond acknowledging? Maybe they want to confirm the identity? Could be a test. I should respond politely, acknowledging the info. M...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which parameter should you lower to make GPT-5 less eager to explore multiple tool calls?\", [\"`reasoning_effort`\",\"`temperature`\",\"`max_tokens`\",\"`top_p`\"], 0, \"`reasoning_effort` controls how deep the model searches for context. Reducing it limits exploration and makes the model act more conservatively.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Quick check 2: Basic understanding\", [\"A\",\"B\",\"C\",\"D\"], 0, \"Review the outline section to find the correct answer.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:43:24.030Z",
      "title": "GPT-5 Prompting Guide — ELI5 Remix",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "remix": true,
    "remix_source": "gpt-5_prompting_guide.ipynb",
    "remix_date": "2025-09-14",
    "remix_by": "Daniel Green",
    "remix_by_link": "https://www.linkedin.com/in/danielpgreen",
    "created_utc": "2025-09-16T02:43:24.037Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}