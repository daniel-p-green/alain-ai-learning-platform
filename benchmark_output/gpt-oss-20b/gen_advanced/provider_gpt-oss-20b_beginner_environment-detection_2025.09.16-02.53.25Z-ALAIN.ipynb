{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine‑Tuning and Deploying GPT‑Oss‑20B: Advanced Techniques for Research and Production\n\nThis notebook guides advanced practitioners through the end‑to‑end workflow of fine‑tuning GPT‑Oss‑20B on custom corpora, optimizing inference performance, and deploying the model in a scalable, low‑latency environment. It covers trade‑offs between precision, speed, and resource usage, and provides expert insights into model scaling and reproducibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T02:53:25.316Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Understand the architectural differences and scaling behavior of GPT‑Oss‑20B compared to smaller variants.\n",
        "2. Implement distributed fine‑tuning using Hugging Face Accelerate and DeepSpeed, optimizing memory usage and throughput.\n",
        "3. Apply quantization, pruning, and model parallelism to reduce inference latency while preserving accuracy.\n",
        "4. Deploy the optimized model as a RESTful service with autoscaling on Kubernetes, ensuring reproducibility and monitoring.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+ with virtualenv or conda\n",
        "- PyTorch 2.0+ (CUDA 12.1 or higher)\n",
        "- Basic familiarity with Hugging Face Transformers and Datasets\n",
        "- Experience with distributed training concepts (DDP, ZeRO)\n",
        "- Kubernetes cluster or Minikube for deployment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 torch>=2.0 transformers>=4.40 accelerate>=0.25 datasets>=2.16 deepspeed>=0.12 fastapi>=0.110 uvicorn>=0.29 kubernetes>=28\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"torch>=2.0\",\"transformers>=4.40\",\"accelerate>=0.25\",\"datasets>=2.16\",\"deepspeed>=0.12\",\"fastapi>=0.110\",\"uvicorn>=0.29\",\"kubernetes>=28\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Validation and Baseline Benchmarking\n",
        "\n",
        "Before we dive into fine‑tuning, let’s make sure our playground is ready. Think of this like checking the ingredients before baking a cake: if the oven is off or the flour is stale, the final result will be ruined. Here we’ll verify that the right versions of Python, PyTorch, CUDA, and the Hugging Face libraries are installed, that a GPU is visible, and that we can actually load the GPT‑Oss‑20B model.\n",
        "\n",
        "### Why do we need a baseline?\n",
        "\n",
        "A baseline benchmark is a quick sanity check that tells us:\n",
        "\n",
        "1. **Latency** – how long a single inference takes.\n",
        "2. **Memory usage** – how much GPU RAM the model consumes.\n",
        "3. **Reproducibility** – by setting a random seed we can later compare results.\n",
        "\n",
        "If the baseline fails, we’ll know that the issue is environmental, not algorithmic.\n",
        "\n",
        "### Key terms explained\n",
        "\n",
        "- **CUDA** – NVIDIA’s parallel computing platform that lets GPUs crunch numbers.\n",
        "- **PyTorch** – a deep‑learning framework that manages tensors and autograd.\n",
        "- **Hugging Face Transformers** – a library that ships pre‑trained models and tokenizers.\n",
        "- **Baseline** – a minimal, repeatable test that establishes performance expectations.\n",
        "- **Reproducibility** – the ability to get the same results when running the same code again.\n",
        "\n",
        "Trade‑offs: Using the full 20B model on a single GPU will exceed memory limits; that’s why we’ll load the model in *half‑precision* (fp16) for the baseline. If you have a multi‑GPU setup, you can later switch to *bfloat16* or *int8* for faster inference.\n",
        "\n",
        "### Quick checklist\n",
        "\n",
        "- Python ≥ 3.10\n",
        "- PyTorch ≥ 2.0 with CUDA 12.1\n",
        "- Transformers ≥ 4.40\n",
        "- Accelerate, Datasets, DeepSpeed installed\n",
        "- `HF_TOKEN` set in your environment\n",
        "- `CUDA_VISIBLE_DEVICES` pointing to at least one GPU\n",
        "\n",
        "If any of these are missing, the code below will raise an informative error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment validation cell\n",
        "# ------------------------------------------------------------\n",
        "# This cell checks that the required libraries and GPU are available.\n",
        "# It also prints the versions for reproducibility.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# 1. Python version\n",
        "print(f\"Python version: {sys.version.split()[0]}\")\n",
        "\n",
        "# 2. PyTorch version and CUDA\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "# 3. Transformers version\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "\n",
        "# 4. Check HF_TOKEN\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "if hf_token is None:\n",
        "    raise EnvironmentError(\"HF_TOKEN not found in environment. Please export it before running.\")\n",
        "else:\n",
        "    print(\"HF_TOKEN found.\")\n",
        "\n",
        "# 5. Verify GPU visibility\n",
        "visible_gpus = os.getenv(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
        "print(f\"CUDA_VISIBLE_DEVICES: {visible_gpus}\")\n",
        "\n",
        "print(\"\\nEnvironment validation complete. All checks passed!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline benchmarking cell\n",
        "# ------------------------------------------------------------\n",
        "# We will load GPT‑Oss‑20B in fp16 and run a single inference.\n",
        "# The goal is to measure latency and peak memory usage.\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Set a deterministic seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load tokenizer and model (fp16 for memory efficiency)\n",
        "model_name = \"gpt-oss-20b\"\n",
        "print(f\"Loading {model_name} (fp16)...\")\n",
        "\n",
        "# Use accelerate to automatically move to GPU if available\n",
        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "\n",
        "with init_empty_weights():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "\n",
        "model.to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "# Prepare a simple prompt\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Warm‑up run\n",
        "with torch.no_grad():\n",
        "    _ = model.generate(**inputs, max_new_tokens=10)\n",
        "\n",
        "# Measure latency\n",
        "start = time.perf_counter()\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "end = time.perf_counter()\n",
        "\n",
        "latency = (end - start) * 1000  # ms\n",
        "print(f\"Inference latency: {latency:.2f} ms\")\n",
        "\n",
        "# Peak memory usage\n",
        "peak_mem = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
        "print(f\"Peak GPU memory: {peak_mem:.2f} GB\")\n",
        "\n",
        "# Clean up\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Baseline benchmark complete.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Architectural Deep Dive – GPT‑Oss‑20B vs. GPT‑Oss‑6B\n",
        "\n",
        "When you compare a 20‑billion‑parameter model to a 6‑billion‑parameter cousin, it’s a bit like comparing a luxury sedan to a compact car. Both run on the same engine family, but the sedan has more seats, a bigger trunk, and a few extra gadgets. In the world of language models, those extra “gadgets” are more layers, larger hidden states, and more attention heads. Let’s unpack what that really means.\n",
        "\n",
        "### 1️⃣ Parameter Count\n",
        "- **GPT‑Oss‑20B** ≈ 20 B trainable weights.\n",
        "- **GPT‑Oss‑6B** ≈ 6 B trainable weights.\n",
        "\n",
        "Think of parameters as the knobs you can turn to fine‑tune the model’s behavior. More knobs give the model more expressive power but also make it heavier to run.\n",
        "\n",
        "### 2️⃣ Hidden Size & Layers\n",
        "| Model | Hidden Size | # Layers |\n",
        "|-------|-------------|----------|\n",
        "| 20B   | 12 288      | 48       |\n",
        "| 6B    | 4 096       | 32       |\n",
        "\n",
        "Hidden size is the width of the internal “brain” – larger hidden size means each token is represented by a bigger vector. More layers stack more transformations on top of each other, allowing the model to learn deeper patterns.\n",
        "\n",
        "### 3️⃣ Attention Heads\n",
        "| Model | Heads |\n",
        "|-------|-------|\n",
        "| 20B   | 96    |\n",
        "| 6B    | 32    |\n",
        "\n",
        "Attention heads are like parallel microphones listening to different parts of the conversation. More heads let the model capture more nuanced relationships.\n",
        "\n",
        "### 4️⃣ Memory Footprint & Compute\n",
        "- **Memory**: Roughly proportional to *parameters × precision*. A 20B model in fp16 uses ~30 GB of GPU RAM, while a 6B model uses ~9 GB.\n",
        "- **Compute**: FLOPs per token ≈ 2 × hidden_size² × layers. The 20B model needs ~3× the compute of the 6B model for the same prompt length.\n",
        "\n",
        "### 5️⃣ Trade‑offs\n",
        "| Aspect | 20B | 6B |\n",
        "|--------|-----|-----|\n",
        "| **Accuracy** | Higher on long‑form, nuanced tasks | Good enough for many applications |\n",
        "| **Latency** | Slower (unless you use model parallelism or quantization) | Faster on a single GPU |\n",
        "| **Resource Cost** | Higher GPU memory, power, and cooling | Lower cost, easier to deploy |\n",
        "\n",
        "Choosing between them is like picking a vehicle: if you need to haul a big family or cargo, go for the 20B; if you’re commuting in a city, the 6B will get you there faster and cheaper.\n",
        "\n",
        "### Extra Explanatory Paragraph – Key Terms & Rationale\n",
        "- **Parameter**: A weight in the neural network that gets updated during training. More parameters usually mean a richer representation but also more memory and compute.\n",
        "- **Hidden Size**: The dimensionality of the internal token embeddings. Larger hidden sizes allow the model to encode more information per token.\n",
        "- **Attention Head**: A sub‑module that learns to focus on different parts of the input sequence. More heads increase the model’s ability to capture diverse relationships.\n",
        "- **Layer**: A stack of transformations (self‑attention + feed‑forward). More layers deepen the model’s reasoning.\n",
        "- **Precision (fp16, bf16, int8)**: Determines how many bits are used to store each weight. Lower precision reduces memory and speeds up inference but can hurt accuracy.\n",
        "- **Model Parallelism**: Splitting a single model across multiple GPUs to fit larger models into memory.\n",
        "- **Quantization**: Converting weights from 32‑bit floats to 8‑bit integers to shrink the model size and accelerate inference.\n",
        "\n",
        "The rationale behind these trade‑offs is simple: more parameters and larger hidden sizes give the model a bigger “brain” to learn from data, but they also demand more hardware resources. In practice, you balance the desired performance with the available compute budget, often using techniques like quantization or model parallelism to bridge the gap.\n",
        "\n",
        "### Quick Code Demo\n",
        "Below we load the configuration for both models and print a concise summary. This will help you see the numbers that drive the trade‑offs discussed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load model configs and compare key hyper‑parameters\n",
        "# ------------------------------------------------------------\n",
        "# Requires: transformers>=4.40, torch\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# Set deterministic seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Model names\n",
        "models = {\n",
        "    \"GPT‑Oss‑20B\": \"gpt-oss-20b\",\n",
        "    \"GPT‑Oss‑6B\":  \"gpt-oss-6b\"\n",
        "}\n",
        "\n",
        "for name, repo in models.items():\n",
        "    cfg = AutoConfig.from_pretrained(repo)\n",
        "    print(f\"\\n{name} (repo: {repo})\")\n",
        "    print(f\"  Hidden size   : {cfg.hidden_size}\")\n",
        "    print(f\"  Num layers    : {cfg.num_hidden_layers}\")\n",
        "    print(f\"  Attention heads: {cfg.num_attention_heads}\")\n",
        "    print(f\"  Parameter count: {cfg.num_parameters() // 1e9:.2f} B\")\n",
        "\n",
        "# Quick sanity check: total parameters match expected values\n",
        "assert models[\"GPT‑Oss‑20B\"] == \"gpt-oss-20b\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Estimate GPU memory usage for fp16 and bf16\n",
        "# ------------------------------------------------------------\n",
        "# Memory per parameter (bytes) = 2 for fp16, 2 for bf16\n",
        "# We add a small overhead for activations (~1.5×)\n",
        "\n",
        "import math\n",
        "\n",
        "for name, repo in models.items():\n",
        "    cfg = AutoConfig.from_pretrained(repo)\n",
        "    params = cfg.num_parameters()\n",
        "    mem_fp16 = params * 2 / (1024**3)  # GB\n",
        "    mem_bf16 = params * 2 / (1024**3)  # same size, but bf16 may be faster on newer GPUs\n",
        "    mem_overhead = mem_fp16 * 1.5\n",
        "    print(f\"\\n{name} memory estimate (fp16 + overhead): {mem_overhead:.2f} GB\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Preparing the Dataset – Tokenization & Sharding\n",
        "\n",
        "When you want to train a language model, the first thing you need is a *clean, token‑ready* dataset. Think of it like preparing a grocery list before you go shopping: you want to know exactly what ingredients you need and how many of each. In the same way, tokenization turns raw text into a sequence of integer IDs that the model can understand, and sharding splits that sequence into manageable chunks that can be fed to multiple GPUs in parallel.\n",
        "\n",
        "### Why tokenization matters\n",
        "\n",
        "- **Vocabulary mapping**: Every word or sub‑word becomes a unique integer. This is the model’s alphabet.\n",
        "- **Fixed‑length sequences**: Models expect tensors of a certain shape; tokenization pads or truncates to that shape.\n",
        "- **Efficiency**: Tokenizers are highly optimized (e.g., byte‑pair encoding) to reduce the number of tokens per sentence, saving memory and compute.\n",
        "\n",
        "### Why sharding matters\n",
        "\n",
        "- **Parallelism**: Each shard can be processed by a different GPU or worker, speeding up data loading.\n",
        "- **Memory safety**: Loading the entire dataset into RAM can explode memory usage; sharding keeps each worker’s memory footprint small.\n",
        "- **Fault tolerance**: If one shard fails to load, you can retry without re‑processing the whole dataset.\n",
        "\n",
        "### Extra Explanatory Paragraph – Key Terms & Rationale\n",
        "\n",
        "- **Tokenizer**: A deterministic mapping from raw text to token IDs. In Hugging Face, `AutoTokenizer` loads a pre‑trained tokenizer that matches the model’s vocabulary.\n",
        "- **Dataset**: A collection of examples (e.g., text passages). The `datasets` library provides lazy loading, caching, and efficient shuffling.\n",
        "- **Sharding**: Splitting a dataset into `n` parts (shards). Each shard is processed independently, enabling distributed training.\n",
        "- **Batch size**: Number of examples processed in one forward pass. Larger batches improve GPU utilization but increase memory usage.\n",
        "- **Sequence length**: Maximum number of tokens per example. Longer sequences capture more context but require more memory.\n",
        "- **Precision**: The bit‑width used to store tensors (fp16, bf16, int8). Lower precision reduces memory and can speed up inference, but may hurt accuracy.\n",
        "- **Trade‑offs**: Tokenization speed vs. token count (e.g., using a larger vocabulary reduces token count but increases lookup time). Sharding granularity vs. overhead (too many small shards increase I/O overhead; too few large shards risk memory spikes).\n",
        "\n",
        "### Practical Workflow\n",
        "\n",
        "1. **Load the raw dataset** (e.g., from Hugging Face Hub or a local CSV). The `datasets` library lazily loads data, so you can start processing without waiting for the entire file to be read.\n",
        "2. **Instantiate the tokenizer** that matches `gpt-oss-20b`. We’ll use `AutoTokenizer` with `use_fast=True` for speed.\n",
        "3. **Define a tokenization function** that maps each example to a dictionary of token IDs, attention masks, and optionally labels.\n",
        "4. **Apply the function in batched mode** to leverage vectorized tokenization.\n",
        "5. **Shard the dataset** using `datasets.Dataset.shard(num_shards, index)` or by splitting into a list of smaller datasets. Each shard will be saved to disk for later loading by the training script.\n",
        "6. **Cache the tokenized shards** to avoid re‑tokenizing on every run.\n",
        "\n",
        "Below is a minimal, reproducible example that demonstrates these steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tokenization & Sharding Demo\n",
        "# ------------------------------------------------------------\n",
        "# Requires: datasets>=2.16, transformers>=4.40, torch\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1️⃣ Set deterministic seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# 2️⃣ Load a small public dataset (replace with your own corpus)\n",
        "#    We use the \"wikitext-2\" dataset for demonstration.\n",
        "raw_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "print(f\"Loaded {len(raw_ds)} raw examples\")\n",
        "\n",
        "# 3️⃣ Instantiate the tokenizer for GPT‑Oss‑20B\n",
        "#    The tokenizer is fast (C++ implementation) and matches the model vocab.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\", use_fast=True)\n",
        "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# 4️⃣ Define a batched tokenization function\n",
        "MAX_LENGTH = 512  # truncate/pad to 512 tokens\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    \"\"\"Tokenize a batch of texts.\n",
        "    Returns a dict with input_ids and attention_mask.\n",
        "    \"\"\"\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
        "\n",
        "# 5️⃣ Apply tokenization in batched mode (batch_size=64)\n",
        "print(\"Tokenizing…\")\n",
        "tokenized_ds = raw_ds.map(tokenize_batch, batched=True, batch_size=64, remove_columns=[\"text\"], num_proc=4)\n",
        "print(\"Tokenization complete.\")\n",
        "\n",
        "# 6️⃣ Shard the dataset into 4 parts (you can increase this for more GPUs)\n",
        "NUM_SHARDS = 4\n",
        "shards = []\n",
        "for i in range(NUM_SHARDS):\n",
        "    shard = tokenized_ds.shard(num_shards=NUM_SHARDS, index=i)\n",
        "    shards.append(shard)\n",
        "    shard_path = f\"./shard_{i}.arrow\"\n",
        "    shard.save_to_disk(shard_path)\n",
        "    print(f\"Shard {i} saved to {shard_path} (size: {len(shard)} examples)\")\n",
        "\n",
        "# 7️⃣ Optional: verify that loading a shard works\n",
        "sample = shards[0][0]\n",
        "print(\"Sample token IDs:\", sample[\"input_ids\"][:10])\n",
        "print(\"Attention mask:\", sample[\"attention_mask\"][:10])\n",
        "\n",
        "print(\"\\nTokenization & sharding demo finished.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Distributed Fine‑Tuning with Accelerate & DeepSpeed\n",
        "\n",
        "Fine‑tuning a 20‑billion‑parameter model is a lot like training a giant orchestra. Each instrument (GPU) must play in sync, and the conductor (your training script) has to keep everyone on tempo while making sure the sheet music (model weights) fits on the stage (GPU memory). Hugging Face **Accelerate** gives you the baton to orchestrate the training across multiple GPUs, while **DeepSpeed** provides the backstage crew that shuffles the sheet music so no single instrument gets overloaded.\n",
        "\n",
        "### Why we need both\n",
        "- **Accelerate** abstracts away the boilerplate of setting up distributed data‑parallel training, handling device placement, and automatically picking the right launch command (`torchrun`, `accelerate launch`, etc.).\n",
        "- **DeepSpeed** adds *ZeRO* optimizations that partition optimizer states, gradients, and parameters across GPUs, letting you train with batch sizes that would otherwise explode memory.\n",
        "\n",
        "Together they let you:\n",
        "1. **Scale** to dozens of GPUs without writing custom DDP code.\n",
        "2. **Reduce** memory footprint by up to 10× with ZeRO‑3.\n",
        "3. **Speed** up training by keeping all GPUs busy.\n",
        "\n",
        "### Extra Explanatory Paragraph – Key Terms & Rationale\n",
        "- **Distributed Data‑Parallel (DDP)**: Each GPU holds a copy of the model and processes a different mini‑batch. Gradients are averaged across GPUs to keep the models synchronized.\n",
        "- **ZeRO‑3**: DeepSpeed’s memory‑efficient optimizer that shards *parameters*, *gradients*, and *optimizer states* across GPUs, dramatically lowering per‑GPU memory usage.\n",
        "- **Mixed‑Precision (fp16/bf16)**: Using 16‑bit floats instead of 32‑bit reduces memory and bandwidth, but requires careful loss‑scaling to avoid underflow.\n",
        "- **Gradient Accumulation**: Accumulating gradients over several forward passes before an optimizer step lets you simulate a larger batch size without increasing peak memory.\n",
        "- **Learning Rate Scheduler**: Adjusts the learning rate during training (e.g., linear warm‑up + cosine decay) to improve convergence.\n",
        "- **Seed**: Setting `torch.manual_seed(42)` and `random.seed(42)` ensures that weight initialization and data shuffling are reproducible.\n",
        "\n",
        "The trade‑off is that more complex setups (DDP + ZeRO) add a learning curve, but the payoff is the ability to fine‑tune massive models on commodity multi‑GPU nodes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Accelerate + DeepSpeed configuration\n",
        "# ------------------------------------------------------------\n",
        "# This cell creates a minimal accelerate config file and a DeepSpeed JSON.\n",
        "# Run it once before launching training.\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create accelerate config\n",
        "acc_cfg = {\n",
        "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
        "    \"deepspeed_config\": \"ds_config.json\",\n",
        "    \"distributed_type\": \"DEEPSPEED\",\n",
        "    \"fp16\": {\n",
        "        \"enabled\": True\n",
        "    },\n",
        "    \"zero_stage\": 3,\n",
        "    \"zero_allow_untested_optimizer\": True,\n",
        "    \"zero_reduce_scatter\": True,\n",
        "    \"zero_reduce_bucket_size\": 5e8,\n",
        "    \"zero_contiguous_gradients\": True\n",
        "}\n",
        "\n",
        "with open(\"accelerate_config.yaml\", \"w\") as f:\n",
        "    f.write(\"compute_environment: LOCAL_MACHINE\\n\")\n",
        "    f.write(\"deepspeed_config: ds_config.json\\n\")\n",
        "    f.write(\"distributed_type: DEEPSPEED\\n\")\n",
        "    f.write(\"fp16:\\n  enabled: true\\n\")\n",
        "    f.write(\"zero_stage: 3\\n\")\n",
        "    f.write(\"zero_allow_untested_optimizer: true\\n\")\n",
        "    f.write(\"zero_reduce_scatter: true\\n\")\n",
        "    f.write(\"zero_reduce_bucket_size: 500000000\\n\")\n",
        "    f.write(\"zero_contiguous_gradients: true\\n\")\n",
        "\n",
        "# DeepSpeed JSON (only a few knobs for demo)\n",
        "# In practice you might tweak batch size, optimizer, etc.\n",
        "\n",
        "ds_cfg = {\n",
        "    \"train_batch_size\": 8,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"fp16\": {\"enabled\": True},\n",
        "    \"zero_optimization\": {\"stage\": 3},\n",
        "    \"optimizer\": {\"type\": \"AdamW\", \"params\": {\"lr\": 5e-5, \"betas\": [0.9, 0.999], \"eps\": 1e-8}},\n",
        "    \"scheduler\": {\"type\": \"WarmupLR\", \"params\": {\"warmup_minibatches\": 100, \"warmup_fraction\": 0.05}}\n",
        "}\n",
        "\n",
        "with open(\"ds_config.json\", \"w\") as f:\n",
        "    json.dump(ds_cfg, f, indent=2)\n",
        "\n",
        "print(\"Accelerate and DeepSpeed configs written to disk.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2️⃣ Minimal training script using Accelerate & DeepSpeed\n",
        "# ------------------------------------------------------------\n",
        "# Save this as train.py and run with:\n",
        "# accelerate launch train.py\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# 1️⃣ Load dataset (already tokenized & sharded in Step 3)\n",
        "#    For demo we load a small shard; replace with full path.\n",
        "shard_path = os.getenv(\"SHARD_PATH\", \"./shard_0.arrow\")\n",
        "dataset = load_dataset(\"arrow\", data_files=shard_path, split=\"train\")\n",
        "\n",
        "# 2️⃣ Load tokenizer & model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-oss-20b\", use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt-oss-20b\", torch_dtype=torch.float16)\n",
        "\n",
        "# 3️⃣ Prepare Accelerator (will read accelerate_config.yaml)\n",
        "accelerator = Accelerator()\n",
        "model, dataset = accelerator.prepare(model, dataset)\n",
        "\n",
        "# 4️⃣ Define training arguments (will be overridden by DeepSpeed config)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=1,  # placeholder; DeepSpeed overrides\n",
        "    gradient_accumulation_steps=1,\n",
        "    fp16=True,\n",
        "    deepspeed=\"ds_config.json\",\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    evaluation_strategy=\"no\",\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=1,\n",
        ")\n",
        "\n",
        "# 5️⃣ Trainer wrapper\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# 6️⃣ Train\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training finished. Check ./results for checkpoints.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Model Optimization – Quantization & Pruning\n",
        "\n",
        "After fine‑tuning, the 20‑B model still feels like a heavy backpack: it’s powerful but slow and memory‑hungry. Think of it as a high‑end sports car that can accelerate fast but consumes a lot of fuel. Quantization and pruning are two techniques that trim that car’s weight without sacrificing too much performance.\n",
        "\n",
        "### Quantization: Turning 32‑bit floats into 8‑bit integers\n",
        "- **What it does**: Replaces 32‑bit floating‑point weights with 8‑bit integers, shrinking the model size by ~4×.\n",
        "- **Why it works**: Neural networks are surprisingly tolerant to reduced precision; the small rounding errors rarely hurt accuracy.\n",
        "- **When to use**: On GPUs that support INT8 kernels (e.g., NVIDIA Ampere+), or when deploying to edge devices with limited memory.\n",
        "\n",
        "### Pruning: Cutting the dead weight\n",
        "- **What it does**: Zeroes out a fraction of the model’s weights (often the smallest magnitude ones), effectively removing unnecessary connections.\n",
        "- **Why it works**: Many weights in large language models are redundant; pruning removes them while keeping the overall function intact.\n",
        "- **When to use**: After fine‑tuning, when you want a leaner model for inference or to fit into a smaller GPU.\n",
        "\n",
        "### Extra Explanatory Paragraph – Key Terms & Rationale\n",
        "- **Bit‑width**: The number of bits used to represent each weight (e.g., 32‑bit float, 8‑bit int). Lower bit‑width reduces memory and bandwidth.\n",
        "- **Quantization‑aware training (QAT)**: Fine‑tuning the model while simulating low‑precision arithmetic to mitigate accuracy loss.\n",
        "- **Post‑training quantization (PTQ)**: Applying quantization after training; faster but can lead to a larger accuracy drop.\n",
        "- **Structured pruning**: Removing entire neurons or attention heads; easier to accelerate on hardware.\n",
        "- **Unstructured pruning**: Zeroing individual weights; can reduce model size but may not speed up inference unless the hardware supports sparse kernels.\n",
        "- **Trade‑offs**: Quantization saves memory and can speed up inference, but may increase latency on CPUs that lack fast INT8 ops. Pruning reduces model size but can degrade accuracy if too aggressive; structured pruning is hardware‑friendly but may remove useful capacity.\n",
        "\n",
        "By combining quantization and pruning, we can often achieve a 3–5× reduction in memory footprint with <1–2 % loss in perplexity on standard benchmarks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Post‑Training 8‑bit Quantization with bitsandbytes\n",
        "# ------------------------------------------------------------\n",
        "# Requires: bitsandbytes>=0.43, transformers>=4.40, torch>=2.0\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load the fine‑tuned model (replace with your checkpoint path)\n",
        "model_name = \"gpt-oss-20b\"\n",
        "print(f\"Loading {model_name} for quantization…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "\n",
        "# Convert to 8‑bit using bitsandbytes\n",
        "print(\"Converting weights to 8‑bit…\")\n",
        "model = bnb.nn.quantize(model, dtype=bnb.nn.int8)\n",
        "\n",
        "# Save the quantized model for later inference\n",
        "quant_path = \"gpt-oss-20b-quant8\"\n",
        "model.save_pretrained(quant_path)\n",
        "print(f\"Quantized model saved to {quant_path}\")\n",
        "\n",
        "# Quick inference test\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "prompt = \"The future of AI is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(\"Generated text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2️⃣ Structured Pruning of Attention Heads\n",
        "# ------------------------------------------------------------\n",
        "# Requires: torch>=2.0, transformers>=4.40\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load the quantized model (or the original if you prefer)\n",
        "model_path = \"gpt-oss-20b-quant8\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
        "\n",
        "# Function to prune a percentage of attention heads in each layer\n",
        "prune_ratio = 0.2  # remove 20% of heads\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.MultiheadAttention):\n",
        "        # Each head corresponds to a slice of the weight matrix\n",
        "        num_heads = module.num_heads\n",
        "        heads_to_keep = int(num_heads * (1 - prune_ratio))\n",
        "        # Create a mask that keeps the first `heads_to_keep` heads\n",
        "        mask = torch.ones(num_heads, dtype=torch.bool)\n",
        "        mask[heads_to_keep:] = False\n",
        "        # Apply the mask to the key, value, and output projections\n",
        "        for proj in [module.in_proj_weight, module.out_proj.weight]:\n",
        "            proj.data[mask.view(-1, 1).repeat(1, proj.shape[1]) == False] = 0\n",
        "\n",
        "print(f\"Pruned {prune_ratio*100}% of attention heads across all layers.\")\n",
        "\n",
        "# Save the pruned model\n",
        "pruned_path = \"gpt-oss-20b-quant8-pruned\"\n",
        "model.save_pretrained(pruned_path)\n",
        "print(f\"Pruned model saved to {pruned_path}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Inference Benchmarking – Latency & Throughput\n",
        "\n",
        "After fine‑tuning and optimizing, the real test is how fast the model can answer questions in a production setting. Think of the model as a chef in a busy kitchen: **latency** is the time it takes to finish a single dish, while **throughput** is how many dishes the chef can serve per minute. In a web service, low latency keeps users happy, and high throughput keeps the system cost‑effective.\n",
        "\n",
        "### Why benchmark?\n",
        "\n",
        "1. **Validate optimizations** – Quantization, pruning, and model parallelism should translate into measurable speed‑ups.\n",
        "2. **Set realistic SLAs** – Knowing the latency distribution helps design request queues and autoscaling policies.\n",
        "3. **Detect regressions** – A new training run or a library update can silently degrade performance; benchmarking catches that early.\n",
        "\n",
        "### What we’ll measure\n",
        "\n",
        "| Metric | What it means | Typical target for GPT‑Oss‑20B on a single A100 |\n",
        "|--------|---------------|----------------------------------------------|\n",
        "| **Latency** | Time to generate a single response (ms) | 80–120 ms (fp16) |\n",
        "| **Throughput** | Tokens per second (tps) | 200–300 tps (fp16) |\n",
        "| **Peak GPU memory** | Max memory used during inference | 30 GB (fp16) |\n",
        "| **CPU usage** | Optional, for edge deployments | < 30 % |\n",
        "\n",
        "### Extra Explanatory Paragraph – Key Terms & Rationale\n",
        "- **Latency**: The wall‑clock time from sending a prompt to receiving the first token. It matters for interactive applications.\n",
        "- **Throughput**: The number of tokens processed per second, averaged over many requests. It drives cost‑efficiency in batch workloads.\n",
        "- **Batch size**: In inference, we often use a batch of 1 for low latency, but larger batches can boost throughput on GPUs with enough memory.\n",
        "- **Precision (fp16, bf16, int8)**: Lower precision reduces memory bandwidth and can accelerate kernels, but may increase latency if the GPU lacks efficient kernels.\n",
        "- **Model parallelism**: Splitting a single model across GPUs. It can reduce per‑GPU memory but introduces inter‑GPU communication overhead that can hurt latency.\n",
        "- **Trade‑offs**: Optimizing for latency often means smaller batches and less parallelism, while optimizing for throughput may sacrifice interactivity. The right balance depends on the deployment scenario.\n",
        "\n",
        "### Practical Workflow\n",
        "1. **Load the model** – Use the same checkpoint you used for training (e.g., the quantized & pruned version).\n",
        "2. **Warm‑up** – Run a few dummy inferences to let the GPU cache the kernels.\n",
        "3. **Measure latency** – Time a single request with `torch.cuda.synchronize()` before and after.\n",
        "4. **Measure throughput** – Run a loop of `N` requests and compute tokens per second.\n",
        "5. **Record memory** – Use `torch.cuda.max_memory_allocated()` to capture peak usage.\n",
        "6. **Repeat for each precision** – Compare fp16, bf16, and int8 to see the impact.\n",
        "\n",
        "Below is a concise, reproducible script that performs these steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference benchmarking for GPT‑Oss‑20B\n",
        "# ------------------------------------------------------------\n",
        "# Requires: torch>=2.0, transformers>=4.40, bitsandbytes (for int8)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# ---------- Configuration -----------------------------------\n",
        "MODEL_NAME = os.getenv(\"MODEL_PATH\", \"gpt-oss-20b\")  # path to checkpoint\n",
        "PROMPT = \"The future of AI is\"\n",
        "MAX_NEW_TOKENS = 20\n",
        "BATCH_SIZE = 1  # keep 1 for latency; increase for throughput\n",
        "NUM_RUNS = 50   # number of inference runs for throughput\n",
        "SEED = 42\n",
        "\n",
        "# ---------- Reproducibility ---------------------------------\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ---------- Load tokenizer & model --------------------------\n",
        "print(f\"Loading tokenizer and model from {MODEL_NAME}…\")\n",
        "# Use fp16 by default; switch to bf16 or int8 by changing torch_dtype\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",  # automatically place on GPU(s)\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# ---------- Warm‑up ----------------------------------------\n",
        "print(\"Warming up…\")\n",
        "inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    _ = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
        "\n",
        "# ---------- Latency measurement -----------------------------\n",
        "print(\"Measuring latency…\")\n",
        "latencies = []\n",
        "for _ in range(10):\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        _ = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
        "    torch.cuda.synchronize()\n",
        "    latencies.append((time.perf_counter() - start) * 1000)  # ms\n",
        "print(f\"Avg latency: {sum(latencies)/len(latencies):.2f} ms\")\n",
        "\n",
        "# ---------- Throughput measurement --------------------------\n",
        "print(\"Measuring throughput…\")\n",
        "start = time.perf_counter()\n",
        "for _ in range(NUM_RUNS):\n",
        "    with torch.no_grad():\n",
        "        _ = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
        "end = time.perf_counter()\n",
        "# Tokens per second = (runs * tokens per run) / elapsed\n",
        "throughput = (NUM_RUNS * MAX_NEW_TOKENS) / (end - start)\n",
        "print(f\"Throughput: {throughput:.1f} tokens/s\")\n",
        "\n",
        "# ---------- Memory usage -----------------------------------\n",
        "peak_mem = torch.cuda.max_memory_allocated(model.device) / (1024 ** 3)\n",
        "print(f\"Peak GPU memory: {peak_mem:.2f} GB\")\n",
        "\n",
        "print(\"\\nBenchmarking complete.\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following best describes the primary benefit of using DeepSpeed ZeRO‑3 during fine‑tuning?\", [\"Reduces GPU memory usage by partitioning optimizer states\",\"Increases model accuracy by adding regularization\",\"Speeds up inference by pruning attention heads\",\"Enables mixed‑precision training without any configuration\"], 0, \"ZeRO‑3 partitions optimizer states, gradients, and parameters across GPUs, dramatically lowering memory footprint and allowing larger batch sizes.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the main trade‑off when applying 8‑bit quantization to GPT‑Oss‑20B?\", [\"Higher inference latency\",\"Reduced model size but potential accuracy drop\",\"Increased GPU memory consumption\",\"Elimination of the need for a GPU\"], 1, \"8‑bit quantization compresses weights, reducing memory and bandwidth usage, but can introduce a small accuracy loss that must be evaluated.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:53:25.308Z",
      "title": "Fine‑Tuning and Deploying GPT‑Oss‑20B: Advanced Techniques for Research and Production",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T02:53:25.316Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}