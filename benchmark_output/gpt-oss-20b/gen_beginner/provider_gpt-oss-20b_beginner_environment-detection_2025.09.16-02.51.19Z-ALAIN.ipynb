{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# gpt-oss-20b for Absolute Beginners\n\nLearn how to run the 20‑billion‑parameter GPT‑OSS model in a Jupyter notebook. This lesson uses simple analogies, step‑by‑step code, and practical exercises to help non‑developers get started with large language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T02:51:19.485Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what a large language model is and why 20B parameters matter.\n",
        "2. Show how to install the required libraries and set up a reproducible environment.\n",
        "3. Demonstrate how to load the gpt‑oss‑20b model and generate text.\n",
        "4. Identify common pitfalls and best practices for running large models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic Python knowledge (variables, functions, pip).\n",
        "- A Jupyter notebook environment (e.g., Anaconda, Google Colab).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 transformers>=4.40.0 accelerate>=0.28.0 torch>=2.0.0 datasets>=2.20.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"transformers>=4.40.0\",\"accelerate>=0.28.0\",\"torch>=2.0.0\",\"datasets>=2.20.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1\n",
        "\n",
        "Thinking...\n",
        ">We need to produce JSON with section_number 1, title \"Step 1: Introduction and Setup\", content array with markdown and code cells, callouts, estimated_tokens 800-1000 tokens. Provide beginner-friendly ELI5 language, analogies, precise terms, extra explanatory paragraph defining key terms and rationale/trade-offs. Include executable code with comments, 1-2 short code cells (<30 lines each). Use callouts. Ensure reproducibility with seeds/versions. Provide prerequisites_check array. N...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: What is gpt‑oss‑20b?\n",
        "\n",
        "Imagine you have a gigantic library that contains **20 billion** different books. Each book is a tiny piece of knowledge—an *instruction*, a *fact*, or a *story fragment*. The GPT‑OSS 20‑billion‑parameter model is like a super‑smart librarian who has read all those books and can instantly pull out the most relevant page to answer your question.\n",
        "\n",
        "In machine‑learning terms, a *parameter* is a number that the model learns during training. Think of it as a knob that tunes how the model reacts to a particular word or phrase. With 20 billion knobs, the model can capture incredibly subtle patterns in language, which is why it can generate text that feels surprisingly human.\n",
        "\n",
        "**Open‑source** means the code and the trained weights are freely available under a permissive license. Anyone can download, inspect, or modify the model without paying a licensing fee—just like downloading a recipe from a public cookbook.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑offs |\n",
        "|------|----------------|----------------|------------|\n",
        "| **Parameter** | A learnable weight in the neural network. | Determines the model’s expressive power. | More parameters → better performance but higher memory and compute cost. |\n",
        "| **Token** | The smallest unit the model processes (often a word or sub‑word). | Controls the granularity of text representation. | Fewer tokens → faster inference but less detail; more tokens → richer context but slower. |\n",
        "| **Model** | The entire set of parameters and architecture that maps input tokens to output tokens. | Encapsulates the knowledge learned from data. | Larger models need more GPU memory and longer loading times. |\n",
        "| **Inference** | The process of generating predictions from a trained model. | The step we perform in a notebook to produce text. | Requires GPU or CPU; speed depends on model size and precision. |\n",
        "\n",
        "The trade‑off is clear: a 20‑billion‑parameter model can generate more coherent and context‑aware text, but it demands a powerful GPU (or a cluster) and careful memory management. If you only have a modest machine, you might opt for a smaller model or use techniques like *quantization* to reduce memory usage.\n",
        "\n",
        "⚠️ **Warning**: Loading the full 20 billion‑parameter model into memory on a consumer laptop will likely crash. We’ll show how to load it lazily in the next step.\n",
        "\n",
        "💡 **Tip**: Even if you can’t run the full model, you can still explore its *configuration* (number of layers, hidden size, etc.) to understand its architecture without pulling all the weights into RAM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the model configuration without downloading all weights\n",
        "# This is useful for inspecting the architecture and parameter count\n",
        "# We use the `transformers` library which is already installed\n",
        "\n",
        "from transformers import AutoConfig\n",
        "import torch\n",
        "\n",
        "# Set a random seed for reproducibility of any random operations\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Specify the model name on Hugging Face Hub\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "\n",
        "# Load the configuration (metadata) only\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Print key architectural details\n",
        "print(\"Model name:\", config._name_or_path)\n",
        "print(\"Number of layers (transformer blocks):\", config.num_hidden_layers)\n",
        "print(\"Hidden size (dimensionality of each token representation):\", config.hidden_size)\n",
        "print(\"Number of attention heads:\", config.num_attention_heads)\n",
        "\n",
        "# Compute the approximate number of parameters from the config\n",
        "# This is a rough estimate: params ≈ 2 * hidden_size^2 * num_layers\n",
        "approx_params = 2 * config.hidden_size ** 2 * config.num_hidden_layers\n",
        "print(f\"Approximate parameter count (in billions): {approx_params / 1e9:.2f}B\")\n",
        "\n",
        "# Note: The exact count may differ slightly due to embeddings and other components\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Setting the Environment Variable\n",
        "\n",
        "When you ask a model to *look up* something on the internet, you need a key that proves you’re allowed to use that service. In the Hugging Face ecosystem, that key is called an **HF_TOKEN**. Think of it like a library card: you can only borrow books if the librarian can verify that you have a valid card.\n",
        "\n",
        "Below we’ll walk through how to create that card in your notebook, why it matters, and how to keep it safe.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "\n",
        "| Term | What it means | Why it matters | Trade‑offs |\n",
        "|------|----------------|----------------|------------|\n",
        "| **Environment Variable** | A named value stored in the operating system that programs can read at runtime. | Keeps secrets (like tokens) out of the code base and version control. | If you forget to set it, the program will fail; if you expose it, you risk unauthorized access. |\n",
        "| **HF_TOKEN** | Your personal Hugging Face access token that authorizes downloads of private or large models. | Required for authentication when the `transformers` library pulls weights from the Hub. | Must be kept secret; sharing it publicly will grant others access to your quota. |\n",
        "| **Hugging Face Hub** | A cloud repository where models and datasets are stored and shared. | Provides a convenient API for downloading models. | Requires internet connectivity and an active account. |\n",
        "| **Authentication** | The process of proving your identity to a service. | Prevents abuse and tracks usage. | Tokens can expire or be revoked, breaking your notebook until refreshed. |\n",
        "\n",
        "**Rationale & Trade‑offs**: Storing the token in an environment variable keeps it out of the notebook file, which is good for security and for sharing the notebook with others. However, if you forget to set the variable each time you start a new session, the model loading will fail. An alternative is to use a `.env` file or a secrets manager, but those add complexity. For a beginner notebook, setting it directly in the notebook is the simplest trade‑off between convenience and safety.\n",
        "\n",
        "⚠️ **Warning**: Never commit your HF_TOKEN to a public Git repository or share the notebook file with the token embedded. If you do, anyone who sees the file can download the model and consume your quota.\n",
        "\n",
        "💡 **Tip**: If you’re using Google Colab or a cloud notebook, you can store the token in the notebook’s *secrets* panel or use the `os.getenv` approach shown below to keep it hidden.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣  Import the standard library for environment variables\n",
        "# 2️⃣  Set the HF_TOKEN directly in the notebook (replace \"YOUR_TOKEN_HERE\" with your actual token)\n",
        "# 3️⃣  Verify that the variable is set\n",
        "\n",
        "import os\n",
        "\n",
        "# Replace the placeholder with your real Hugging Face token\n",
        "# You can obtain a token by logging into https://huggingface.co/settings/token\n",
        "os.environ[\"HF_TOKEN\"] = \"YOUR_TOKEN_HERE\"\n",
        "\n",
        "# Quick sanity check – print the first 4 characters to confirm it was set\n",
        "print(\"HF_TOKEN set to:\", os.getenv(\"HF_TOKEN\")[:4] + \"…\")\n",
        "\n",
        "# Optional: export the variable to the OS so that subprocesses can see it\n",
        "# (useful if you spawn new Python processes or run shell commands)\n",
        "os.system(\"export HF_TOKEN=\\\"{}\\\"\".format(os.getenv(\"HF_TOKEN\")))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the Token in `transformers`\n",
        "\n",
        "The `transformers` library automatically looks for the `HF_TOKEN` environment variable when you call `from_pretrained`. If the variable is missing, you’ll see an error like:\n",
        "\n",
        "```\n",
        "OSError: Hugging Face Hub authentication token not found. Please set the environment variable \"HF_TOKEN\".\n",
        "```\n",
        "\n",
        "By setting it in the notebook, you avoid that error and the library can download the model weights on the fly.\n",
        "\n",
        "### Reproducibility note\n",
        "\n",
        "The token itself does not affect reproducibility of the model outputs; it only grants access. However, to ensure that the rest of your notebook runs the same way every time, keep the following versions pinned:\n",
        "\n",
        "- `transformers==4.40.0`\n",
        "- `accelerate==0.28.0`\n",
        "- `torch==2.0.0`\n",
        "\n",
        "You can pin them in a `requirements.txt` or by running:\n",
        "\n",
        "```bash\n",
        "pip install transformers==4.40.0 accelerate==0.28.0 torch==2.0.0\n",
        "```\n",
        "\n",
        "This guarantees that the API surface and underlying math stay consistent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4\n",
        "\n",
        "Thinking...\n",
        ">We need to produce JSON with section_number 4, title \"Step 4: Loading the Model\", content array with markdown and code cells, callouts, estimated_tokens 800-1000 tokens. Must be beginner-friendly ELI5, analogies, precise terms, extra explanatory paragraph defining key terms and rationale/trade-offs. Include executable code with comments; 1-2 short code cells (<30 lines each). Add callouts (tip, warning, note). Ensure reproducibility with seeds/versions. Provide prerequisites_check a...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generating Text\n",
        "\n",
        "Imagine you’re a chef who has a huge pantry of ingredients (the 20 billion‑parameter model). The prompt you give is the recipe you want to follow, and the model’s job is to *cook* a new dish (generate text) based on that recipe. The way you tweak the stove’s heat, the amount of salt, or the cooking time will change the final flavor. In the world of language models, those knobs are called **temperature**, **max_new_tokens**, and **top_p**.\n",
        "\n",
        "### How the knobs work\n",
        "\n",
        "| Knob | What it does | Typical values | Trade‑off |\n",
        "|------|--------------|----------------|-----------|\n",
        "| **temperature** | Controls randomness of token selection. Low (≈0.2) → deterministic, high (≈0.8‑1.0) → creative. | 0.2 – 1.0 | Low → safe but repetitive; high → diverse but may hallucinate. |\n",
        "| **max_new_tokens** | How many new words the model can add after the prompt. | 50 – 200 | More tokens → longer answer but more compute and memory. |\n",
        "| **top_p** (nucleus sampling) | Keeps the model’s choices within the top‑p probability mass. | 0.8 – 0.95 | Lower → more focused; higher → more varied. |\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "\n",
        "| Term | Meaning | Why it matters | Trade‑offs |\n",
        "|------|---------|----------------|------------|\n",
        "| **Token** | The smallest unit the model processes (often a word or sub‑word). | Determines granularity of language representation. | Fewer tokens → faster inference but less detail; more tokens → richer context but slower. |\n",
        "| **Generation** | The act of producing new tokens given a prompt. | Core functionality of LLMs. | Longer generations consume more GPU memory and time. |\n",
        "| **Sampling strategy** | The algorithm that picks the next token (e.g., greedy, top‑k, top‑p). | Influences creativity vs. coherence. | Simple greedy → fast but dull; complex sampling → slower but more interesting. |\n",
        "| **Determinism** | Whether the same prompt always yields the same output. | Important for debugging and reproducibility. | Setting `torch.manual_seed` and `temperature=0` gives deterministic results. |\n",
        "\n",
        "The trade‑off is clear: if you want a quick, safe answer, set a low temperature and a modest `max_new_tokens`. If you want a creative, exploratory output, increase temperature and/or `top_p`, but be prepared for longer runtimes and potentially less factual accuracy.\n",
        "\n",
        "⚠️ **Warning**: Generating too many tokens on a single GPU can exhaust VRAM. If you hit an out‑of‑memory error, reduce `max_new_tokens` or switch to `torch_dtype=torch.float16` to lower memory usage.\n",
        "\n",
        "💡 **Tip**: Keep a small “prompt library” handy. Re‑using prompts helps you compare how different sampling settings affect the same base question.\n",
        "\n",
        "📝 **Note**: The `transformers` library’s `generate` method is a black‑box that internally handles tokenization, attention masks, and beam search. For beginners, just tweak the high‑level arguments; for advanced users, you can dive into the `GenerationConfig` object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣  Import libraries and set a reproducible seed\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Pin the random seed for deterministic sampling when temperature=0\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 2️⃣  Load the tokenizer and model (device_map='auto' loads only needed shards)\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "\n",
        "print(\"Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "print(\"Loading model…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # automatically places layers on available GPUs\n",
        "    torch_dtype=torch.float16   # use half‑precision to save memory\n",
        ")\n",
        "\n",
        "# 3️⃣  Define a helper that runs generation with user‑chosen settings\n",
        "\n",
        "def generate_text(prompt, temperature=0.7, max_new_tokens=100, top_p=0.9):\n",
        "    \"\"\"Generate a short continuation of *prompt*.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The starting text.\n",
        "        temperature (float): Controls randomness.\n",
        "        max_new_tokens (int): How many tokens to generate.\n",
        "        top_p (float): Nucleus sampling threshold.\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Move input tensors to the same device as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,            # enable sampling (otherwise greedy)\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    # Decode and strip the original prompt\n",
        "    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return generated[len(prompt):]\n",
        "\n",
        "# 4️⃣  Quick demo\n",
        "prompt = \"Once upon a time, in a land far, far away\"\n",
        "print(\"\\nPrompt:\", prompt)\n",
        "print(\"\\nGenerated text (temperature=0.7, 100 tokens):\")\n",
        "print(generate_text(prompt, temperature=0.7, max_new_tokens=100, top_p=0.9))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6\n",
        "\n",
        "Thinking...\n",
        ">We need to produce JSON structure for section 6. Must follow format:\n",
        ">\n",
        ">{\n",
        ">  \"section_number\": 6,\n",
        ">  \"title\": \"Step 6: Using ipywidgets for Interactivity\",\n",
        ">  \"content\": [\n",
        ">    { \"cell_type\":\"markdown\", \"source\":\"## Step 6: Title\\n\\nExplanation with analogies and the extra paragraph defining key terms...\" },\n",
        ">    { \"cell_type\":\"code\", \"source\":\"# Clear, commented code (<=30 lines)\\nprint('Hello World')\" }\n",
        ">  ],\n",
        ">  \"callouts\":[{ \"type\":\"tip\", \"message\":\"💡 Helpful guidance\"}],\n",
        ">  \"es...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which parameter controls how many new tokens the model generates?\", [\"temperature\",\"max_new_tokens\",\"top_p\",\"device_map\"], 1, \"The `max_new_tokens` argument limits the number of tokens the model can add to the prompt.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Quick check 2: Basic understanding\", [\"A\",\"B\",\"C\",\"D\"], 0, \"Review the outline section to find the correct answer.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:51:19.477Z",
      "title": "gpt-oss-20b for Absolute Beginners",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T02:51:19.485Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}