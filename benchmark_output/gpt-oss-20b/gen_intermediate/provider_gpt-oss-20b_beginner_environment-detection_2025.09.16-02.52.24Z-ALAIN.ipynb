{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying and Fine‑Tuning GPT‑Oss‑20B in Jupyter: A Practitioner’s Guide\n\nThis notebook walks experienced ML practitioners through the end‑to‑end process of loading the GPT‑Oss‑20B model, setting up a GPU‑accelerated environment, fine‑tuning on a custom dataset, and deploying a lightweight inference API. It balances hands‑on code with practical explanations, ensuring you can replicate the workflow on your own hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T02:52:24.638Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Understand the architecture and tokenization of GPT‑Oss‑20B.\n",
        "2. Set up a reproducible GPU environment with the required libraries.\n",
        "3. Fine‑tune GPT‑Oss‑20B on a domain‑specific text corpus.\n",
        "4. Deploy a minimal inference endpoint using FastAPI and ipywidgets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Python 3.10+ with GPU support (CUDA 11.8 or higher).\n",
        "- Basic familiarity with PyTorch, Hugging Face Transformers, and Jupyter notebooks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 torch>=2.0.0 transformers>=4.40.0 datasets>=2.18.0 accelerate>=0.28.0 fastapi>=0.110.0 uvicorn>=0.29.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"torch>=2.0.0\",\"transformers>=4.40.0\",\"datasets>=2.18.0\",\"accelerate>=0.28.0\",\"fastapi>=0.110.0\",\"uvicorn>=0.29.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Validation and GPU Check\n",
        "\n",
        "Before we can start playing with GPT‑Oss‑20B, we need to make sure the playground is ready. Think of your notebook as a kitchen: the GPU is the stove, the PyTorch library is the chef, and the model is the recipe. If the stove is off or the chef doesn’t know how to use it, the dish will never come out right.\n",
        "\n",
        "1. **Check that PyTorch is installed and can talk to CUDA** – this is like making sure the stove is plugged in and the chef has a recipe book.\n",
        "2. **Verify that at least one GPU is visible** – we need a burner to cook on. If there’s no GPU, we’ll fall back to the CPU, but training will be painfully slow.\n",
        "3. **Print out the CUDA version and GPU name** – this tells us which stove model we’re using and whether it’s compatible with the recipe.\n",
        "4. **Set a random seed** – this is the same as pre‑measuring all ingredients so that every time we cook the dish, it tastes exactly the same.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "\n",
        "- **CUDA** (Compute Unified Device Architecture) is NVIDIA’s programming framework that lets software run on the GPU. Think of it as the language the chef uses to instruct the stove.\n",
        "- **GPU** (Graphics Processing Unit) is a massively parallel processor that can handle many operations at once, ideal for the matrix‑heavy work of language models.\n",
        "- **torch** is the PyTorch library that provides tensors (multi‑dimensional arrays) and automatic differentiation. It’s the chef’s toolbox.\n",
        "- **Device** refers to the hardware context (CPU or GPU) where tensors live. Moving a tensor to a device is like moving ingredients to the stove.\n",
        "- **Seed** is a starting number for random number generators. Setting a seed ensures reproducibility – the same dish every time.\n",
        "- **Reproducibility** is crucial when fine‑tuning because we want to be able to compare experiments and debug issues reliably.\n",
        "\n",
        "By validating the environment now, we avoid costly surprises later when the model starts training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Environment and GPU validation\n",
        "# ------------------------------------------------------------\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# 1. Import torch and handle the case where it isn’t installed\n",
        "try:\n",
        "    import torch\n",
        "except ImportError as e:\n",
        "    print(\"❌ PyTorch is not installed. Please run:\n",
        "    pip install torch==2.0.0+cu118 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# 2. Set a global random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 3. Basic environment info\n",
        "print(f\"🧠 PyTorch version: {torch.__version__}\")\n",
        "print(f\"🛠️  CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"🧑‍💻  Number of GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "# 4. If a GPU is present, print its name and memory stats\n",
        "if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    gpu_name = torch.cuda.get_device_name(device)\n",
        "    total_mem = torch.cuda.get_device_properties(device).total_memory / (1024 ** 3)\n",
        "    print(f\"🔥 GPU 0: {gpu_name} ({total_mem:.2f} GB total memory)\")\n",
        "    # Show current memory usage (should be near zero at this point)\n",
        "    allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)\n",
        "    reserved = torch.cuda.memory_reserved(device) / (1024 ** 3)\n",
        "    print(f\"📦  Memory allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
        "else:\n",
        "    print(\"⚠️  No GPU detected. Training will run on CPU and may be very slow.\")\n",
        "\n",
        "# 5. Quick sanity check: create a small tensor on the chosen device\n",
        "x = torch.randn(3, 3, device=device if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✅ Tensor created on {x.device}:\\n{x}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Loading GPT‑Oss‑20B and Inspecting the Tokenizer\n",
        "\n",
        "In the previous step we made sure the kitchen (your notebook) was ready. Now we bring in the main ingredient: the GPT‑Oss‑20B model and its tokenizer. Think of the model as a gigantic recipe book with 20 billion words of experience, and the tokenizer as the translator that turns raw text into the book’s language.\n",
        "\n",
        "### Why load the model first?\n",
        "- **Memory layout**: Loading the weights into a `torch.nn.Module` places them on the CPU by default. We’ll later move them to the GPU for training.\n",
        "- **Configuration sanity**: The `config` object tells us the hidden size, number of layers, and attention heads—useful for debugging and for deciding how many GPUs we need.\n",
        "- **Tokenizer inspection**: Knowing the vocabulary size and special tokens lets us craft prompts that the model understands.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "\n",
        "- **GPT‑Oss‑20B**: A 20‑billion‑parameter transformer model from the GPT‑Oss family, built on the same architecture as GPT‑3 but open‑source. It uses a causal (autoregressive) attention mask, meaning it predicts the next token based on all previous tokens.\n",
        "- **Tokenizer**: GPT‑Oss‑20B uses a **Byte‑Pair Encoding (BPE)** tokenizer. BPE starts with a character‑level vocabulary and iteratively merges the most frequent pairs of tokens, producing sub‑word units that balance coverage and efficiency.\n",
        "- **Special tokens**: `bos_token` (begin‑of‑sentence), `eos_token` (end‑of‑sentence), `pad_token` (padding), and `unk_token` (unknown). These are placeholders that the model uses to structure sequences.\n",
        "- **Trade‑offs**: Loading the full model consumes ~80 GB of GPU memory on a single 24 GB GPU, so we often freeze most layers or use LoRA adapters. Inspecting the tokenizer early lets us decide whether to add a custom `pad_token` or adjust `max_length` for inference.\n",
        "\n",
        "By the end of this section you’ll have the model on the GPU, the tokenizer ready, and a quick sanity check that everything is wired correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Import libraries and set reproducibility flags\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Ensure deterministic behavior for reproducibility\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load the tokenizer\n",
        "# ------------------------------------------------------------\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(f\"🔍 Loading tokenizer for {MODEL_NAME}…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_fast=True,  # fast tokenizer uses Rust implementation\n",
        "    trust_remote_code=True,  # allow custom tokenizer code if needed\n",
        ")\n",
        "\n",
        "# Quick sanity checks\n",
        "print(f\"🗂️  Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"🔤  Special tokens: bos={tokenizer.bos_token}, eos={tokenizer.eos_token}, pad={tokenizer.pad_token}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Load the model (CPU first, then move to GPU if available)\n",
        "# ------------------------------------------------------------\n",
        "print(\"⚙️  Loading model… this may take a few minutes…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,  # use FP16 to save memory\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",  # let accelerate decide placement\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Show a few config values\n",
        "print(\"📐 Model configuration:\")\n",
        "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
        "print(f\"  Number of layers: {model.config.num_hidden_layers}\")\n",
        "print(f\"  Attention heads: {model.config.num_attention_heads}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Verify that the model is on GPU (if available)\n",
        "# ------------------------------------------------------------\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"🚀 Moving model to GPU ({torch.cuda.get_device_name(device)})…\")\n",
        "    model.to(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"⚠️  No GPU detected. Model remains on CPU – training will be slow.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Simple tokenization demo\n",
        "# ------------------------------------------------------------\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "encoded = tokenizer(sample_text, return_tensors=\"pt\")\n",
        "print(\"📝 Token IDs:\", encoded[\"input_ids\"][0][:10])\n",
        "print(\"🔎 Detokenized:\", tokenizer.decode(encoded[\"input_ids\"][0]))\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Preparing the Fine‑Tuning Dataset\n",
        "\n",
        "Fine‑tuning is like teaching a student a new language. The student (our model) already knows a lot of words, but we want them to speak a specific dialect (your domain). To do that, we need a *text corpus* that contains many examples of that dialect.\n",
        "\n",
        "1. **Choose or collect a dataset** – It can be a public Hugging Face dataset, a local CSV/JSON file, or a scraped web‑page collection. The key is that the text is representative of the prompts you’ll later give the model.\n",
        "2. **Clean and filter** – Remove non‑text artifacts, strip HTML tags, and optionally filter out very short or very long passages that might confuse the model.\n",
        "3. **Tokenize** – Convert raw text into the integer IDs that GPT‑Oss‑20B understands. We’ll use the same tokenizer we loaded in Step 2 so the vocabulary matches.\n",
        "4. **Create training examples** – For causal language modeling we usually split the token stream into chunks of a fixed `block_size` (e.g., 1024 tokens). Each chunk becomes a training sample where the model predicts the next token.\n",
        "5. **Split into train/validation** – A typical split is 90 % training, 10 % validation. The validation set lets us monitor over‑fitting during fine‑tuning.\n",
        "6. **Wrap with a DataCollator** – Because each chunk may be shorter than `block_size`, the collator pads them on the fly so the batch tensor is rectangular.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "\n",
        "- **Dataset**: In machine learning, a dataset is a collection of examples that the model learns from. For language models, each example is usually a piece of text.\n",
        "- **Tokenizer**: The tokenizer turns text into a sequence of integer IDs. GPT‑Oss‑20B uses a Byte‑Pair Encoding (BPE) tokenizer, which splits words into sub‑word units.\n",
        "- **Block size**: The maximum number of tokens per training example. Larger block sizes give the model more context but require more GPU memory.\n",
        "- **DataCollator**: A helper that batches examples together, handling padding and attention masks automatically.\n",
        "- **Trade‑offs**: A larger `block_size` improves context but increases memory usage. A smaller `block_size` is cheaper but may hurt performance on long‑range dependencies.\n",
        "- **Reproducibility**: Setting a random seed before shuffling ensures that the train/validation split is the same every run.\n",
        "\n",
        "By the end of this section you’ll have a `datasets.Dataset` object ready for training, with all the preprocessing steps applied.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Load and preprocess the dataset\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# 1️⃣  Choose a dataset – here we use the \"wikitext\" public dataset as an example\n",
        "# Replace \"wikitext\" with your own dataset path or Hugging Face ID\n",
        "DATASET_NAME = \"wikitext\"\n",
        "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
        "print(f\"📥 Loading dataset {DATASET_NAME}/{DATASET_CONFIG}…\")\n",
        "raw_datasets = load_dataset(DATASET_NAME, DATASET_CONFIG, split=\"train+validation\")\n",
        "\n",
        "# 2️⃣  Basic cleaning – drop rows that are too short or contain non‑text\n",
        "MIN_LENGTH = 50  # characters\n",
        "raw_datasets = raw_datasets.filter(lambda x: len(x[\"text\"]) >= MIN_LENGTH)\n",
        "print(f\"✅ {len(raw_datasets)} examples after filtering\")\n",
        "\n",
        "# 3️⃣  Tokenizer (same as in Step 2)\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(\"🔍 Loading tokenizer…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# 4️⃣  Tokenize and chunk into blocks\n",
        "BLOCK_SIZE = 1024\n",
        "\n",
        "def tokenize_and_chunk(example):\n",
        "    # Tokenize the raw text\n",
        "    tokens = tokenizer(example[\"text\"], add_special_tokens=False, truncation=False)\n",
        "    input_ids = tokens[\"input_ids\"]\n",
        "    # Split into blocks of BLOCK_SIZE\n",
        "    chunks = [input_ids[i : i + BLOCK_SIZE] for i in range(0, len(input_ids), BLOCK_SIZE)]\n",
        "    return {\"input_ids\": chunks}\n",
        "\n",
        "print(\"⚙️  Tokenizing and chunking… this may take a minute…\")\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_and_chunk,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"],\n",
        "    num_proc=4,\n",
        "    load_from_cache_file=True,\n",
        ")\n",
        "\n",
        "# 5️⃣  Flatten the list of chunks into individual examples\n",
        "flat_examples = []\n",
        "for chunk_list in tokenized_datasets[\"input_ids\"]:\n",
        "    for chunk in chunk_list:\n",
        "        flat_examples.append({\"input_ids\": chunk})\n",
        "\n",
        "train_val_dataset = Dataset.from_dict({\"input_ids\": [ex[\"input_ids\"] for ex in flat_examples]})\n",
        "print(f\"📦 Total training examples: {len(train_val_dataset)}\")\n",
        "\n",
        "# 6️⃣  Split into train/validation\n",
        "train_dataset, val_dataset = train_val_dataset.train_test_split(test_size=0.1, seed=SEED).values()\n",
        "print(f\"🟢 Train: {len(train_dataset)} | Validation: {len(val_dataset)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Create DataCollator and DataLoaders\n",
        "# ------------------------------------------------------------\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# DataCollator pads the input_ids to the longest sequence in the batch\n",
        "collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # causal LM – no masked language modeling\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 4  # adjust based on GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
        "\n",
        "print(f\"✅ DataLoaders created – batch size {BATCH_SIZE}\")\n",
        "\n",
        "# Quick sanity check: iterate one batch\n",
        "batch = next(iter(train_loader))\n",
        "print(\"Input shape:\", batch[\"input_ids\"].shape)\n",
        "print(\"Attention mask shape:\", batch[\"attention_mask\"].shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4\n",
        "\n",
        "Thinking...\n",
        ">We need to produce JSON with section_number 4, title \"Step 4: Configuring Accelerate for Multi‑GPU Training\". Content: markdown + code cells. Must be 800-1000 tokens. Use beginner-friendly ELI5 language, analogies, precise terms. Add extra explanatory paragraph defining key terms and rationale/trade-offs. Include code cells <=30 lines each. Add callouts. Provide estimated_tokens 1000. prerequisites_check: [\"item verified\"]? Should list prerequisites? In previous sections, prerequisi...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5\n",
        "\n",
        "Thinking...\n",
        ">We need to output JSON with section_number 5, title \"Step 5: Fine‑Tuning GPT‑Oss‑20B with LoRA\", content array with markdown and code cells, callouts, estimated_tokens 1000, prerequisites_check list, next_section_hint. Must follow guidelines: 800-1000 tokens, beginner-friendly ELI5, analogies, precise terms, extra explanatory paragraph defining key terms and rationale/trade-offs, code cells <=30 lines each, include callouts. Provide reproducibility seeds, versions. Use LoRA fine-tun...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Evaluating the Fine‑Tuned Model\n",
        "\n",
        "After the model has finished learning from your data, the next logical step is to *check* how well it has learned. Think of the model as a student who just finished a course: you want to give them a test to see if they really understood the material. In machine learning, that test is called **evaluation**.\n",
        "\n",
        "### Why evaluate?\n",
        "- **Perplexity** tells us how surprised the model is by the validation data. A lower perplexity means the model is more confident and accurate.\n",
        "- **Sample generation** lets us see the model’s *creative* side: does it produce coherent, domain‑appropriate text?\n",
        "- **Metric comparison** (e.g., BLEU, ROUGE, accuracy) lets us benchmark against baselines or previous checkpoints.\n",
        "- **Early stopping**: if the validation loss stops improving, we can halt training to avoid over‑fitting.\n",
        "\n",
        "### Extra explanatory paragraph\n",
        "\n",
        "- **Perplexity** is the exponential of the cross‑entropy loss. It’s a measure of how many choices the model thinks it has at each step. A perplexity of 10 means the model is, on average, as uncertain as picking one out of ten equally likely words.\n",
        "- **Cross‑entropy loss** is the standard loss for language modeling; it penalises the model when it assigns low probability to the true next token.\n",
        "- **Validation set** is a held‑out portion of the data that the model never saw during training. It simulates unseen real‑world data.\n",
        "- **GPU memory trade‑off**: evaluating on large batches gives more stable metrics but consumes more memory. A common compromise is to use a batch size of 1–4 for GPT‑OSS‑20B.\n",
        "- **Reproducibility**: Setting the same random seed before evaluation ensures that the same subset of validation examples is used each run, making metric comparisons fair.\n",
        "\n",
        "By the end of this section you’ll have a clear numeric score (perplexity), a handful of generated samples, and a sanity‑check that the model behaves as expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Evaluation utilities – compute perplexity on the validation set\n",
        "# ------------------------------------------------------------\n",
        "import torch\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Reproducibility: same seed for evaluation shuffling\n",
        "EVAL_SEED = 42\n",
        "torch.manual_seed(EVAL_SEED)\n",
        "\n",
        "# Assume `val_loader` from Step 3 is still in memory\n",
        "# If not, re‑create it here (same tokenizer, collator, batch size 1 for memory safety)\n",
        "# from transformers import DataCollatorForLanguageModeling\n",
        "# collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
        "\n",
        "model.eval()\n",
        "model.to(\"cuda\")  # ensure model is on GPU\n",
        "\n",
        "total_loss = 0.0\n",
        "total_tokens = 0\n",
        "\n",
        "print(\"📊  Evaluating on validation set…\")\n",
        "for batch in tqdm(val_loader, desc=\"Eval batches\"):\n",
        "    # Move batch to GPU\n",
        "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "\n",
        "    # Shift labels for causal LM: predict next token\n",
        "    labels = input_ids.clone()\n",
        "    labels[:, :-1] = input_ids[:, 1:]\n",
        "    labels[:, -1] = -100  # ignore last token in loss\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "    total_loss += loss.item() * input_ids.size(0)\n",
        "    total_tokens += input_ids.numel()\n",
        "\n",
        "# Cross‑entropy loss\n",
        "avg_loss = total_loss / total_tokens\n",
        "perplexity = math.exp(avg_loss)\n",
        "print(f\"✅  Average loss: {avg_loss:.4f}\")\n",
        "print(f\"🔥  Perplexity: {perplexity:.2f}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Quick sample generation – see what the fine‑tuned model says\n",
        "# ------------------------------------------------------------\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer again if not already in memory\n",
        "MODEL_NAME = \"gpt-oss-20b\"\n",
        "print(\"🔍  Loading tokenizer for generation…\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Helper to generate a few samples\n",
        "def generate_samples(prompt, max_new_tokens=50, num_samples=3):\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_samples):\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "                temperature=0.8,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "            text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "            print(f\"\\n--- Sample {i+1} ---\")\n",
        "            print(text)\n",
        "\n",
        "# Example prompt – tweak to match your domain\n",
        "prompt = \"In the realm of quantum computing, the most promising approach is\"\n",
        "print(\"📣  Generating samples for prompt:\")\n",
        "print(f\"\\n{prompt}\\n\")\n",
        "generate_samples(prompt, max_new_tokens=60, num_samples=2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following is NOT a recommended step before fine‑tuning GPT‑Oss‑20B?\", [\"Validate GPU availability and CUDA version.\",\"Install ipywidgets>=8.0.0.\",\"Use the original model weights without any modifications.\",\"Configure Accelerate for distributed training.\"], 2, \"Fine‑tuning should always start from the pre‑trained weights; using the original weights without any modifications is not a step but the starting point.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the primary benefit of using LoRA during fine‑tuning?\", [\"It increases the model size dramatically.\",\"It reduces the number of trainable parameters.\",\"It eliminates the need for GPU memory.\",\"It guarantees zero overfitting.\"], 1, \"LoRA introduces low‑rank adapters, keeping the majority of the model frozen and drastically reducing trainable parameters.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:52:24.632Z",
      "title": "Deploying and Fine‑Tuning GPT‑Oss‑20B in Jupyter: A Practitioner’s Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T02:52:24.638Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}