{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Remix Note**: This ELI5 version was created with the Applied Learning AI Notebooks (ALAIN) Project on 09.14.2025.\\n",
        "Created by [Daniel Green](https://www.linkedin.com/in/danielpgreen).\\n",
        "---\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîß Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('üìù Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîê Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'üîè Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '‚àÖ'\\n    return v[:3] + '‚Ä¶' + v[-2:] if len(v) > 6 else '‚Ä¢‚Ä¢‚Ä¢'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üåê ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('‚úÖ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('‚ö†Ô∏è Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üîé Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('‚ö†Ô∏è Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('‚úÖ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('‚ö†Ô∏è Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) ‚Äî 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT‚Äë5 Prompting Guide ‚Äî ELI5 Remix\n\nA beginner‚Äëfriendly, analogy‚Äëheavy walkthrough that turns the complex art of GPT‚Äë5 prompting into a simple, step‚Äëby‚Äëstep recipe. Learners will see how to ask the model like a helpful friend, control its curiosity, and keep it on track without any coding jargon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ‚è±Ô∏è Estimated time to complete: 36‚Äì60 minutes (rough).  ",
        "\n> üïí Created (UTC): 2025-09-16T02:49:30.115Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what agentic prompting is and why it matters for GPT‚Äë5.\n",
        "2. Show how to set the model‚Äôs curiosity level with simple, everyday language.\n",
        "3. Teach how to give clear, friendly instructions that keep the model focused.\n",
        "4. Demonstrate how to use tool calls and preambles in a way that feels natural to non‚Äëdevelopers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic understanding of what an AI chatbot is.\n",
        "- Access to a web browser and an OpenAI account.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 openai>=1.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"openai>=1.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('‚úÖ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Welcome & What Is Prompting?\n",
        "\n",
        "Imagine you‚Äôre at a kitchen where a super‚Äësmart chef (the model) can cook anything you ask. The *prompt* is the recipe you hand to the chef. It tells the chef what you want, how you want it, and sometimes even how to *think* about it. The chef then follows the recipe and gives you a finished dish (the *completion*).\n",
        "\n",
        "### Why Prompting Matters\n",
        "Prompting is the bridge between your human intent and the model‚Äôs language generation. A well‚Äëcrafted prompt:\n",
        "\n",
        "- **Reduces ambiguity**: The model knows exactly what you‚Äôre after.\n",
        "- **Controls style and length**: By adding constraints, you can get concise answers or detailed explanations.\n",
        "- **Guides reasoning**: You can ask the model to think step‚Äëby‚Äëstep, which is especially useful for complex tasks.\n",
        "\n",
        "### Key Terms (and why they matter)\n",
        "- **Prompt**: The text you send to the model. Think of it as the recipe.\n",
        "- **Completion**: The text the model returns. The finished dish.\n",
        "- **Temperature**: A knob that controls randomness. 0 = very deterministic, 1 = more creative.\n",
        "- **Max Tokens**: The maximum length of the completion. It‚Äôs like setting a time limit for the chef.\n",
        "- **Tool Calls**: Optional actions the model can request (e.g., look up a fact). They‚Äôre like asking the chef to fetch a special ingredient.\n",
        "\n",
        "### Rationale & Trade‚Äëoffs\n",
        "When you set a low temperature, the chef sticks closely to the recipe, producing safe and predictable answers. However, it may miss creative solutions. A higher temperature encourages exploration but can lead to off‚Äëtopic or nonsensical responses. Similarly, a very short max token limit forces brevity but may cut off important details. Balancing these settings is like seasoning a dish: too little and it‚Äôs bland, too much and it‚Äôs overwhelming.\n",
        "\n",
        "### Quick sanity check\n",
        "Before we dive into code, make sure you have an OpenAI API key set in your environment:\n",
        "\n",
        "```bash\n",
        "export OPENAI_API_KEY=your_api_key_here\n",
        "```\n",
        "\n",
        "If you‚Äôre using a notebook, you can also set it directly in Python:\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'your_api_key_here'\n",
        "```\n",
        "\n",
        "Now let‚Äôs see a minimal example that sends a prompt and prints the completion.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages (run once)\n",
        "try:\n",
        "    import openai\n",
        "except ImportError:\n",
        "    import subprocess, sys\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", 'openai>=1.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "    import openai\n",
        "\n",
        "# Set your API key (replace with your own key or set via environment variable)\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Basic prompt example\n",
        "prompt_text = \"\"\"Explain the concept of photosynthesis in simple terms.\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-5\",  # placeholder for the actual GPT‚Äë5 model name\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
        "    temperature=0.5,   # moderate creativity\n",
        "    max_tokens=150,    # keep answer concise\n",
        ")\n",
        "\n",
        "# Print the model‚Äôs completion\n",
        "print(response.choices[0].message.content.strip())\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Meet the Agent ‚Äî GPT‚Äë5‚Äôs Personality\n",
        "\n",
        "Think of GPT‚Äë5 as a friendly robot chef in a kitchen. The *personality* of the chef is the way it talks, the tone it uses, and how it decides what to cook. In the world of language models, we give the chef a *system prompt* that tells it \"You are a helpful, patient, and slightly witty assistant who loves to explain things in simple steps. Keep your answers friendly and avoid jargon unless the user asks for it.\"\n",
        "\n",
        "### Why Personality Matters\n",
        "Just like a real chef, a model‚Äôs personality can change how it reacts to the same recipe. A cheerful chef might add a playful comment, while a serious chef sticks to facts. In prompting, setting a clear personality:\n",
        "\n",
        "- **Reduces ambiguity**: The model knows the expected tone.\n",
        "- **Improves consistency**: Every answer feels like it comes from the same friend.\n",
        "- **Guides behavior**: It can politely refuse to answer harmful questions or ask clarifying questions.\n",
        "\n",
        "### Key Terms (and why they matter)\n",
        "- **System Prompt**: The first message in a chat that sets the model‚Äôs role and style.\n",
        "- **Agent**: The instantiated model that follows the system prompt and user messages.\n",
        "- **Persona**: The combination of tone, style, and behavioral rules the agent follows.\n",
        "- **Temperature**: A knob that controls randomness; 0 = very deterministic, 1 = more creative.\n",
        "- **Max Tokens**: The maximum length of the model‚Äôs reply.\n",
        "\n",
        "### Rationale & Trade‚Äëoffs\n",
        "A highly specific system prompt can make the model feel very consistent, but it may also *lock* the model into a narrow style, reducing its ability to adapt to unexpected user requests. Conversely, a vague prompt gives the model more freedom but can lead to inconsistent tone or off‚Äëtopic answers. Think of it like seasoning: too much salt (over‚Äëspecific) can overpower the dish, while too little (under‚Äëspecific) can leave it bland.\n",
        "\n",
        "### Quick sanity check\n",
        "Make sure you have the OpenAI library installed and your API key set (see Step‚ÄØ1). Then run the code below to see the agent in action.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages (run once)\n",
        "try:\n",
        "    import openai\n",
        "except ImportError:\n",
        "    import subprocess, sys\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", 'openai>=1.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "    import openai\n",
        "\n",
        "# Set your API key (replace with your own key or set via environment variable)\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Define a friendly persona via a system prompt\n",
        "system_prompt = (\n",
        "    \"You are GPT‚Äë5, a helpful assistant with a friendly, patient tone. \"\n",
        "    \"Explain concepts in simple, everyday language and ask clarifying \"\n",
        "    \"questions if needed. Avoid jargon unless the user explicitly asks for it.\"\n",
        "        )\n",
        "\n",
        "# Example user request\n",
        "user_message = \"Can you explain how a battery works?\"\n",
        "\n",
        "# Call the model with the persona\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-5\",  # placeholder for the actual GPT‚Äë5 model name\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "        ],\n",
        "    temperature=0.3,   # keep answers consistent with the persona\n",
        "    max_tokens=200,    # limit length for a quick reply\n",
        ")\n",
        "\n",
        "# Print the model‚Äôs completion\n",
        "print(response.choices[0].message.content.strip())\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: The Curiosity Scale ‚Äî Controlling Eagerness\n",
        "\n",
        "Imagine you‚Äôre a curious child in a candy shop. If you let yourself wander freely, you‚Äôll taste every sweet, but you might also get overwhelmed by the sheer number of choices. If you stick to a single candy bar, you‚Äôll enjoy it fully but miss out on the rest. GPT‚Äë5‚Äôs *curiosity* is the same idea: it decides how much it wants to explore beyond the prompt you give it.\n",
        "\n",
        "### How the Model‚Äôs Eagerness Works\n",
        "\n",
        "- **Temperature**: Think of it as the \"spice level\" in a recipe. 0 is bland and predictable; 1 is fiery and unpredictable. A higher temperature makes the model pick less‚Äëlikely words, which can lead to creative but sometimes off‚Äëtopic answers.\n",
        "- **Top‚ÄëP (nucleus sampling)**: This is like a filter that only lets the top‚Äë% of probability mass through. A low top‚Äëp (e.g., 0.5) forces the model to stay in the most common words, while a high top‚Äëp (e.g., 0.95) opens the door to rarer, more surprising words.\n",
        "- **Max Tokens**: The length limit is the \"time you‚Äôre allowed to talk\". A short limit keeps the answer concise but can cut off important details; a long limit lets the model elaborate but may wander.\n",
        "- **Seed**: When you want reproducible results, you can set a seed. It‚Äôs like locking the model‚Äôs random number generator so you get the same answer every time.\n",
        "\n",
        "### Key Terms (and why they matter)\n",
        "\n",
        "- **Curiosity**: The model‚Äôs tendency to deviate from the prompt and explore new ideas. Controlled by temperature and top‚Äëp.\n",
        "- **Temperature**: Controls randomness; 0 = deterministic, 1 = highly random.\n",
        "- **Top‚ÄëP**: The cumulative probability threshold for word selection.\n",
        "- **Max Tokens**: The maximum number of tokens the model can output.\n",
        "- **Seed**: A number that seeds the random generator for reproducibility.\n",
        "\n",
        "### Rationale & Trade‚Äëoffs\n",
        "\n",
        "Balancing curiosity is like seasoning a dish: too little and the answer is bland and uninteresting; too much and it becomes chaotic or irrelevant. For fact‚Äëchecking or technical explanations, a low temperature (0.2‚Äë0.3) keeps the model grounded. For brainstorming, creative writing, or exploring possibilities, a higher temperature (0.7‚Äë0.9) can spark novel ideas but may also introduce noise. Top‚Äëp works hand‚Äëin‚Äëhand with temperature‚Äîlower top‚Äëp tightens the word pool, higher top‚Äëp expands it. Max tokens should be set to just enough to cover the answer without truncation, and a seed is useful when you need consistent outputs for demos or unit tests.\n",
        "\n",
        "### Quick sanity check\n",
        "\n",
        "Below we‚Äôll run two short examples: one with low curiosity (safe, factual) and one with high curiosity (creative, exploratory). Feel free to tweak the values and see how the responses change.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages (run once)\n",
        "try:\n",
        "    import openai\n",
        "except ImportError:\n",
        "    import subprocess, sys\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", 'openai>=1.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "    import openai\n",
        "\n",
        "# Set your API key (replace with your own key or set via environment variable)\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Helper function to call GPT‚Äë5 with custom settings\n",
        "def ask_gpt(prompt, temperature=0.5, top_p=0.95, max_tokens=150, seed=None):\n",
        "    \"\"\"Return the model‚Äôs completion for a given prompt.\n",
        "    Parameters:\n",
        "        prompt (str): The user message.\n",
        "        temperature (float): Controls randomness.\n",
        "        top_p (float): Nucleus sampling threshold.\n",
        "        max_tokens (int): Length limit.\n",
        "        seed (int, optional): Seed for reproducibility.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"model\": \"gpt-5\",  # placeholder for the actual GPT‚Äë5 model name\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"max_tokens\": max_tokens,\n",
        "    }\n",
        "    if seed is not None:\n",
        "        params[\"seed\"] = seed\n",
        "    response = openai.ChatCompletion.create(**params)\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Example 1: Low curiosity (safe, factual)\n",
        "prompt1 = \"Explain how a solar panel converts sunlight into electricity in simple terms.\"\n",
        "answer1 = ask_gpt(prompt1, temperature=0.2, top_p=0.9, max_tokens=120)\n",
        "print(\"\\n--- Low Curiosity Response ---\\n\")\n",
        "print(answer1)\n",
        "\n",
        "# Example 2: High curiosity (creative, exploratory)\n",
        "prompt2 = \"Imagine a future where solar panels can also grow plants. Describe how that might work.\"\n",
        "answer2 = ask_gpt(prompt2, temperature=0.8, top_p=0.95, max_tokens=180)\n",
        "print(\"\\n--- High Curiosity Response ---\\n\")\n",
        "print(answer2)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4\n",
        "\n",
        "Thinking...\n",
        ">We need to produce JSON structure for section 4. Must follow guidelines: 800-1000 tokens per section. Provide markdown and code cells. Include callouts. Provide estimated_tokens 1000. Provide prerequisites_check. Next section hint. Use beginner-friendly ELI5 language with analogies, but precise technical terms. Add one extra explanatory paragraph defining key terms and explaining rationale/trade-offs. Include executable code with comments; prefer 1-2 short code cells (<30 lines each...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5\n",
        "\n",
        "Thinking...\n",
        ">We need to produce JSON structure for section 5. Must follow guidelines: 800-1000 tokens per section. Provide markdown and code cells. Include callouts. Provide estimated_tokens 1000. Provide prerequisites_check. Next section hint. Use beginner-friendly ELI5 language with analogies, but precise technical terms. Add one extra explanatory paragraph defining key terms and explaining rationale/trade-offs. Include executable code with comments; prefer 1-2 short code cells (<30 lines each...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Tool Preambles ‚Äî The Model‚Äôs Roadmap\n",
        "\n",
        "Imagine you‚Äôre a traveler in a big city. Before you start walking, you look at a map that shows the main streets, the landmarks you want to visit, and the best route to get there. That map is your *tool preamble* for GPT‚Äë5. It tells the model what the user‚Äôs goal is, what tools it can use, and a high‚Äëlevel plan for how to get there.\n",
        "\n",
        "### Why a Preamble Helps\n",
        "\n",
        "- **Clarity**: Just like a map removes guesswork, a preamble removes ambiguity about *why* the model should call a tool.\n",
        "- **Efficiency**: The model can skip the \"I‚Äôm not sure what to do\" step and jump straight to the next tool.\n",
        "- **Safety**: By explicitly listing allowed tools, you prevent the model from calling something it shouldn‚Äôt.\n",
        "\n",
        "### Key Terms (and why they matter)\n",
        "\n",
        "- **Tool Preamble**: A short, friendly description that appears before the user message. It sets the goal, lists available tools, and outlines a plan.\n",
        "- **Tool**: A pre‚Äëdefined function the model can invoke, such as `search`, `calculator`, or `weather`. Think of it as a kitchen appliance the chef can use.\n",
        "- **Tool Call**: The actual request the model sends to the tool, including arguments.\n",
        "- **Plan**: A step‚Äëby‚Äëstep outline of how the model will reach the goal, often expressed in plain English.\n",
        "\n",
        "### Rationale & Trade‚Äëoffs\n",
        "\n",
        "A concise preamble is like a good GPS route: it gives you enough information to stay on track without overwhelming you with details. If the preamble is too long, the model may waste tokens parsing it and might even misinterpret the priority of steps. On the other hand, a very short preamble can leave the model guessing what tools are allowed, which can lead to unexpected or unsafe calls. The sweet spot is a 1‚Äë2 sentence preamble that lists the goal, the tools, and a simple plan.\n",
        "\n",
        "### Quick sanity check\n",
        "\n",
        "Below is a minimal, reproducible example that shows how to set up a tool preamble, define a couple of tools, and let GPT‚Äë5 follow the roadmap. Make sure you have the `openai` library installed and your API key set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages (run once)\n",
        "try:\n",
        "    import openai\n",
        "except ImportError:\n",
        "    import subprocess, sys\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", 'openai>=1.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('‚ö†Ô∏è Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "    import openai\n",
        "\n",
        "# Set your API key (replace with your own key or set via environment variable)\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Define a simple tool: a calculator that adds two numbers\n",
        "def add_numbers(a: float, b: float) -> float:\n",
        "    return a + b\n",
        "\n",
        "# Register the tool with the OpenAI client (this is a mock; real tools use the tool schema)\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"add_numbers\",\n",
        "            \"description\": \"Adds two numbers together.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
        "                    \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
        "                },\n",
        "                \"required\": [\"a\", \"b\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Tool preamble: goal, tools, and plan\n",
        "tool_preamble = (\n",
        "    \"You are a helpful assistant. Your goal is to answer the user‚Äôs math question. \"\n",
        "    \"You can use the following tool: add_numbers. \"\n",
        "    \"Plan: 1) Parse the user‚Äôs numbers. 2) Call add_numbers with those numbers. 3) Return the result.\"\n",
        "        )\n",
        "\n",
        "# User message\n",
        "user_msg = \"What is 12.5 plus 7.3?\"\n",
        "\n",
        "# Call GPT‚Äë5 with the tool preamble and the tool list\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-5\",  # placeholder for the actual GPT‚Äë5 model name\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": tool_preamble},\n",
        "        {\"role\": \"user\", \"content\": user_msg}\n",
        "    ],\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",\n",
        "    temperature=0.2,\n",
        "    max_tokens=60,\n",
        "    seed=42,  # reproducibility\n",
        ")\n",
        "\n",
        "# Print the model‚Äôs final answer\n",
        "print(\"\\nModel response:\\n\", response.choices[0].message.content.strip())\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>‚ö†Ô∏è Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>‚úÖ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>‚ùå Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of the following best describes the purpose of a tool preamble?\", [\"To give the model a list of all possible tools.\",\"To explain the user‚Äôs goal and outline the plan before calling tools.\",\"To set the model‚Äôs temperature to 0.\",\"To limit the number of tokens the model can use.\"], 1, \"A tool preamble is a brief, friendly summary that tells the model what the user wants and how it will proceed, making the interaction easier to follow.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"When setting an agent‚Äôs curiosity level to a lower value, what effect does it primarily have?\", [\"It increases the model‚Äôs tendency to ask clarifying questions.\",\"It makes the model produce longer, more detailed responses.\",\"It limits the model‚Äôs willingness to explore beyond the prompt.\",\"It raises the temperature of the generation.\"], 2, \"A lower curiosity (often achieved with a lower temperature or explicit instruction) keeps the model more focused on the prompt and reduces exploratory behavior.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T02:49:30.109Z",
      "title": "GPT‚Äë5 Prompting Guide ‚Äî ELI5 Remix",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "remix": true,
    "remix_source": "gpt-5_prompting_guide.ipynb",
    "remix_date": "2025-09-14",
    "remix_by": "Daniel Green",
    "remix_by_link": "https://www.linkedin.com/in/danielpgreen",
    "created_utc": "2025-09-16T02:49:30.115Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36‚Äì60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}