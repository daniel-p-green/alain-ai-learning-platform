{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with GPT-OSS 20B: A Beginner's Guide\n\nThis lesson walks absolute beginners through everything needed to run the GPT-OSS 20B model locally. You’ll learn how to set up the environment, load the model, and run it in interactive notebooks with simple examples, all explained with everyday analogies and hands‑on exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:51:37.448Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Understand what GPT-OSS 20B is and why it matters.\n",
        "2. Set up a reproducible Jupyter environment with ipywidgets for demos.\n",
        "3. Load the 20B model and run a basic text generation.\n",
        "4. Identify common pitfalls and best practices for working with large language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic Python knowledge (no deep learning required).\n",
        "- Access to a machine with an NVIDIA GPU or sufficient CPU memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q python>=3.10 pip>=23.1 git ipynb ipywidgets>=8.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"python>=3.10\",\"pip>=23.1\",\"git\",\"ipynb\",\"ipywidgets>=8.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Meet GPT‑OSS 20B\n",
        "\n",
        "Welcome to the first step of your journey with GPT‑OSS 20B! Think of GPT‑OSS as a gigantic library of sentences that has read almost every book, article, and webpage on the internet. The *20B* part tells you how many “words” (more precisely, *parameters*) it has memorized—20 billion of them. That’s a lot of memory, but it also means the model can understand and generate text that feels surprisingly human.\n",
        "\n",
        "## Why 20B matters\n",
        "- **Scale vs. Speed**: A larger model usually gives better, more nuanced answers, but it also needs more GPU memory and takes longer to run. 20B is a sweet spot for many developers: it’s powerful enough for creative writing, code generation, and conversation, yet still fits on a single high‑end GPU (12 GB+).\n",
        "- **Fine‑tuning friendliness**: With 20B, you can fine‑tune on a small dataset (a few thousand lines) and get a specialized bot without needing a super‑cluster.\n",
        "\n",
        "## Key terms\n",
        "| Term | What it means | Why it matters |\n",
        "|------|---------------|----------------|\n",
        "| **Parameter** | A numeric weight inside the neural network that the model learns during training. | The more parameters, the more patterns the model can capture. |\n",
        "| **Checkpoint** | A saved snapshot of all parameters at a specific training step. | Allows you to load a pre‑trained model without training from scratch. |\n",
        "| **Tokenizer** | A tool that splits text into tokens (words, sub‑words, or characters). | GPT‑OSS uses a byte‑pair‑encoding (BPE) tokenizer to convert your prompt into numbers the model can process. |\n",
        "| **Inference** | Running the model to generate predictions (text) from a prompt. | This is what you’ll do in the notebook. |\n",
        "\n",
        "## Trade‑offs to keep in mind\n",
        "- **Memory vs. Latency**: 20B needs ~30 GB of VRAM for full precision inference. If you’re on a 12 GB GPU, you’ll need to use *mixed‑precision* (FP16) or *quantization* to fit. The trade‑off is a tiny drop in accuracy for a huge speed boost.\n",
        "- **Speed vs. Quality**: Generating longer passages or using higher temperature settings will slow down inference. For quick demos, keep the prompt short and temperature low.\n",
        "\n",
        "## Quick sanity check\n",
        "Below we’ll import the library, load the tokenizer, and print a short snippet of the model’s architecture. This will confirm that everything is wired up correctly.\n",
        "\n",
        "> **Note**: If you see an error about missing GPU drivers, you might need to install CUDA 12.1 or switch to CPU mode.\n",
        "\n",
        "## What you’ll learn\n",
        "- How to import GPT‑OSS and its tokenizer.\n",
        "- How to inspect the model’s configuration.\n",
        "- How to set a random seed for reproducible results.\n",
        "\n",
        "Let’s dive in!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import the GPT‑OSS library and set a reproducible seed\n",
        "# The seed ensures that any random choices (e.g., token sampling) are the same each run\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Import the model and tokenizer\n",
        "from gpt_oss import GPTOSS, GPTOSSTokenizer\n",
        "\n",
        "# Load the tokenizer (this is lightweight and fast)\n",
        "print(\"Loading tokenizer…\")\n",
        "try:\n",
        "    tokenizer = GPTOSSTokenizer.from_pretrained(\"gpt-oss-20b\")\n",
        "except Exception as e:\n",
        "    print(\"Error loading tokenizer:\", e)\n",
        "    raise\n",
        "\n",
        "# Load the model in FP16 for speed (requires a GPU with at least 12 GB VRAM)\n",
        "print(\"Loading model… (this may take a minute)\\n\")\n",
        "try:\n",
        "    model = GPTOSS.from_pretrained(\"gpt-oss-20b\", device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\", torch_dtype=torch.float16)\n",
        "except Exception as e:\n",
        "    print(\"Error loading model:\", e)\n",
        "    raise\n",
        "\n",
        "# Quick sanity check: print the number of parameters\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model loaded with {num_params/1e9:.2f} B parameters.\")\n",
        "\n",
        "# Show a tiny slice of the model config\n",
        "print(\"\\nModel config snippet:\")\n",
        "print(model.config)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Set Up Your Jupyter Notebook\n",
        "\n",
        "Before we can play with GPT‑OSS, we need a playground that lets us write code, see results, and even build interactive widgets. Think of Jupyter as a *digital notebook* where each page (cell) can contain either a paragraph of text or a snippet of code that runs right away. It’s like a recipe book that lets you tweak ingredients on the fly.\n",
        "\n",
        "## Why Jupyter matters\n",
        "- **Live coding**: You can run a line, see the output, and immediately tweak it. No need to write a script, run it, and then open a log file.\n",
        "- **Rich media**: Images, tables, and interactive widgets can be embedded directly in the notebook.\n",
        "- **Reproducibility**: By saving the notebook, you capture the exact sequence of commands that produced your results.\n",
        "\n",
        "## Key terms (and why they matter)\n",
        "| Term | What it is | Why you care |\n",
        "|------|------------|--------------|\n",
        "| **Notebook** | A file with `.ipynb` extension that mixes Markdown and executable code cells. | It’s the main interface for experimenting with GPT‑OSS. |\n",
        "| **ipywidgets** | A library that turns Python objects into interactive UI elements (sliders, buttons, etc.). | Lets you build demos where you can change temperature or prompt length without editing code. |\n",
        "| **nbextension** | A Jupyter extension that enables additional features, such as the widgets UI. | Without enabling it, the interactive widgets won’t render. |\n",
        "| **Environment variable** | A key‑value pair that tells programs where to look for resources (e.g., `GPT_OSS_MODEL_HOME`). | Keeps your model checkpoints organized and portable across machines. |\n",
        "\n",
        "## Trade‑offs to keep in mind\n",
        "- **Installation time vs. convenience**: Installing `ipywidgets` and enabling the nbextension takes a few seconds, but it saves you from having to write custom HTML/JavaScript later.\n",
        "- **GPU vs. CPU**: If you’re on a machine without a GPU, the notebook will still run but will be slower. The setup steps are the same; only the `torch_dtype` flag changes.\n",
        "- **Version compatibility**: Using the latest `ipywidgets` (≥ 8.0.0) ensures smooth integration with JupyterLab 4.x. Older versions may require additional configuration.\n",
        "\n",
        "## What you’ll do in this section\n",
        "1. Install the required Python packages.\n",
        "2. Enable the widgets extension so you can build interactive demos later.\n",
        "3. Verify that Jupyter and the extensions are working.\n",
        "\n",
        "Let’s get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Install the required packages\n",
        "# We use the `-q` flag for quiet output and `--upgrade` to ensure we have the latest compatible versions.\n",
        "# The `try/except` block is optional but helps catch installation errors early.\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "packages = [\n",
        "    \"gpt-oss==0.1.0\",  # the core library\n",
        "    \"ipywidgets>=8.0.0\"  # interactive widgets\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        print(f\"Installing {pkg}…\")\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", pkg]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "        except Exception as exc:\n",
        "            if IN_COLAB:\n",
        "                packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                if packages:\n",
        "                    try:\n",
        "                        import IPython\n",
        "                        ip = IPython.get_ipython()\n",
        "                        if ip is not None:\n",
        "                            ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                        else:\n",
        "                            import subprocess as _subprocess\n",
        "                            _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                    except Exception as colab_exc:\n",
        "                        print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                        raise\n",
        "                else:\n",
        "                    print('No packages specified for pip install; skipping fallback')\n",
        "            else:\n",
        "                raise\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"❌ Failed to install {pkg}: {e}\")\n",
        "        raise\n",
        "\n",
        "print(\"✅ All packages installed successfully.\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2️⃣ Enable the widgets nbextension\n",
        "# This command tells Jupyter to load the JavaScript that powers ipywidgets.\n",
        "# If you’re using JupyterLab, the extension is enabled automatically.\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "try:\n",
        "    print(\"Enabling widgets nbextension…\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"jupyter\", \"nbextension\", \"enable\", \"--py\", \"widgetsnbextension\", \"--sys-prefix\", \"-q\"])\n",
        "    print(\"✅ Widgets extension enabled.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"⚠️  Could not enable widgets: {e}\")\n",
        "    print(\"Make sure you have Jupyter installed and the extension is available.\")\n",
        "\n",
        "# 3️⃣ Quick sanity check: print Jupyter version\n",
        "import jupyter\n",
        "print(f\"Jupyter version: {jupyter.__version__}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Load & Inspect the Model\n",
        "\n",
        "In the previous step we pulled the tokenizer and the heavy‑weight 20‑billion‑parameter model into memory.  Think of the model as a gigantic Lego set: each block (parameter) is a tiny piece of knowledge that the model uses to build sentences.  In this step we’ll *inspect* that Lego set to make sure it’s the right size, understand its internal architecture, and run a quick test to see it in action.\n",
        "\n",
        "## Why inspection matters\n",
        "- **Debugging**: If the model fails to load or behaves oddly, a quick look at the config can reveal mismatched vocab sizes or missing layers.\n",
        "- **Performance tuning**: Knowing the number of layers, hidden size, and attention heads helps you decide whether to run on FP16, FP32, or quantized mode.\n",
        "- **Reproducibility**: Printing the config and seed guarantees that anyone else can replicate your results.\n",
        "\n",
        "## Key terms (and why they matter)\n",
        "| Term | What it is | Why you care |\n",
        "|------|------------|--------------|\n",
        "| **Device map** | A mapping that tells PyTorch which GPU or CPU each part of the model lives on. | It lets you spread a huge model across multiple GPUs or keep it on a single GPU with sharding. |\n",
        "| **torch_dtype** | The numerical precision (e.g., `float16`, `float32`) used for the model’s weights. | Lower precision saves memory and speeds up inference but can slightly degrade quality. |\n",
        "| **Config** | A dictionary of hyper‑parameters that describe the model’s architecture (e.g., `num_hidden_layers`, `hidden_size`). | It’s the blueprint that the library uses to rebuild the model from scratch. |\n",
        "| **Checkpoint** | A saved snapshot of all model weights. | It’s what you download once and then load many times. |\n",
        "\n",
        "## Trade‑offs to keep in mind\n",
        "- **Memory vs. Speed**: FP16 cuts memory usage by ~50 % and doubles throughput on modern GPUs.  If you’re on a 12 GB GPU, FP16 is usually mandatory for 20B.  FP32 gives the most accurate results but will likely OOM.\n",
        "- **Precision vs. Quality**: Quantization (e.g., 8‑bit) can reduce memory further but may introduce small artifacts.  For most demos, FP16 is the sweet spot.\n",
        "- **Sharding vs. Single‑GPU**: `device_map=\"auto\"` automatically shards the model across all available GPUs.  If you only have one GPU, you’ll need to enable `torch_dtype=torch.float16` and possibly use `accelerate` for automatic sharding.\n",
        "\n",
        "## Quick sanity check\n",
        "Below we’ll:\n",
        "1. Load the tokenizer and model again (this time with a clear device map).\n",
        "2. Print the total number of parameters and a snippet of the config.\n",
        "3. Run a tiny forward pass to generate a short sentence.\n",
        "\n",
        "> **Tip**: If you see an out‑of‑memory error, try adding `torch_dtype=torch.float16` and/or `device_map=\"auto\"`.\n",
        "\n",
        "Let’s dive in!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Load tokenizer & model with reproducible settings\n",
        "# -----------------------------------------------------------\n",
        "# Import libraries\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set a fixed seed for reproducibility\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Optional: ensure the environment variable points to the weights folder\n",
        "os.environ.setdefault(\"GPT_OSS_MODEL_HOME\", os.path.expanduser(\"~/.gpt-oss\"))\n",
        "\n",
        "# Import GPT‑OSS components\n",
        "from gpt_oss import GPTOSS, GPTOSSTokenizer\n",
        "\n",
        "# Load tokenizer (lightweight, no GPU needed)\n",
        "print(\"Loading tokenizer…\")\n",
        "try:\n",
        "    tokenizer = GPTOSSTokenizer.from_pretrained(\"gpt-oss-20b\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Tokenizer load failed:\", e)\n",
        "    raise\n",
        "\n",
        "# Load the model on GPU with FP16 precision\n",
        "print(\"Loading model… (this may take a minute)\\n\")\n",
        "try:\n",
        "    model = GPTOSS.from_pretrained(\n",
        "        \"gpt-oss-20b\",\n",
        "        device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",          # automatically shard across GPUs\n",
        "        torch_dtype=torch.float16   # use mixed precision for speed/memory\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"❌ Model load failed:\", e)\n",
        "    raise\n",
        "\n",
        "# 2️⃣ Inspect the model\n",
        "# ---------------------\n",
        "# Total number of parameters\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"✅ Model loaded with {num_params/1e9:.2f} B parameters.\")\n",
        "\n",
        "# Show a concise config snippet\n",
        "print(\"\\nModel config snippet:\")\n",
        "config = model.config\n",
        "print(f\"  vocab_size: {config.vocab_size}\")\n",
        "print(f\"  hidden_size: {config.hidden_size}\")\n",
        "print(f\"  num_hidden_layers: {config.num_hidden_layers}\")\n",
        "print(f\"  num_attention_heads: {config.num_attention_heads}\")\n",
        "print(f\"  device_map: {model.device_map}\")\n",
        "\n",
        "# 3️⃣ Quick forward pass – generate a short sentence\n",
        "# ---------------------------------------------------\n",
        "prompt = \"Once upon a time\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate 20 tokens after the prompt\n",
        "generated_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=20,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated text: \\n\", generated_text)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4️⃣ Optional: Visualize the attention pattern for the first token\n",
        "# ---------------------------------------------------------------\n",
        "# This is a lightweight example that shows how to extract the attention weights\n",
        "# for the first token in the generated sequence.  It’s useful for debugging\n",
        "# and for educational purposes.\n",
        "\n",
        "# Grab the attention weights from the last layer\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids,\n",
        "        output_attentions=True\n",
        "    )\n",
        "\n",
        "attn = outputs.attentions[-1]  # shape: (batch, heads, seq_len, seq_len)\n",
        "print(\"\\nAttention shape (last layer):\", attn.shape)\n",
        "\n",
        "# Convert to CPU numpy for plotting (if you want to visualize)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Take the first head and first token\n",
        "head0 = attn[0, 0, 0].cpu().numpy()\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(head0.reshape(1, -1), cmap=\"viridis\", cbar=False)\n",
        "plt.title(\"Attention of first token (head 0) over sequence\")\n",
        "plt.xlabel(\"Token position\")\n",
        "plt.ylabel(\"Token\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Generate Your First Prompt\n",
        "\n",
        "Now that the model is up and running, it’s time to give it a *prompt*—the little seed of text that tells the model what you want it to write. Think of the prompt like a question you ask a friend: the clearer and more specific you are, the better the answer you’ll get.\n",
        "\n",
        "## What is a *prompt*?\n",
        "A prompt is simply a string of text that you feed into the model’s tokenizer. The tokenizer turns that string into a sequence of integer IDs (tokens) that the neural network can understand. The model then predicts the next token in the sequence, one by one, until it reaches a stopping condition (like a maximum length or an end‑of‑sentence token).\n",
        "\n",
        "## Key terms you’ll see in the code\n",
        "| Term | What it means | Why it matters |\n",
        "|------|---------------|----------------|\n",
        "| **prompt** | The user‑supplied text that starts the generation. | It anchors the model’s output in a context you care about. |\n",
        "| **temperature** | A float that controls randomness in token sampling. | Low values (≈0.2) make the output deterministic; high values (≈1.0) add creativity. |\n",
        "| **top‑p (nucleus sampling)** | A probability threshold that keeps only the most likely tokens whose cumulative probability exceeds *p*. | It balances diversity and coherence; a lower *p* keeps the output more focused. |\n",
        "| **max_new_tokens** | The maximum number of tokens the model will generate after the prompt. | It limits latency and output length. |\n",
        "| **do_sample** | Boolean flag that tells the model to sample from the probability distribution instead of picking the highest‑probability token. | Sampling is required for temperature and top‑p to have an effect. |\n",
        "| **seed** | A fixed integer that initializes the random number generator. | Ensures reproducible generations across runs. |\n",
        "| **device** | The hardware (CPU or GPU) where tensors and the model live. | Using a GPU dramatically speeds up inference. |\n",
        "\n",
        "## Trade‑offs to keep in mind\n",
        "- **Determinism vs. Creativity**: Setting `temperature=0.0` and `do_sample=False` will always produce the same output for a given prompt, which is great for debugging but not for creative writing. Raising temperature or enabling sampling introduces variability.\n",
        "- **Speed vs. Length**: `max_new_tokens` controls how many words the model will produce. More tokens mean longer generation time and higher memory usage.\n",
        "- **Precision vs. Memory**: Running the model in `torch.float16` (FP16) halves memory usage and doubles throughput on modern GPUs, but a tiny loss in numerical precision can slightly affect the output.\n",
        "- **Sampling Strategy**: Temperature and top‑p are complementary. A high temperature with a low top‑p can still produce coherent text, but if you want truly diverse outputs, increase both.\n",
        "\n",
        "## Rationale for the code below\n",
        "We’ll keep the code short and focused on the core generation call. The snippet sets a reproducible seed, builds a prompt, and calls `model.generate`. It also demonstrates how to tweak temperature and top‑p to see the effect on the output. The code is wrapped in a `try/except` block so that any runtime errors (e.g., out‑of‑memory) are caught early.\n",
        "\n",
        "> **Tip**: If you’re running on a machine with a single 12 GB GPU, keep `torch_dtype=torch.float16` and `device_map=\"auto\"` to avoid OOM errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Generate text from a simple prompt\n",
        "# -------------------------------------------------\n",
        "# Import required libraries (already imported in previous steps, but re‑import for safety)\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Re‑set the seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Assume `model` and `tokenizer` are already loaded from Step 3\n",
        "# If not, you can load them again (see Step 3 for details)\n",
        "\n",
        "# Define a short, concrete prompt\n",
        "prompt = \"Write a short poem about a sunrise over the ocean\"\n",
        "\n",
        "# Encode the prompt into token IDs and move to the model’s device\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate 50 new tokens with a moderate temperature and top‑p\n",
        "generated_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=50,          # keep output short for demo purposes\n",
        "    temperature=0.7,            # a bit of randomness, but not too wild\n",
        "    top_p=0.9,                  # nucleus sampling for coherence\n",
        "    do_sample=True,             # enable sampling (required for temperature/top_p)\n",
        "    pad_token_id=tokenizer.eos_token_id  # avoid warnings on padding\n",
        ")\n",
        "\n",
        "# Decode the generated token IDs back to text\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"\\n=== Generated Text ===\\n\")\n",
        "print(generated_text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2️⃣ Quick experiment: change temperature and top‑p\n",
        "# -------------------------------------------------------\n",
        "# Feel free to modify the values below to see how the output changes.\n",
        "# Lower temperature (0.2) → more deterministic, safer output.\n",
        "# Higher temperature (1.0) → more creative, riskier output.\n",
        "# Lower top_p (0.8) → stricter selection of tokens.\n",
        "# Higher top_p (0.95) → more diverse token pool.\n",
        "\n",
        "for temp, top in [(0.2, 0.9), (1.0, 0.9), (0.7, 0.95)]:\n",
        "    gen_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=30,\n",
        "        temperature=temp,\n",
        "        top_p=top,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "    print(f\"\\n--- Temp={temp}, Top‑p={top} ---\")\n",
        "    print(text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5: Experiment with Temperature & Top‑P\n",
        "\n",
        "In the last step we saw how to generate text with a fixed set of hyper‑parameters.  Now we’ll treat *temperature* and *top‑p* as knobs on a soundboard and see how turning them changes the music the model plays.\n",
        "\n",
        "## What are we turning?\n",
        "- **Temperature** is like the volume knob on a radio.  A low temperature (≈0.2) makes the model play the most popular, safe songs.  A high temperature (≈1.0) lets it try out more obscure tracks, sometimes producing surprising melodies.\n",
        "- **Top‑p (nucleus sampling)** is a filter that keeps only the most likely next notes whose cumulative probability reaches *p*.  Think of it as a “focus” button that says, “Only play the top 90 % of the most probable notes.”  A low *p* keeps the song tight; a high *p* opens the door to more variety.\n",
        "\n",
        "## Extra explanatory paragraph\n",
        "\n",
        "| Term | What it means | Why it matters |\n",
        "|------|---------------|----------------|\n",
        "| **Logits** | Raw, un‑scaled scores the model outputs for every token. | They are the raw “opinions” before turning into probabilities. |\n",
        "| **Softmax** | The mathematical function that turns logits into a probability distribution. | It ensures the probabilities sum to 1, making sampling possible. |\n",
        "| **Sampling** | Randomly picking the next token according to the probability distribution. | It injects creativity; deterministic generation uses the highest‑probability token instead. |\n",
        "| **Temperature scaling** | Dividing logits by *temperature* before softmax. | Low temperatures sharpen the distribution (more deterministic), high temperatures flatten it (more random). |\n",
        "| **Top‑p threshold** | The smallest cumulative probability *p* that captures the most likely tokens. | It limits the token pool to the most promising candidates, balancing coherence and diversity. |\n",
        "\n",
        "**Rationale & trade‑offs**\n",
        "- **Determinism vs. Creativity**: Setting `temperature=0.0` and `do_sample=False` guarantees the same output every time, which is great for debugging or reproducible demos.  Raising temperature or enabling sampling introduces variability, useful for creative writing but harder to test.\n",
        "- **Coherence vs. Surprise**: A low top‑p (e.g., 0.8) keeps the model within a tight, coherent band of tokens, reducing the chance of nonsensical jumps.  A higher top‑p (e.g., 0.95) opens the door to more unexpected words, which can be exciting but may also break the narrative flow.\n",
        "- **Speed vs. Length**: Both temperature and top‑p affect the number of tokens the model needs to evaluate.  Higher temperature can cause the model to wander longer before hitting a stop token, slightly increasing latency.\n",
        "- **Memory vs. Precision**: These parameters do not change memory usage, but the choice of `torch_dtype` (FP16 vs. FP32) does.  FP16 halves memory and doubles throughput on modern GPUs, at the cost of a tiny numerical precision loss.\n",
        "\n",
        "## Quick sanity check\n",
        "Below we’ll:\n",
        "1. Define a helper that generates text given temperature and top‑p.\n",
        "2. Run a few experiments to see how the output changes.\n",
        "3. Wrap the helper in an interactive widget so you can play with the knobs in real time.\n",
        "\n",
        "> **Tip**: Keep `torch_dtype=torch.float16` and `device_map=\"auto\"` if you’re on a single 12 GB GPU to avoid out‑of‑memory errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Helper to generate text with temperature & top‑p\n",
        "# ----------------------------------------------------------\n",
        "# Assumes `model` and `tokenizer` are already loaded (see Step 4)\n",
        "# We keep the seed fixed for reproducibility.\n",
        "import torch, random, numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "def generate_text(prompt, temp=0.7, top_p=0.9, max_new=50):\n",
        "    \"\"\"Return a generated string for *prompt*.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt : str\n",
        "        The seed text.\n",
        "    temp : float\n",
        "        Temperature scaling.\n",
        "    top_p : float\n",
        "        Nucleus sampling threshold.\n",
        "    max_new : int\n",
        "        Max tokens to generate.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    gen_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new,\n",
        "        temperature=temp,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Quick demo with three settings\n",
        "prompts = [\"Once upon a time\", \"The quick brown fox\", \"In a galaxy far, far away\"]\n",
        "settings = [\n",
        "    (0.2, 0.9),\n",
        "    (0.7, 0.9),\n",
        "    (1.0, 0.95),\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\n=== Prompt: {prompt} ===\")\n",
        "    for temp, top in settings:\n",
        "        out = generate_text(prompt, temp=temp, top_p=top, max_new=30)\n",
        "        print(f\"\\nTemp={temp:.1f}, Top‑p={top:.2f} →\\n{out}\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2️⃣ Interactive slider to play with temperature & top‑p\n",
        "# ----------------------------------------------------------\n",
        "# Requires ipywidgets.  If you haven’t installed it, run:\n",
        "#   pip install ipywidgets>=8.0.0\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "@widgets.interact\n",
        "def interactive_demo(\n",
        "    prompt: widgets.Text(value=\"Write a short story about a robot\", description=\"Prompt:\"),\n",
        "    temperature: widgets.FloatSlider(value=0.7, min=0.1, max=1.5, step=0.1, description=\"Temperature:\"),\n",
        "    top_p: widgets.FloatSlider(value=0.9, min=0.5, max=1.0, step=0.05, description=\"Top‑p:\"),\n",
        "    max_new: widgets.IntSlider(value=50, min=10, max=200, step=10, description=\"Max tokens:\"),\n",
        "):\n",
        "    \"\"\"Generate text live as you adjust the sliders.\"\"\"\n",
        "    out = generate_text(prompt, temp=temperature, top_p=top_p, max_new=max_new)\n",
        "    print(\"\\n--- Generated Text ---\\n\")\n",
        "    print(out)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 6: Use ipywidgets for Interactive Demo\n",
        "\n",
        "In the previous steps we learned how to load GPT‑OSS and generate text with fixed hyper‑parameters.  Now we’ll turn the notebook into a *live playground* where you can tweak the prompt, temperature, and top‑p on the fly and see the model’s response instantly.\n",
        "\n",
        "## Why interactive demos matter\n",
        "Think of a recipe book that lets you change the amount of salt or the cooking time and immediately taste the result.  An interactive widget does the same for language models: you can experiment with different settings without editing code or re‑running long cells.  This is especially useful when you want to *explore* the model’s behavior or when you’re teaching others how the knobs affect output.\n",
        "\n",
        "## Key terms (and why they matter)\n",
        "| Term | What it is | Why you care |\n",
        "|------|------------|--------------|\n",
        "| **ipywidgets** | A Python library that turns objects into UI elements (sliders, text boxes, buttons). | Lets you build interactive controls inside a Jupyter notebook. |\n",
        "| **Interact** | A decorator that automatically creates widgets from function arguments. | Simplifies the wiring between UI and code – no manual widget layout needed. |\n",
        "| **Slider** | A UI element that lets you pick a numeric value within a range. | Controls continuous parameters like temperature or top‑p. |\n",
        "| **Text** | A widget that accepts free‑form text input. | Lets you change the prompt without editing code. |\n",
        "| **Output area** | The region where the generated text appears. | Provides immediate visual feedback. |\n",
        "\n",
        "## Extra explanatory paragraph\n",
        "The interactive demo is built on top of the `generate_text` helper we defined in Step 5.  That helper takes a prompt, temperature, top‑p, and maximum token count, then returns the decoded string.  The `ipywidgets.interact` decorator automatically creates a UI for each argument: a text box for the prompt, sliders for temperature and top‑p, and a slider for the maximum number of tokens.  When you move a slider or type a new prompt, the function runs again and prints the new output.  This real‑time feedback loop is invaluable for understanding how each hyper‑parameter shapes the model’s creativity and coherence.\n",
        "\n",
        "**Trade‑offs to keep in mind**\n",
        "- **Speed vs. Interactivity**: Generating 200 tokens can take a few seconds, especially on a single GPU.  The UI will freeze until the generation finishes, so keep `max_new_tokens` moderate for a smooth experience.\n",
        "- **Memory vs. Precision**: Using `torch.float16` (FP16) halves VRAM usage and doubles throughput on modern GPUs, but a tiny loss in numerical precision can slightly affect the output.  For demos, FP16 is usually the sweet spot.\n",
        "- **Determinism vs. Exploration**: Setting `temperature=0.0` and `do_sample=False` makes the output repeatable, which is great for debugging.  Raising temperature or enabling sampling introduces variability, which is useful for creative writing but harder to test.\n",
        "\n",
        "Let’s put it all together and create a live demo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1️⃣ Helper to generate text (re‑used from Step 5)\n",
        "# ----------------------------------------------------------\n",
        "# Assumes `model` and `tokenizer` are already loaded.\n",
        "import torch, random, numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "def generate_text(prompt, temp=0.7, top_p=0.9, max_new=50):\n",
        "    \"\"\"Return a generated string for *prompt*.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt : str\n",
        "        The seed text.\n",
        "    temp : float\n",
        "        Temperature scaling.\n",
        "    top_p : float\n",
        "        Nucleus sampling threshold.\n",
        "    max_new : int\n",
        "        Max tokens to generate.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    gen_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new,\n",
        "        temperature=temp,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2️⃣ Interactive demo with ipywidgets\n",
        "# ----------------------------------------------------------\n",
        "# Requires ipywidgets.  If you haven’t installed it, run:\n",
        "#   pip install ipywidgets>=8.0.0\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "@widgets.interact\n",
        "def interactive_demo(\n",
        "    prompt: widgets.Text(value=\"Write a short story about a robot\", description=\"Prompt:\"),\n",
        "    temperature: widgets.FloatSlider(value=0.7, min=0.1, max=1.5, step=0.1, description=\"Temperature:\"),\n",
        "    top_p: widgets.FloatSlider(value=0.9, min=0.5, max=1.0, step=0.05, description=\"Top‑p:\"),\n",
        "    max_new: widgets.IntSlider(value=50, min=10, max=200, step=10, description=\"Max tokens:\"),\n",
        "):\n",
        "    \"\"\"Generate text live as you adjust the sliders.\"\"\"\n",
        "    out = generate_text(prompt, temp=temperature, top_p=top_p, max_new=max_new)\n",
        "    print(\"\\n--- Generated Text ---\\n\")\n",
        "    print(out)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which parameter controls the diversity of the output?\", [\"Batch size\",\"Temperature\",\"Learning rate\",\"Optimizer\"], 1, \"Temperature is a knob that adjusts how random the model’s predictions are; a higher temperature makes the output more diverse.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What does the variable GPT_OSS_MODEL_HOME specify?\", [\"The path to the Jupyter installation\",\"Where the GPT-OSS checkpoints are stored\",\"The name of the model architecture\",\"The default prompt text\"], 1, \"GPT_OSS_MODEL_HOME tells the library where to look for the pre‑downloaded model weights.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:51:37.441Z",
      "title": "Getting Started with GPT-OSS 20B: A Beginner's Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "created_utc": "2025-09-16T03:51:37.448Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}