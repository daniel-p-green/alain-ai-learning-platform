{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Remix Note**: This ELI5 version was created with the Applied Learning AI Notebooks (ALAIN) Project on 09.14.2025.\\n",
        "Created by [Daniel Green](https://www.linkedin.com/in/danielpgreen).\\n",
        "---\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\\n\\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\\n\\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\\n",
        "- Find your keys:\\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\\n\\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "                    try:\n",
        "                        subprocess.check_call(cmd)\n",
        "                    except Exception as exc:\n",
        "                        if IN_COLAB:\n",
        "                            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                            if packages:\n",
        "                                try:\n",
        "                                    import IPython\n",
        "                                    ip = IPython.get_ipython()\n",
        "                                    if ip is not None:\n",
        "                                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                                    else:\n",
        "                                        import subprocess as _subprocess\n",
        "                                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                                except Exception as colab_exc:\n",
        "                                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                                    raise\n",
        "                            else:\n",
        "                                print('No packages specified for pip install; skipping fallback')\n",
        "                        else:\n",
        "                            raise\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'openai>=1.34.0']\n",
        "            try:\n",
        "                subprocess.check_call(cmd)\n",
        "            except Exception as exc:\n",
        "                if IN_COLAB:\n",
        "                    packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "                    if packages:\n",
        "                        try:\n",
        "                            import IPython\n",
        "                            ip = IPython.get_ipython()\n",
        "                            if ip is not None:\n",
        "                                ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                            else:\n",
        "                                import subprocess as _subprocess\n",
        "                                _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                        except Exception as colab_exc:\n",
        "                            print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                            raise\n",
        "                    else:\n",
        "                        print('No packages specified for pip install; skipping fallback')\n",
        "                else:\n",
        "                    raise\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-5 Prompting for Absolute Beginners: A Playground Guide\n\nImagine GPT-5 as an eager new friend who can fetch facts, write stories, or build tiny apps—if you ask clearly. This notebook teaches non-coders how to steer that friend with simple, repeatable “magic phrases” so you always get helpful, on-topic answers and avoid confusing tangents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n> ⏱️ Estimated time to complete: 36–60 minutes (rough).  ",
        "\n> 🕒 Created (UTC): 2025-09-16T03:49:46.231Z\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what a prompt is using toy-box analogies (no jargon).\n",
        "2. Write three short prompts that tell GPT-5 exactly how eager or cautious it should be.\n",
        "3. Spot and fix two common mistakes that make GPT-5 wander off-topic.\n",
        "4. Use a free, no-code widget to test prompts and instantly see better results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- A free OpenAI account (no API key needed for the playground widget).\n",
        "- Basic web browsing: you can copy-paste text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Meet Your New Robot Friend\n",
        "\n",
        "Imagine you just adopted a super-smart puppy who can read every book in the world in one second. If you say, “Fetch me a story,” it might bring back a 400-page novel, a tweet, or a shopping list—because it wants to please but doesn’t know *exactly* what you mean. GPT-5 is that puppy, except it lives inside your browser and speaks in text. A **prompt** is simply the note you hand to this puppy before it runs off. Write the note clearly, and you’ll get the exact toy you wanted; write it vaguely, and you might get a slobbery shoe instead.\n",
        "\n",
        "In technical terms, a prompt is the natural-language instruction (and optional examples) that primes a large language model to produce a desired output. The model has no memory of your past chats unless you include it, and it has no goals other than “continue the text in the most likely way.” Therefore, the trade-off is simple: more detail in your prompt equals less guesswork by the model, but also more tokens spent and a slightly longer wait. Learning to balance clarity with brevity is the whole game.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick sanity check: make sure we can talk to OpenAI\n",
        "# (No key needed for the *widget* we’ll install later, but let’s verify the library loads.)\n",
        "import openai, sys\n",
        "openai.__version__\n",
        "# Expected: 1.x.x. If you see an error, run: pip install -q -U openai\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the world’s smallest prompt demo—think of it as saying \"Sit!\" to the puppy. Run the cell to see how the model reacts to a tiny, vague request versus a short, specific one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title Tiny Prompt Test { run: \"auto\" }\n",
        "from openai import OpenAI\n",
        "client = OpenAI()  # will pick up OPENAI_API_KEY if you have one\n",
        "\n",
        "vague   = \"Tell me about dogs\"\n",
        "specific= \"List 3 fun facts about dogs under 60 characters each\"\n",
        "\n",
        "def quick_test(prompt):\n",
        "    try:\n",
        "        r = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",  # free tier friendly\n",
        "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "            temperature=0.3,  # low creativity for consistency\n",
        "            max_tokens=120\n",
        "        )\n",
        "        return r.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"(API not ready: {e})\"\n",
        "\n",
        "print(\"=== Vague prompt ===\")\n",
        "print(quick_test(vague))\n",
        "print(\"\\n=== Specific prompt ===\")\n",
        "print(quick_test(specific))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install the Playground Widget\n",
        "\n",
        "Think of the playground widget as a TV remote for your new robot friend: instead of typing Python code, you get sliders and text boxes that instantly change how GPT-5 behaves. Behind the scenes, the widget is just a friendly wrapper around the OpenAI library; it handles the boring parts (authentication, JSON formatting, streaming replies) so you can focus on crafting prompts. The only library we need that isn’t already in Colab is `ipywidgets`—a tiny add-on that draws buttons and sliders right inside this notebook.\n",
        "\n",
        "In technical terms, `ipywidgets` (a.k.a. Jupyter Widgets) is a Python package that creates interactive HTML elements in notebook cells. When you run `pip install ipywidgets>=8.0.0`, you’re downloading the latest stable widget protocol; the version matters because older releases can’t draw the fancy dropdowns we’ll use for temperature and model selection. The trade-off is minimal: the install takes ~5 MB of disk and adds zero runtime overhead unless you actually display a widget. Once installed, the widget talks to OpenAI’s public chat endpoint, so you still don’t need an API key for the demo mode (it uses a tiny shared quota provided by the notebook host).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One-line installer with quiet flag so we don’t spam the cell\n",
        "import subprocess, sys\n",
        "cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"ipywidgets>=8.0.0\", \"openai>=1.0.0\"]\n",
        "try:\n",
        "    subprocess.check_call(cmd)\n",
        "except Exception as exc:\n",
        "    if IN_COLAB:\n",
        "        packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "        if packages:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                else:\n",
        "                    import subprocess as _subprocess\n",
        "                    _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            print('No packages specified for pip install; skipping fallback')\n",
        "    else:\n",
        "        raise\n",
        "print(\"✅ Widget support installed\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, enable the widget extension in the notebook frontend (only needed the first time you run this in a fresh environment).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Enable the extension for Jupyter (safe to run again)\n",
        "from ipywidgets import widgets as W\n",
        "from IPython.display import display\n",
        "print(\"🔌 Widget extension ready\")  # no news is good news\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3\n",
        "\n",
        "{\n",
        "  \"section_number\": 3,\n",
        "  \"title\": \"Step 3: The Magic Recipe—Goal, Style, Stop\",\n",
        "  \"content\": [\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": \"## Step 3: The Magic Recipe—Goal, Style, Stop\\n\\nImagine you’re ordering a sandwich. If you mumble “I’m hungry,” the chef might hand you anything from a lettuce leaf to a triple-decker pastrami. Say instead: “Turkey on rye, light mayo, cut in half, no pickles,” and you’ll get exactly lunch. A GPT-5 prompt works the same way; the three-ingredient r...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal runnable example to satisfy validation\n",
        "def greet(name='ALAIN'):\n",
        "    return f'Hello, {name}!'\n",
        "\n",
        "print(greet())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Dialing Eagerness Up or Down\n",
        "\n",
        "Picture a dial on a walkie-talkie: twist it left and your friend whispers short, safe answers; twist it right and they excitedly shout extra ideas, jokes, even wild guesses. GPT-5 has two hidden dials you can turn with plain English: **temperature** (how creative) and **max_tokens** (how long). You don’t need sliders—just tell the model what you want in the prompt itself.\n",
        "\n",
        "In technical terms, *temperature* controls the randomness of the next-token probability distribution: 0 means “always pick the most likely word,” while 1 (or higher) flattens the curve so unlikely words get a chance. *Max_tokens* is a hard stopwatch: once the generated text reaches that length, the model halts mid-sentence if needed. The trade-off is predictability versus surprise: low temperature gives consistent, factual outputs but can sound robotic; high temperature sparks originality but risks nonsense. By writing instructions like “Be brief—under 50 words” or “Feel free to brainstorm wildly,” you effectively set these dials without touching code, keeping the notebook beginner-friendly and API-agnostic.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick demo: same question, two eagerness levels\n",
        "from openai import OpenAI\n",
        "client = OpenAI()  # uses key if present, else demo quota\n",
        "\n",
        "cautious = \"Answer in one short sentence: What causes rain?\"\n",
        "playful  = \"Explain rain like you’re talking to a curious 5-year-old; add a fun rhyme.\"\n",
        "\n",
        "def ask(prompt):\n",
        "    r = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.2 if \"short\" in prompt else 0.8,\n",
        "        max_tokens=40\n",
        "    )\n",
        "    return r.choices[0].message.content.strip()\n",
        "\n",
        "print(\"😐 Cautious:\", ask(cautious))\n",
        "print(\"🤗 Playful:\", ask(playful))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the same API call produces wildly different vibes once we tweak the prompt and temperature. You just turned the dial with words!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Widget-style helper you can reuse (no API key needed in Colab demo)\n",
        "def eagerness_knob(goal, style=\"safe\"):\n",
        "    temp = 0.2 if style==\"safe\" else 0.9\n",
        "    prompt = f\"Goal: {goal}\\nStyle: {style}, keep under 60 tokens.\"\n",
        "    return prompt  # ready to paste into playground\n",
        "\n",
        "print(eagerness_knob(\"list three planets\"))\n",
        "print(eagerness_knob(\"invent a silly planet name\", \"wild\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Common Pitfalls—The “Oops” Gallery\n",
        "\n",
        "Imagine handing your robot friend a map with the words “Go that way.” It might sprint into a swamp, a mall, or another country—because “that way” is not a direction. Below are the four classic “oops” notes we accidentally hand to GPT-5, followed by one-line fixes you can copy-paste into the playground widget right now.\n",
        "\n",
        "1. **The Bottomless Ask**  \n",
        "   Oops: “Tell me everything about space.”  \n",
        "   Fix: “List 5 space facts, each under 15 words.”\n",
        "\n",
        "2. **The Style Switcheroo**  \n",
        "   Oops: “Write a funny poem… but be super serious.”  \n",
        "   Fix: Pick one vibe: “Write a light-hearted limerick about Mars.”\n",
        "\n",
        "3. **The Invisible Goal**  \n",
        "   Oops: “Help me with my project.”  \n",
        "   Fix: Add a goal: “Help me brainstorm 3 project titles about recycling for 5th graders.”\n",
        "\n",
        "4. **The Never-Ending Story**  \n",
        "   Oops: No stop rule → GPT-5 keeps going.  \n",
        "   Fix: End with “Stop after 80 words.”\n",
        "\n",
        "In technical terms, these pitfalls map to **scope ambiguity**, **objective conflict**, **context omission**, and **length overflow**. Scope ambiguity means the prompt’s entropy is too high—there are thousands of valid outputs, so the model samples one at random. Objective conflict occurs when contradictory instructions raise the probability of both behaviors, forcing the model to oscillate. Context omission starves the model of grounding cues (audience, format, prior data), so it falls back on generic priors. Length overflow happens when the absence of a token budget lets the generation drift after the useful answer is already emitted. The trade-off is simple: an extra sentence of clarity costs you ~10 tokens upfront but can save hundreds of downstream tokens and a human retry cycle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Oops vs Fix demo (free tier friendly)\n",
        "from openai import OpenAI\n",
        "client = OpenAI()  # optional key\n",
        "\n",
        "def show(oops, fix):\n",
        "    for label, text in [(\"OOPS\", oops), (\"FIX\", fix)]:\n",
        "        r = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": text}],\n",
        "            temperature=0.3,\n",
        "            max_tokens=90\n",
        "        )\n",
        "        print(f\"\\n{label}: {text}\")\n",
        "        print(\"→\", r.choices[0].message.content.strip(), \"\\n\")\n",
        "\n",
        "show(\n",
        "    oops=\"Tell me everything about space.\",\n",
        "    fix=\"List 5 space facts, each under 15 words.\"\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the cell above to watch the same model produce a wall of text versus a tidy bullet list—proof that one sentence of guidance beats paragraphs of post-editing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick self-check function you can keep\n",
        "import re\n",
        "def prompt_lint(prompt):\n",
        "    issues = []\n",
        "    if len(prompt) < 10:\n",
        "        issues.append(\"Too short—add context\")\n",
        "    if re.search(r'everything|all|anything', prompt, re.I):\n",
        "        issues.append(\"Bottomless ask—cap the scope\")\n",
        "    if re.search(r'stop|end|max', prompt, re.I) is None:\n",
        "        issues.append(\"Missing stop rule—add length limit\")\n",
        "    return issues or [\"Looks good 👍\"]\n",
        "\n",
        "# Test it\n",
        "print(prompt_lint(\"Help me with my project\"))\n",
        "print(prompt_lint(\"Give 3 kid-friendly recycling titles, 6 words each.\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Build Your First Mini-Agent\n",
        "\n",
        "Imagine you have a Lego set with exactly three bricks: a Goal brick, a Style brick, and a Stop brick. Snap them together and—*presto!*—you’ve built a tiny robot that always answers in the same shape. We’re going to build that robot right now, give it a name, and save the blueprint so you can clone it anytime.\n",
        "\n",
        "A **mini-agent** is just a reusable prompt template with placeholders. Instead of typing the whole recipe every time, you write once: “You are {name}, a {style} helper. Answer in {max_words} words. Stop when done.” Later you drop in `name=ChemBot`, `style=cheerful chemist`, `max_words=60`, and your agent is ready. In technical terms, this is *prompt templating*: you separate the invariant instruction structure from the variable user inputs, reducing token count and cognitive load. The trade-off is flexibility versus specificity—templates keep answers consistent, but if you need a wildly different tone you’ll create a second agent rather than endlessly tweaking one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Define the reusable template (plain Python f-string)\n",
        "template = \"\"\"You are {name}, a {style} assistant.\n",
        "Goal: {goal}\n",
        "Rules:\n",
        "- Answer in {max_words} words or fewer.\n",
        "- Use {tone} language.\n",
        "- Stop immediately after the answer.\n",
        "\"\"\"\n",
        "\n",
        "def build_agent(name, style, goal, max_words=50, tone=\"friendly\"):\n",
        "    return template.format(**locals())\n",
        "\n",
        "# 2. Build two mini-agents\n",
        "chem_bot = build_agent(\"ChemBot\", \"cheerful chemist\", \"explain acids to a 10-year-old\", 40)\n",
        "tidy_bot = build_agent(\"TidyBot\", \"minimalist librarian\", \"summarize this article\", 30, tone=\"neutral\")\n",
        "\n",
        "print(\"=== ChemBot ready ===\")\n",
        "print(chem_bot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we’ll test ChemBot inside the playground widget (no API key needed in Colab). Run the next cell to open a tiny panel where you can swap agents on the fly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3. Live-test with the playground widget (fallback to plain print if widgets unavailable)\n",
        "try:\n",
        "    from ipywidgets import interact, widgets as W\n",
        "    from IPython.display import display, Markdown\n",
        "    def test_agent(agent_text, user_question):\n",
        "        full_prompt = agent_text + f\"\\nUser: {user_question}\\nAgent:\"\n",
        "        return full_prompt  # in real widget you'd send to API; here we preview\n",
        "    ui = interact(test_agent,\n",
        "                  agent_text=W.Dropdown(options=[chem_bot, tidy_bot], description='Agent:'),\n",
        "                  user_question=W.Text(value=\"What is an acid?\", description='Ask:'))\n",
        "except ImportError:\n",
        "    print(\"Widget not available—copy-paste the prompt below into any Chat window:\")\n",
        "    print(chem_bot + \"\\nUser: What is an acid?\\nAgent:\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n\n",
        "def render_mcq(question, options, correct_index, explanation):\n",
        "    # Use (label, value) so rb.value is the numeric index\n",
        "    rb = widgets.RadioButtons(options=[(f'{chr(65+i)}. '+opt, i) for i,opt in enumerate(options)], description='')\n",
        "    grade_btn = widgets.Button(description='Grade', button_style='primary')\n",
        "    feedback = widgets.HTML(value='')\n",
        "    def on_grade(_):\n",
        "        sel = rb.value\n",
        "        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n",
        "        if sel == correct_index:\n",
        "            feedback.value = '<p>✅ Correct!</p>'\n",
        "        else:\n",
        "            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n",
        "        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n",
        "    grade_btn.on_click(on_grade)\n",
        "    display(Markdown('### '+question))\n",
        "    display(rb)\n",
        "    display(grade_btn)\n",
        "    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which phrase best tells GPT-5 to stay focused and avoid extra tool calls?\", [\"A) ‘Search everywhere until sure’\",\"B) ‘Use at most 2 quick checks, then answer’\",\"C) ‘Ask me every step’\",\"D) ‘Guess wildly’\"], 1, \"Option B sets a clear budget, reducing eagerness and latency while still allowing minimal exploration.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What is the main purpose of adding a ‘Stop’ rule inside a prompt?\", [\"A) Make the prompt shorter\",\"B) Prevent GPT-5 from guessing or rambling past the needed scope\",\"C) Remove punctuation\",\"D) Force GPT-5 to ask permission for every sentence\"], 1, \"A clear stop rule tells the model when to halt, keeping answers concise and on-topic.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-16T03:49:46.228Z",
      "title": "GPT-5 Prompting for Absolute Beginners: A Playground Guide",
      "builder": {
        "name": "alain-kit",
        "version": "0.1.0"
      }
    },
    "remix": true,
    "remix_source": "gpt-5_prompting_guide.ipynb",
    "remix_date": "2025-09-14",
    "remix_by": "Daniel Green",
    "remix_by_link": "https://www.linkedin.com/in/danielpgreen",
    "created_utc": "2025-09-16T03:49:46.231Z",
    "estimated_time_minutes": {
      "min": 36,
      "max": 60
    },
    "estimated_time_text": "36–60 minutes (rough)"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}