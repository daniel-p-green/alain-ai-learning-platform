{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TinyLlama 1.1B Chat: From GGUF to Interactive Inference\n\nLearn how to load the TinyLlama‚Äë1.1B‚ÄëChat‚Äëv1.0 GGUF model from Hugging Face, run fast inference, customize prompts, and deploy a lightweight chat API. This lesson covers environment setup, model loading, generation, batching, and basic fine‚Äëtuning using the Hugging Face ecosystem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Install and configure the required Python packages for GGUF inference.\n",
        "2. Load the TinyLlama‚Äë1.1B‚ÄëChat model and tokenizer from the Hugging Face Hub.\n",
        "3. Generate text with custom prompts and control generation parameters.\n",
        "4. Deploy the model as a simple REST API for real‚Äëtime chat.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic Python programming (3.8+).\n",
        "- Familiarity with Hugging Face Transformers API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create requirements.txt\n",
        "requirements = '''transformers==4.35.0\ntorch==2.0.1\naccelerate==0.21.0\nhuggingface_hub==0.20.3\nfastapi==0.110.0\nuvicorn==0.29.0'''\n\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(requirements)\n\n",
        "print('‚úÖ requirements.txt created!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment & Dependencies\n\nWelcome to the first step of your TinyLlama adventure! Think of this section as setting up a tiny kitchen before you start cooking a fancy dish. We‚Äôll install the right tools (Python packages), make sure your computer can talk to the Hugging Face Hub, and set up a safe place to keep your credentials.\n\n**Why do we need all this?**\n- `transformers` gives us the recipe for TinyLlama.\n- `torch` is the engine that runs the math.\n- `accelerate` helps us use your GPU (if you have one) or CPU efficiently.\n- `huggingface_hub` lets us download the GGUF file.\n- `fastapi` and `uvicorn` are for the future chat API.\n\nWe‚Äôll also show you how to create a virtual environment so your project stays tidy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Create a virtual environment (recommended)\n# This keeps your project dependencies isolated.\n# If you already have one, skip to the pip install step.\n\nimport subprocess, sys\n\ntry:\n    subprocess.check_call([sys.executable, \"-m\", \"venv\", \"venv\"])\n    print(\"‚úÖ Virtual environment 'venv' created.\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è  Virtual environment creation failed:\", e)\n\n# 2Ô∏è‚É£ Activate the environment (Unix/macOS)\n#    source venv/bin/activate\n#    (Windows) venv\\Scripts\\activate.bat\n\n# 3Ô∏è‚É£ Install the required packages\n# We pin exact versions for reproducibility.\n\nrequirements = [\n    \"transformers==4.35.0\",\n    \"torch==2.0.1\",\n    \"accelerate==0.21.0\",\n    \"huggingface_hub==0.20.3\",\n    \"fastapi==0.110.0\",\n    \"uvicorn==0.29.0\"\n]\n\ntry:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *requirements])\n    print(\"‚úÖ All packages installed successfully.\")\nexcept subprocess.CalledProcessError as e:\n    print(\"‚ö†Ô∏è  Package installation failed. Try running pip manually.\")\n    sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Set up your Hugging Face token\n\nIf you‚Äôre downloading a private model, you‚Äôll need a **HF_TOKEN**. For public models like TinyLlama‚Äë1.1B‚ÄëChat‚Äëv1.0, it‚Äôs optional, but it speeds up downloads.\n\n**How to get one:**\n1. Sign in to [Hugging Face](https://huggingface.co/).\n2. Go to *Settings ‚Üí Access Tokens*.\n3. Click *New token*, give it a name, and copy the value.\n\n**Why store it in an environment variable?**\n- Keeps it out of your code.\n- Lets `huggingface_hub` automatically pick it up.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Export the token for Unix/macOS\n# Replace YOUR_HF_TOKEN with the string you copied.\n# You can add this line to ~/.bashrc or ~/.zshrc for persistence.\n\nimport os\n\nos.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN\"  # <-- replace this\nprint(\"‚úÖ HF_TOKEN set in environment (for this session).\")\n\n# For Windows PowerShell:\n#   $env:HF_TOKEN = \"YOUR_HF_TOKEN\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ You‚Äôre all set!\n\nRun the cells above in a Jupyter notebook or a Python script. If everything prints `‚úÖ`, you‚Äôre ready to load TinyLlama in the next section. If you hit any errors, double‚Äëcheck:\n- Python version (`python --version` should be 3.8+).\n- Internet connection.\n- Correct spelling of package names.\n\nHappy coding! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Understanding GGUF and TinyLlama\n\nBefore we can load TinyLlama, let‚Äôs demystify two key concepts:\n\n1. **GGUF** ‚Äì Think of a GGUF file as a *tiny, super‚Äëcompressed cookbook*. It stores the model‚Äôs weights in a binary format that‚Äôs smaller than the usual `.bin` or `.pt` files, but still contains all the information the model needs to talk.\n2. **TinyLlama‚Äë1.1B‚ÄëChat** ‚Äì This is a lightweight version of the Llama family, trimmed down to 1.1‚ÄØbillion parameters. It‚Äôs like a compact smartphone that still runs most apps, but uses less memory and CPU.\n\nWhy GGUF? The main benefits are:\n- **Speed** ‚Äì Loading is faster because the file is smaller.\n- **Memory** ‚Äì Less RAM is required to keep the model in memory.\n- **Portability** ‚Äì You can ship the model to edge devices or cloud functions with minimal bandwidth.\n\nBelow we‚Äôll inspect the GGUF file, check its size, and confirm that the Hugging Face Hub can handle it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Inspect the GGUF file size and metadata\n# We‚Äôll use the `huggingface_hub` library to download the file locally\n# and then read a few bytes to confirm it‚Äôs a GGUF file.\n\nimport os\nfrom huggingface_hub import hf_hub_download\n\n# Path where the GGUF will be stored\nmodel_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\nfilename = \"TinyLlama-1.1B-Chat-v1.0.Q4_K_M.gguf\"  # one of the quantized variants\n\n# Download (or use cached copy)\nlocal_path = hf_hub_download(repo_id=model_id, filename=filename)\nprint(f\"‚úÖ GGUF file downloaded to: {local_path}\")\n\n# Show file size in megabytes\nsize_mb = os.path.getsize(local_path) / (1024 ** 2)\nprint(f\"üì¶ File size: {size_mb:.2f}‚ÄØMB\")\n\n# Peek at the first few bytes ‚Äì GGUF files start with the ASCII string 'GGUF'\nwith open(local_path, \"rb\") as f:\n    header = f.read(4)\nprint(f\"üóÇÔ∏è Header bytes: {header}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What we just did\n- **`hf_hub_download`** pulls the file from Hugging Face and caches it locally.\n- We printed the file size to see how lightweight it is.\n- The header check confirms the file is indeed a GGUF binary.\n\nIf you see `b'GGUF'` in the header, you‚Äôre good to go!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2Ô∏è‚É£ Quick sanity check: load the model with transformers\n# This will load the GGUF file into memory. It may take a few seconds.\n# We‚Äôll use the `trust_remote_code=True` flag to allow the model class to be loaded.\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load tokenizer ‚Äì TinyLlama uses the Llama tokenizer\nprint(\"üîÑ Loading tokenizer‚Ä¶\")\nmodel_name = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n# The tokenizer is the same as the original Llama model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nprint(\"‚úÖ Tokenizer loaded.\")\n\n# Load the GGUF model\nprint(\"üîÑ Loading GGUF model‚Ä¶\")\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\nprint(\"‚úÖ GGUF model loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TL;DR\n- GGUF is a compact, binary format for model weights.\n- TinyLlama‚Äë1.1B‚ÄëChat is a 1.1‚ÄØbillion‚Äëparameter Llama variant, great for quick inference.\n- The Hugging Face Hub and `transformers` make it trivial to download and load these files.\n\nIn the next section we‚Äôll actually generate text with this model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the Model & Tokenizer\n\nNow that we‚Äôve downloaded the TinyLlama GGUF file, it‚Äôs time to bring it into memory. Think of the model as a *recipe book* and the tokenizer as the *chef‚Äôs measuring spoon*. The tokenizer turns your words into numbers the model can understand, and the model uses those numbers to predict the next word.\n\nWe‚Äôll load both components with a single line each, using the `transformers` library. The `trust_remote_code=True` flag tells Hugging Face that it‚Äôs okay to run the custom code that ships with TinyLlama.\n\nIf you‚Äôre on a machine with a GPU, the model will automatically use it. If not, it will fall back to the CPU.\n\nLet‚Äôs get started!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Load the tokenizer\n# The tokenizer is the same as the original Llama tokenizer\n# It converts text to token ids and back.\n\nfrom transformers import AutoTokenizer\n\nmodel_name = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        use_fast=True  # fast tokenizers are a bit faster\n    )\n    print(\"‚úÖ Tokenizer loaded successfully.\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è  Failed to load tokenizer:\", e)\n    raise\n\n# Quick sanity check: encode a simple sentence\nsample = \"Hello, TinyLlama!\"\nencoded = tokenizer.encode(sample, add_special_tokens=True)\nprint(\"üî¢ Token IDs:\", encoded)\nprint(\"üß© Decoded back:\", tokenizer.decode(encoded))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What just happened?\n- `AutoTokenizer.from_pretrained` pulls the tokenizer files from the Hugging Face Hub.\n- `add_special_tokens=True` adds the beginning‚Äëof‚Äësentence token that TinyLlama expects.\n- The quick encode/decode round‚Äëtrip confirms the tokenizer works.\n\nNow we‚Äôre ready to load the heavy‚Äëlifting part: the model itself.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2Ô∏è‚É£ Load the GGUF model\n# This step can take a few seconds, especially on a CPU.\n# If you have a GPU, the model will automatically use it.\n\nfrom transformers import AutoModelForCausalLM\nimport torch\n\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        device_map=\"auto\",  # let accelerate decide where to place layers\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n    )\n    print(\"‚úÖ GGUF model loaded successfully.\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è  Failed to load model:\", e)\n    raise\n\n# Verify that the model can run a quick forward pass\ninput_ids = tokenizer.encode(\"Tell me a joke.\", return_tensors=\"pt\")\nif torch.cuda.is_available():\n    input_ids = input_ids.to(\"cuda\")\n\nwith torch.no_grad():\n    outputs = model.generate(input_ids, max_new_tokens=20)\n\nprint(\"ü§ñ Model output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why `device_map=\"auto\"`?\n- On a GPU, the model‚Äôs layers are split across memory to avoid overflow.\n- On a CPU, it keeps everything in RAM but still uses the most efficient layout.\n\nIf you want to force the model onto a specific device, you can replace `device_map=\"auto\"` with `device_map=\"cpu\"` or `device_map=\"cuda:0\"`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check\n\nTest your understanding with these questions:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1\n\n",
        "Which library is required to load GGUF models in Hugging Face Transformers?\n\n",
        "A. torch\n",
        "B. transformers\n",
        "C. accelerate\n",
        "D. huggingface_hub\n",
        "\n**Answer:** B\n\n",
        "**Explanation:** The `transformers` library provides the `AutoModelForCausalLM.from_pretrained` method that supports GGUF format via the `trust_remote_code=True` flag.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}