{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- \n",
        "Generated by ALAIN (Applied Learning AI Notebooks) on 2025-09-14\n",
        "Teacher Model: GPT-OSS-20B\n",
        "Provider: poe\n",
        "Target Model: gpt-oss-20b\n",
        "Learn more: https://github.com/daniel-p-green/alain-ai-learning-platform/\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TinyLlama GGUF: Your First Small Language Model\n",
        "\n",
        "Learn how to use TinyLlama, a tiny but powerful language model in GGUF format. Perfect for beginners who want to run AI on their own computer!\n",
        "\n",
        "> Provider: `poe`  •  Model: `gpt-oss-20b`\n",
        "Runtime: poe\n",
        "\n",
        "This notebook was generated by ALAIN. It calls AI models via OpenAI-compatible APIs (no arbitrary code).\n",
        "\n",
        "---\n",
        "\n",
        "### Reproducibility Tips\n",
        "- Avoid network access in core cells.\n",
        "- Seed randomness where applicable (e.g., numpy, random).\n",
        "- Pin package versions in your own environment if needed.\n",
        "- Set `OPENAI_BASE_URL` and `OPENAI_API_KEY` via env (or Colab userdata).\n",
        "- Widgets optional: text-based MCQs are provided if widgets are unavailable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create requirements.txt for reproducible environment\n",
        "%%writefile requirements.txt\n",
        "openai>=1.34.0\n",
        "ipywidgets>=8.0.0\n",
        "requests>=2.31.0\n",
        "python-dotenv>=1.0.0\n",
        "numpy>=1.24.0\n",
        "pandas>=2.0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e2f098",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "41047462",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create .env.example template\n",
        "%%writefile .env.example\n",
        "# Copy this file to .env and fill in your actual values\n",
        "OPENAI_API_KEY=your_api_key_here\n",
        "OPENAI_BASE_URL=https://api.poe.com/v1\n",
        "POE_API_KEY=your_poe_key_here\n",
        "# For local models (LM Studio/Ollama), any non-empty string works for API_KEY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies from requirements.txt\n",
        "!pip install -q -r requirements.txt\n",
        "print('✅ Dependencies installed successfully')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure OpenAI-compatible client\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from getpass import getpass\n",
        "# Load environment variables from .env file if it exists\n",
        "load_dotenv()\n",
        "# Try to read secrets from Colab userdata if available\n",
        "try:\n",
        "  from google.colab import userdata  # type: ignore\n",
        "  _poe = userdata.get('POE_API_KEY')\n",
        "  _openai = userdata.get('OPENAI_API_KEY')\n",
        "except Exception:\n",
        "  _poe = None; _openai = None\n",
        "PROVIDER = \"poe\"  # \"poe\" or \"openai-compatible\"\n",
        "os.environ.setdefault(\"OPENAI_BASE_URL\", \"https://api.poe.com/v1\")\n",
        "# Set your API key. For Poe, set POE_API_KEY; for local (LM Studio/Ollama) any non-empty string works.\n",
        "os.environ.setdefault(\"OPENAI_API_KEY\", _poe or _openai or os.getenv(\"POE_API_KEY\") or os.getenv(\"OPENAI_API_KEY\") or \"\")\n",
        "# Local-friendly defaults to avoid prompting beginners\n",
        "if (PROVIDER == 'openai-compatible') and not os.environ.get('OPENAI_API_KEY'):\n",
        "  base = os.environ.get('OPENAI_BASE_URL','')\n",
        "  if 'localhost:1234' in base or '127.0.0.1:1234' in base:\n",
        "    os.environ['OPENAI_API_KEY'] = 'lm-studio'\n",
        "  elif 'localhost:11434' in base or '127.0.0.1:11434' in base:\n",
        "    os.environ['OPENAI_API_KEY'] = 'ollama'\n",
        "# Fallback interactive prompt if still missing\n",
        "if not os.environ.get('OPENAI_API_KEY'):\n",
        "  os.environ['OPENAI_API_KEY'] = getpass('Enter API key (input hidden): ')\n",
        "# OPENAI_BASE_URL and OPENAI_API_KEY environment variables are set above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-flight check: verify API connectivity\n",
        "from openai import OpenAI\n",
        "import os, sys\n",
        "base = os.environ.get('OPENAI_BASE_URL')\n",
        "key = os.environ.get('OPENAI_API_KEY')\n",
        "if not base or not key:\n",
        "    print('❌ Missing OPENAI_BASE_URL or OPENAI_API_KEY. Set them above.')\n",
        "else:\n",
        "    try:\n",
        "        client = OpenAI(base_url=base, api_key=key)\n",
        "        # lightweight call: list models or small completion\n",
        "        ok = False\n",
        "        try:\n",
        "            _ = client.models.list()\n",
        "            ok = True\n",
        "        except Exception:\n",
        "            # Fallback to a 1-token chat call\n",
        "            _ = client.chat.completions.create(model=\"${meta.model}\", messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "            ok = True\n",
        "        if ok:\n",
        "            print('✅ API key is working and connected to provider.')\n",
        "    except Exception as e:\n",
        "        print('❌ Connection failed. Please check your API key and base URL.\\n', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick smoke test\n",
        "from openai import OpenAI\n",
        "import os\n",
        "base = os.environ.get('OPENAI_BASE_URL')\n",
        "key = os.environ.get('OPENAI_API_KEY')\n",
        "assert base and key, 'Please set OPENAI_BASE_URL and OPENAI_API_KEY env vars'\n",
        "client = OpenAI(base_url=base, api_key=key)\n",
        "resp = client.chat.completions.create(model=\"gpt-oss-20b\", messages=[{\"role\":\"user\",\"content\":\"Hello from ALAIN\"}], max_tokens=32)\n",
        "print(resp.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: What is TinyLlama GGUF?\n",
        "\n",
        "TinyLlama is like having a mini AI assistant that can run on your laptop! GGUF is a special file format that makes the model smaller and faster - think of it like compressing a huge movie file so it fits on your phone. This model has 1.1 billion parameters (that's like 1.1 billion tiny brain connections) but only takes about 1GB of space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the step prompt using the configured provider\n",
        "PROMPT = \"\"\"\n",
        "# Let's check if we have the requirements\n",
        "import os\n",
        "print('Python version:', os.sys.version)\n",
        "print('Ready to download TinyLlama!')\n",
        "\"\"\"\n",
        "from openai import OpenAI\n",
        "import os\n",
        "client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "resp = client.chat.completions.create(model=\"gpt-oss-20b\", messages=[{\"role\":\"user\",\"content\":PROMPT}], temperature=0.7, max_tokens=400)\n",
        "print(resp.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assessment for Step 1\n",
        "question = \"What does GGUF format do for AI models?\"\n",
        "options = [\"Makes models smaller and faster to load\",\"Makes models bigger and more accurate\",\"Changes the model's personality\",\"Converts text to images\"]\n",
        "correct_index = 0\n",
        "print('Q:', question)\n",
        "for i, o in enumerate(options):\n",
        "    print(f\"{i}. {o}\")\n",
        "choice = 0  # <- change this to your answer index\n",
        "print('Correct!' if choice == correct_index else 'Incorrect')\n",
        "print('Explanation:', \"GGUF format compresses AI models to make them smaller and faster to load, just like how ZIP files make regular files smaller. This lets you run powerful AI models on regular computers instead of needing expensive servers.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive quiz for Step 1\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "q = \"What does GGUF format do for AI models?\"\n",
        "opts = [\"Makes models smaller and faster to load\",\"Makes models bigger and more accurate\",\"Changes the model's personality\",\"Converts text to images\"]\n",
        "correct = 0\n",
        "rb = widgets.RadioButtons(options=[(o, i) for i, o in enumerate(opts)], description='', disabled=False)\n",
        "btn = widgets.Button(description='Submit Answer')\n",
        "out = widgets.Output()\n",
        "def on_click(b):\n",
        "  with out:\n",
        "    out.clear_output()\n",
        "    sel = rb.value if hasattr(rb, 'value') else 0\n",
        "    if sel == correct:\n",
        "      display(Markdown('**Correct!**' + ' — ' + \"GGUF format compresses AI models to make them smaller and faster to load, just like how ZIP files make regular files smaller. This lets you run powerful AI models on regular computers instead of needing expensive servers.\"))\n",
        "    else:\n",
        "      display(Markdown('Incorrect, please try again.'))\n",
        "btn.on_click(on_click)\n",
        "display(Markdown(f\"### {q}\"))\n",
        "display(rb, btn, out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assessment for Step 1\n",
        "question = \"How many parameters does TinyLlama have?\"\n",
        "options = [\"1.1 million parameters\",\"1.1 billion parameters\",\"1.1 trillion parameters\",\"1.1 thousand parameters\"]\n",
        "correct_index = 1\n",
        "print('Q:', question)\n",
        "for i, o in enumerate(options):\n",
        "    print(f\"{i}. {o}\")\n",
        "choice = 0  # <- change this to your answer index\n",
        "print('Correct!' if choice == correct_index else 'Incorrect')\n",
        "print('Explanation:', \"TinyLlama has 1.1 billion parameters. Think of parameters like brain connections - more connections usually mean smarter responses, but TinyLlama proves that even 'small' models with 1.1 billion connections can be very useful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive quiz for Step 1\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "q = \"How many parameters does TinyLlama have?\"\n",
        "opts = [\"1.1 million parameters\",\"1.1 billion parameters\",\"1.1 trillion parameters\",\"1.1 thousand parameters\"]\n",
        "correct = 1\n",
        "rb = widgets.RadioButtons(options=[(o, i) for i, o in enumerate(opts)], description='', disabled=False)\n",
        "btn = widgets.Button(description='Submit Answer')\n",
        "out = widgets.Output()\n",
        "def on_click(b):\n",
        "  with out:\n",
        "    out.clear_output()\n",
        "    sel = rb.value if hasattr(rb, 'value') else 0\n",
        "    if sel == correct:\n",
        "      display(Markdown('**Correct!**' + ' — ' + \"TinyLlama has 1.1 billion parameters. Think of parameters like brain connections - more connections usually mean smarter responses, but TinyLlama proves that even 'small' models with 1.1 billion connections can be very useful!\"))\n",
        "    else:\n",
        "      display(Markdown('Incorrect, please try again.'))\n",
        "btn.on_click(on_click)\n",
        "display(Markdown(f\"### {q}\"))\n",
        "display(rb, btn, out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Installing Requirements\n",
        "\n",
        "Before we can use TinyLlama, we need to install some helper tools. Think of these like apps you need on your phone before you can use other apps. We'll use llama-cpp-python which is like a translator that helps Python talk to our AI model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the step prompt using the configured provider\n",
        "PROMPT = \"\"\"\n",
        "# Install the required packages\n",
        "!pip install llama-cpp-python requests\n",
        "print('✅ Installation complete!')\n",
        "\"\"\"\n",
        "from openai import OpenAI\n",
        "import os\n",
        "client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "resp = client.chat.completions.create(model=\"gpt-oss-20b\", messages=[{\"role\":\"user\",\"content\":PROMPT}], temperature=0.7, max_tokens=400)\n",
        "print(resp.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assessment for Step 2\n",
        "question = \"What is llama-cpp-python used for?\"\n",
        "options = [\"Creating new AI models from scratch\",\"Converting text to speech\",\"Helping Python communicate with GGUF models\",\"Training models on your data\"]\n",
        "correct_index = 2\n",
        "print('Q:', question)\n",
        "for i, o in enumerate(options):\n",
        "    print(f\"{i}. {o}\")\n",
        "choice = 0  # <- change this to your answer index\n",
        "print('Correct!' if choice == correct_index else 'Incorrect')\n",
        "print('Explanation:', \"llama-cpp-python is like a translator that helps Python programs talk to AI models in GGUF format. Without it, Python wouldn't know how to load and use these special model files.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive quiz for Step 2\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "q = \"What is llama-cpp-python used for?\"\n",
        "opts = [\"Creating new AI models from scratch\",\"Converting text to speech\",\"Helping Python communicate with GGUF models\",\"Training models on your data\"]\n",
        "correct = 2\n",
        "rb = widgets.RadioButtons(options=[(o, i) for i, o in enumerate(opts)], description='', disabled=False)\n",
        "btn = widgets.Button(description='Submit Answer')\n",
        "out = widgets.Output()\n",
        "def on_click(b):\n",
        "  with out:\n",
        "    out.clear_output()\n",
        "    sel = rb.value if hasattr(rb, 'value') else 0\n",
        "    if sel == correct:\n",
        "      display(Markdown('**Correct!**' + ' — ' + \"llama-cpp-python is like a translator that helps Python programs talk to AI models in GGUF format. Without it, Python wouldn't know how to load and use these special model files.\"))\n",
        "    else:\n",
        "      display(Markdown('Incorrect, please try again.'))\n",
        "btn.on_click(on_click)\n",
        "display(Markdown(f\"### {q}\"))\n",
        "display(rb, btn, out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assessment for Step 2\n",
        "question = \"What happens when you set temperature=0.7 in the model?\"\n",
        "options = [\"The model gets physically warmer\",\"The model responds more creatively and randomly\",\"The model responds more predictably and safely\",\"The model runs 70% faster\"]\n",
        "correct_index = 1\n",
        "print('Q:', question)\n",
        "for i, o in enumerate(options):\n",
        "    print(f\"{i}. {o}\")\n",
        "choice = 0  # <- change this to your answer index\n",
        "print('Correct!' if choice == correct_index else 'Incorrect')\n",
        "print('Explanation:', \"Temperature controls creativity! Higher temperature (like 0.7) makes the AI more creative and unpredictable, like a creative writer. Lower temperature (like 0.1) makes it more focused and predictable, like a careful scientist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive quiz for Step 2\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "q = \"What happens when you set temperature=0.7 in the model?\"\n",
        "opts = [\"The model gets physically warmer\",\"The model responds more creatively and randomly\",\"The model responds more predictably and safely\",\"The model runs 70% faster\"]\n",
        "correct = 1\n",
        "rb = widgets.RadioButtons(options=[(o, i) for i, o in enumerate(opts)], description='', disabled=False)\n",
        "btn = widgets.Button(description='Submit Answer')\n",
        "out = widgets.Output()\n",
        "def on_click(b):\n",
        "  with out:\n",
        "    out.clear_output()\n",
        "    sel = rb.value if hasattr(rb, 'value') else 0\n",
        "    if sel == correct:\n",
        "      display(Markdown('**Correct!**' + ' — ' + \"Temperature controls creativity! Higher temperature (like 0.7) makes the AI more creative and unpredictable, like a creative writer. Lower temperature (like 0.1) makes it more focused and predictable, like a careful scientist.\"))\n",
        "    else:\n",
        "      display(Markdown('Incorrect, please try again.'))\n",
        "btn.on_click(on_click)\n",
        "display(Markdown(f\"### {q}\"))\n",
        "display(rb, btn, out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Downloading TinyLlama\n",
        "\n",
        "Now we'll download the actual AI model. It's like downloading a smart app that can chat with you. The file will be about 1GB, so it might take a few minutes depending on your internet speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the step prompt using the configured provider\n",
        "PROMPT = \"\"\"\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download TinyLlama GGUF model\n",
        "url = 'https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf'\n",
        "filename = 'tinyllama.gguf'\n",
        "\n",
        "if not Path(filename).exists():\n",
        "    print('Downloading TinyLlama...')\n",
        "    response = requests.get(url)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print('✅ Download complete!')\n",
        "else:\n",
        "    print('✅ TinyLlama already downloaded!')\n",
        "\"\"\"\n",
        "from openai import OpenAI\n",
        "import os\n",
        "client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "resp = client.chat.completions.create(model=\"gpt-oss-20b\", messages=[{\"role\":\"user\",\"content\":PROMPT}], temperature=0.7, max_tokens=400)\n",
        "print(resp.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Your First Chat with TinyLlama\n",
        "\n",
        "Time for the fun part! We'll load our AI model and have a conversation. It's like waking up your AI assistant and asking it questions. The model will generate responses based on what it learned during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the step prompt using the configured provider\n",
        "PROMPT = \"\"\"\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model\n",
        "llm = Llama(model_path='tinyllama.gguf', n_ctx=2048)\n",
        "\n",
        "# Have a conversation\n",
        "prompt = 'Hello! Can you tell me a joke?'\n",
        "response = llm(prompt, max_tokens=100, temperature=0.7)\n",
        "\n",
        "print('You:', prompt)\n",
        "print('TinyLlama:', response['choices'][0]['text'])\n",
        "\"\"\"\n",
        "from openai import OpenAI\n",
        "import os\n",
        "client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "resp = client.chat.completions.create(model=\"gpt-oss-20b\", messages=[{\"role\":\"user\",\"content\":PROMPT}], temperature=0.7, max_tokens=400)\n",
        "print(resp.choices[0].message.content)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "model": "gpt-oss-20b",
    "provider": "poe",
    "teacher_downgraded": false,
    "teacher_model_used": "GPT-OSS-20B",
    "title": "TinyLlama GGUF: Your First Small Language Model"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
