{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT‑OSS 20B — Active Learning Notebook (from HF link)\n",
        "\n",
        "Model: https://huggingface.co/openai/gpt-oss-20b\n",
        "\n",
        "[Open in Colab](https://colab.research.google.com/github/YOUR_ORG/YOUR_REPO/***REMOVED***/main/alain-ai-learning-platform/docs/examples/gpt-oss-20b_active_learning.ipynb)\n",
        "\n",
        "- Outcomes: Engage with GPT‑OSS 20B locally via OpenAI‑compatible API, with MCQs, a golden‑set evaluation, and token/latency logging.\n",
        "- Time: ~45–75 minutes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f64831",
      "metadata": {},
      "source": [
        "## About OpenAI and GPT‑OSS 20B\n",
        "This lesson uses GPT‑OSS 20B as an open‑weights model for local experimentation and teaching. It pairs well with an OpenAI‑compatible API (e.g., Ollama or vLLM) for fast, local chat.\n",
        "\n",
        "- Company: OpenAI (research and deployment of AI systems)\n",
        "- Model: GPT‑OSS 20B (open‑weights model used in ALAIN for local, reproducible teaching)\n",
        "- Release date: <add when known>\n",
        "- Why interesting: local, cost‑aware experimentation; deterministic evaluations; strong teaching fit.\n",
        "\n",
        "See the model card for details: https://huggingface.co/openai/gpt-oss-20b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d570bb29",
      "metadata": {},
      "source": [
        "### Optional: Fetch Model Card Summary\n",
        "Fetches the model card (if network is available) to display context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7f3c13",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import requests\n",
        "    url = 'https://huggingface.co/openai/gpt-oss-20b/raw/main/README.md'\n",
        "    r = requests.get(url, timeout=8)\n",
        "    if r.status_code == 200:\n",
        "        text = r.text[:2000]\n",
        "        print('--- Model Card (first 2k chars) ---\\n')\n",
        "        print(text)\n",
        "    else:\n",
        "        print('Could not fetch model card:', r.status_code)\n",
        "except Exception as e:\n",
        "    print('Offline or blocked; skip model card. Error:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d5d9112",
      "metadata": {},
      "source": [
        "## Parameters (Colab form)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc13f05",
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Model and Runtime\n",
        "HF_MODEL = 'openai/gpt-oss-20b' #@param {type:'string'}\n",
        "RUNTIME = 'gpt-oss' #@param ['gpt-oss','transformers']\n",
        "GPT_OSS_MODEL = 'gpt-oss:20b' #@param {type:'string'}\n",
        "OPENAI_BASE_URL = 'http://localhost:11434/v1' #@param {type:'string'}\n",
        "TEMPERATURE = 0.0 #@param {type:'number'}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074bb7e1",
      "metadata": {},
      "source": [
        "## Install (pinned)\n",
        "If running on Colab, uncomment and run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9befec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install openai==1.43.0 transformers==4.44.2 datasets==2.20.0 ipywidgets==8.1.3 requests==2.32.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68f29a9",
      "metadata": {},
      "source": [
        "## Setup & Seeds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b3fa157",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, platform, random, time\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "\n",
        "import numpy as np\n",
        "SEED=42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        print('CUDA device:', torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "        print('CUDA not available')\n",
        "except Exception as e:\n",
        "    print('Torch not installed; skipping torch seed.', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94df939d",
      "metadata": {},
      "source": [
        "## Secrets\n",
        "Keys are read from environment if needed; no hardcoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de7f445c",
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'ollama')\n",
        "print('Have OPENAI_API_KEY:', bool(OPENAI_API_KEY))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cec021ab",
      "metadata": {},
      "source": [
        "## Quickstart (GPT‑OSS local via OpenAI‑compatible API)\n",
        "Ensure you have an OpenAI‑compatible server (e.g., Ollama) with `gpt-oss:20b` available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e63eeac",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(base_url=OPENAI_BASE_URL, api_key=OPENAI_API_KEY)\n",
        "\n",
        "def chat(prompt: str, model: str = None, temperature: float = None):\n",
        "    model = model or GPT_OSS_MODEL\n",
        "    temperature = temperature if temperature is not None else TEMPERATURE\n",
        "    t0 = time.time()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{'role':'user','content':prompt}],\n",
        "        temperature=temperature\n",
        "    )\n",
        "    dt = time.time()-t0\n",
        "    txt = resp.choices[0].message.content\n",
        "    usage = getattr(resp,'usage',None)\n",
        "    print(txt)\n",
        "    if usage:\n",
        "        print('Tokens total:', getattr(usage,'total_tokens',None))\n",
        "    print(f'Latency: {dt:.2f}s')\n",
        "    return txt\n",
        "\n",
        "_ = chat('Say hello in five words.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5faed292",
      "metadata": {},
      "source": [
        "## Guided Steps\n",
        "1. Explore core parameters (temperature, max tokens).\n",
        "2. Add a structured output (JSON) or schema validation.\n",
        "3. Run a small batch; measure latency and (if exposed) tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a27bde",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pydantic schema hint (optional)\n",
        "try:\n",
        "    from pydantic import BaseModel\n",
        "    class Item(BaseModel):\n",
        "        title: str\n",
        "        rating: int\n",
        "    print('Use Item.model_json_schema() to prompt for strict JSON and validate outputs.')\n",
        "except Exception:\n",
        "    print('Install pydantic to validate structured outputs.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13148e12",
      "metadata": {},
      "source": [
        "## Evaluation (Golden Set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad5eb02",
      "metadata": {},
      "outputs": [],
      "source": [
        "golden = [\n",
        "    {'prompt':'2+2?','expect':'4'},\n",
        "    {'prompt':'Capital of France?','expect':'Paris'},\n",
        "]\n",
        "ok=0\n",
        "for ex in golden:\n",
        "    out = chat(ex['prompt'])\n",
        "    ok += int(ex['expect'].lower() in (out or '').lower())\n",
        "acc = ok/len(golden)\n",
        "print(f'Accuracy: {acc:.2%} ({ok}/{len(golden)})')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ecffb6b",
      "metadata": {},
      "source": [
        "## MCQ — Understanding Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "710b56eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "question = 'Which parameter most reduces randomness?'\n",
        "options = ['top_p','temperature','max_tokens','presence_penalty']\n",
        "correct_index = 1\n",
        "explanation = 'Lower temperature yields more deterministic outputs.'\n",
        "print(question)\n",
        "for i,o in enumerate(options):\n",
        "    print(f'  {i}) {o}')\n",
        "try:\n",
        "    import ipywidgets as W\n",
        "    from IPython.display import display\n",
        "    dd = W.Dropdown(options=[(o,i) for i,o in enumerate(options)], description='Answer:')\n",
        "    btn = W.Button(description='Submit')\n",
        "    out = W.Output()\n",
        "    def on_click(_):\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            print('Correct!' if dd.value==correct_index else f'Not quite. {explanation}')\n",
        "    btn.on_click(on_click)\n",
        "    display(dd, btn, out)\n",
        "except Exception:\n",
        "    choice = int(input('Your choice (0-3): ').strip() or -1)\n",
        "    print('Correct!' if choice==correct_index else f'Not quite. {explanation}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a589611",
      "metadata": {},
      "source": [
        "## MCQ — Tokens & Costs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b85180d",
      "metadata": {},
      "outputs": [],
      "source": [
        "question = 'Which fields commonly indicate token usage in OpenAI-compatible responses?'\n",
        "options = ['usage.prompt_tokens & usage.completion_tokens', 'num_tokens & token_count', 'price.prompt & price.completion', 'gpu_tokens & cpu_tokens']\n",
        "correct_index = 0\n",
        "explanation = 'Look for usage.prompt_tokens, usage.completion_tokens, and sometimes total_tokens.'\n",
        "print(question)\n",
        "for i,o in enumerate(options):\n",
        "    print(f'  {i}) {o}')\n",
        "choice = int(input('Your choice (0-3): ').strip() or -1)\n",
        "print('Correct!' if choice==correct_index else f'Not quite. {explanation}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf5a6f93",
      "metadata": {},
      "source": [
        "## MCQ — Secrets & Safety\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dda40cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "question = 'What is the safest way to use API keys in notebooks?'\n",
        "options = ['Hardcode in code cells', 'Store in environment variables and read with os.getenv', 'Embed in the prompt', 'Commit to git and rotate later']\n",
        "correct_index = 1\n",
        "explanation = 'Read secrets from environment variables; never hardcode or commit.'\n",
        "print(question)\n",
        "for i,o in enumerate(options):\n",
        "    print(f'  {i}) {o}')\n",
        "choice = int(input('Your choice (0-3): ').strip() or -1)\n",
        "print('Correct!' if choice==correct_index else f'Not quite. {explanation}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84540235",
      "metadata": {},
      "source": [
        "## Customization Playground\n",
        "Experiment with prompt and parameters interactively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa65bc8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import ipywidgets as W\n",
        "    from IPython.display import display\n",
        "    prompt_w = W.Text(value='Summarize this lesson in 20 words.', description='Prompt:', layout=W.Layout(width='90%'))\n",
        "    temp_w = W.FloatSlider(value=float(TEMPERATURE), min=0.0, max=1.0, step=0.1, description='Temp')\n",
        "    btn = W.Button(description='Run')\n",
        "    out = W.Output()\n",
        "    def on_click(_):\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            txt = chat(prompt_w.value) if RUNTIME=='gpt-oss' else 'Switch RUNTIME to transformers if needed.'\n",
        "            print('--- Output ---')\n",
        "            print(txt)\n",
        "    btn.on_click(on_click)\n",
        "    display(prompt_w, temp_w, btn, out)\n",
        "except Exception as e:\n",
        "    print('Widgets unavailable; use chat(\\'...\\') directly.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfda3586",
      "metadata": {},
      "source": [
        "## Cost & Observability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d41c1c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = ['List 3 cities in 5 words','Name 3 fruits in 5 words']\n",
        "t0=time.time(); outs=[]\n",
        "for p in prompts:\n",
        "    outs.append(chat(p))\n",
        "dt=time.time()-t0\n",
        "print('Batch latency:', round(dt,2),'s for', len(prompts),'items')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c5d6e1a",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "- Add a JSON schema and validate outputs.\n",
        "- Expand the golden set to 20 items and report accuracy.\n",
        "- Log average latency over 10 trials at two temperatures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b434776",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What changed when you varied temperature from 0.0 to 0.8?\n",
        "- How would you adapt the prompt for structured JSON output?\n",
        "- Which exercises would reveal model limitations most clearly?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "- Connection errors: verify `OPENAI_BASE_URL` and that your server exposes `/models`.\n",
        "- 401/403: check keys/permissions; never hardcode secrets.\n",
        "- OOM: reduce sequence length/batch; switch to CPU or smaller model.\n",
        "- JSON parse errors: strip code fences; validate/repair before loads.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostics\n",
        "import requests\n",
        "try:\n",
        "    r = requests.get(OPENAI_BASE_URL + '/models', timeout=5)\n",
        "    print(r.status_code, r.text[:200])\n",
        "except Exception as e:\n",
        "    print('Conn error:', e)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
