{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT‑OSS Active Learning Template — From HF Link to Lesson\n",
        "\n",
        "[Open in Colab](https://colab.research.google.com/github/your-org/your-repo/blob/main/path/to/this.ipynb)\n",
        "\n",
        "- Outcomes: Turn a Hugging Face model link into an engaging lesson with minimal setup, MCQs, a golden‑set evaluation, and token/latency logging.\n",
        "- Audience: Practitioners evaluating or teaching local/open models.\n",
        "- Time: ~45–75 minutes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters (Colab form)\n",
        "Provide model and runtime settings.\n",
        "\n",
        "(When using Colab, this cell appears as a form; values are stored in variables.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Model and Runtime\n",
        "HF_MODEL = 'org/name' #@param {type:'string'}\n",
        "RUNTIME = 'gpt-oss' #@param ['gpt-oss','transformers']\n",
        "GPT_OSS_MODEL = 'gpt-oss:20b' #@param {type:'string'}\n",
        "OPENAI_BASE_URL = 'http://localhost:11434/v1' #@param {type:'string'}\n",
        "TEMPERATURE = 0.0 #@param {type:'number'}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install (pinned)\n",
        "If running on Colab, uncomment and run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install openai==1.43.0 transformers==4.44.2 datasets==2.20.0 ipywidgets==8.1.3 requests==2.32.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Seeds\n",
        "Environment info, seeds, and device checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, platform, random, time\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "\n",
        "import numpy as np\n",
        "SEED=42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        print('CUDA device:', torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "        print('CUDA not available')\n",
        "except Exception as e:\n",
        "    print('Torch not installed; skipping torch seed.', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Secrets\n",
        "Keys are read from environment if needed; no hardcoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'ollama')\n",
        "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
        "print('Have OPENAI_API_KEY:', bool(OPENAI_API_KEY))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quickstart\n",
        "Run a smallest working example via GPT‑OSS or Transformers based on `RUNTIME`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_oss(prompt: str, model: str = None, temperature: float = None):\n",
        "    from openai import OpenAI\n",
        "    model = model or GPT_OSS_MODEL\n",
        "    temperature = temperature if temperature is not None else TEMPERATURE\n",
        "    client = OpenAI(base_url=OPENAI_BASE_URL, api_key=OPENAI_API_KEY)\n",
        "    t0 = time.time()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{'role':'user','content':prompt}],\n",
        "        temperature=temperature\n",
        "    )\n",
        "    dt = time.time()-t0\n",
        "    txt = resp.choices[0].message.content\n",
        "    usage = getattr(resp,'usage',None)\n",
        "    print(txt)\n",
        "    if usage:\n",
        "        print('Tokens total:', getattr(usage,'total_tokens',None))\n",
        "    print(f'Latency: {dt:.2f}s')\n",
        "    return txt\n",
        "\n",
        "def infer_transformers(inp: str):\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(HF_MODEL)\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(HF_MODEL)\n",
        "        gen = pipeline('text-generation', model=mdl, tokenizer=tok)\n",
        "        out = gen(inp, max_new_tokens=32)[0]['generated_text']\n",
        "        print(out)\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        print('Transformers fallback failed:', e)\n",
        "        return ''\n",
        "\n",
        "_ = chat_oss('Say hello in five words.') if RUNTIME=='gpt-oss' else infer_transformers('Hello, world')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Guided Steps\n",
        "1. Explore core parameters (e.g., temperature, max tokens).\n",
        "2. Add a structured output (JSON) or schema validation.\n",
        "3. Run a small batch; measure latency and (if exposed) tokens.\n",
        "4. Optional: simple RAG/fine‑tune illustration for this model family.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Structured output hint (Pydantic)\n",
        "try:\n",
        "    from pydantic import BaseModel\n",
        "    class Item(BaseModel):\n",
        "        title: str\n",
        "        rating: int\n",
        "    print('Define JSON schema with Item.model_json_schema() and validate outputs.')\n",
        "except Exception:\n",
        "    print('Install pydantic to validate structured outputs.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation (Golden Set)\n",
        "Deterministic checks for success criteria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "golden = [\n",
        "    {'prompt':'2+2?','expect':'4'},\n",
        "    {'prompt':'Capital of France?','expect':'Paris'},\n",
        "]\n",
        "ok=0\n",
        "for ex in golden:\n",
        "    out = chat_oss(ex['prompt']) if RUNTIME=='gpt-oss' else infer_transformers(ex['prompt'])\n",
        "    ok += int(ex['expect'].lower() in (out or '').lower())\n",
        "acc = ok/len(golden)\n",
        "print(f'Accuracy: {acc:.2%} ({ok}/{len(golden)})')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MCQ — Understanding Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = 'Which parameter most reduces randomness?'\n",
        "options = ['top_p','temperature','max_tokens','presence_penalty']\n",
        "correct_index = 1\n",
        "explanation = 'Lower temperature yields more deterministic outputs.'\n",
        "print(question)\n",
        "for i,o in enumerate(options):\n",
        "    print(f'  {i}) {o}')\n",
        "try:\n",
        "    import ipywidgets as W\n",
        "    from IPython.display import display\n",
        "    dd = W.Dropdown(options=[(o,i) for i,o in enumerate(options)], description='Answer:')\n",
        "    btn = W.Button(description='Submit')\n",
        "    out = W.Output()\n",
        "    def on_click(_):\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            print('Correct!' if dd.value==correct_index else f'Not quite. {explanation}')\n",
        "    btn.on_click(on_click)\n",
        "    display(dd, btn, out)\n",
        "except Exception:\n",
        "    choice = int(input('Your choice (0-3): ').strip() or -1)\n",
        "    print('Correct!' if choice==correct_index else f'Not quite. {explanation}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost & Observability\n",
        "Show tokens (if exposed) and latency; run a small batch and summarize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = ['List 3 cities in 5 words','Name 3 fruits in 5 words']\n",
        "t0=time.time();\n",
        "outs=[]\n",
        "for p in prompts:\n",
        "    outs.append(chat_oss(p)) if RUNTIME=='gpt-oss' else outs.append(infer_transformers(p))\n",
        "dt=time.time()-t0\n",
        "print('Batch latency:', round(dt,2),'s for', len(prompts),'items')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "- Add a JSON schema and validate outputs.\n",
        "- Introduce a tool/function and parse its response.\n",
        "- Expand the golden set to 20 items and report accuracy.\n",
        "- Log average latency over 10 trials at two temperatures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "- Connection errors: verify `OPENAI_BASE_URL` and that your server exposes `/models`.\n",
        "- 401/403: check keys/permissions; never hardcode secrets.\n",
        "- OOM: reduce sequence length/batch; switch to CPU or smaller model.\n",
        "- JSON parse errors: strip code fences; validate/repair before loads.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostics\n",
        "import requests\n",
        "try:\n",
        "    r = requests.get(OPENAI_BASE_URL + '/models', timeout=5)\n",
        "    print(r.status_code, r.text[:200])\n",
        "except Exception as e:\n",
        "    print('Conn error:', e)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

