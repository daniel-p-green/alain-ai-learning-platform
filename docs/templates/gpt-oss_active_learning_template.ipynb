{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT\u2011OSS Active Learning Template \u2014 From HF Link to Lesson\n",
        "\n",
        "[Open in Colab](https://colab.research.google.com/github/daniel-p-green/alain-ai-learning-platform/blob/main/path/to/this.ipynb)\n",
        "\n",
        "- Outcomes: Turn a Hugging Face model link into an engaging lesson with minimal setup, MCQs, a golden\u2011set evaluation, and token/latency logging.\n",
        "- Audience: Practitioners evaluating or teaching local/open models.\n",
        "- Time: ~45\u201375 minutes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters (Colab form)\n",
        "Provide model and runtime settings.\n",
        "\n",
        "(When using Colab, this cell appears as a form; values are stored in variables.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Model and Runtime\n",
        "HF_MODEL = 'org/name' #@param {type:'string'}\n",
        "RUNTIME = 'gpt-oss' #@param ['gpt-oss','transformers']\n",
        "GPT_OSS_MODEL = 'gpt-oss:20b' #@param {type:'string'}\n",
        "OPENAI_BASE_URL = 'http://localhost:11434/v1' #@param {type:'string'}\n",
        "TEMPERATURE = 0.0 #@param {type:'number'}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install (pinned)\n",
        "If running on Colab, uncomment and run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install openai==1.43.0 transformers==4.44.2 datasets==2.20.0 ipywidgets==8.1.3 requests==2.32.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Seeds\n",
        "Environment info, seeds, and device checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, platform, random, time\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "\n",
        "import numpy as np\n",
        "SEED=42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        print('CUDA device:', torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "        print('CUDA not available')\n",
        "except Exception as e:\n",
        "    print('Torch not installed; skipping torch seed.', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Secrets\n",
        "Keys are read from environment if needed; no hardcoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'ollama')\n",
        "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
        "print('Have OPENAI_API_KEY:', bool(OPENAI_API_KEY))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quickstart\n",
        "Run a smallest working example via GPT\u2011OSS or Transformers based on `RUNTIME`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_oss(prompt: str, model: str = None, temperature: float = None):\n",
        "    from openai import OpenAI\n",
        "    model = model or GPT_OSS_MODEL\n",
        "    temperature = temperature if temperature is not None else TEMPERATURE\n",
        "    client = OpenAI(base_url=OPENAI_BASE_URL, api_key=OPENAI_API_KEY)\n",
        "    t0 = time.time()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{'role':'user','content':prompt}],\n",
        "        temperature=temperature\n",
        "    )\n",
        "    dt = time.time()-t0\n",
        "    txt = resp.choices[0].message.content\n",
        "    usage = getattr(resp,'usage',None)\n",
        "    print(txt)\n",
        "    if usage:\n",
        "        print('Tokens total:', getattr(usage,'total_tokens',None))\n",
        "    print(f'Latency: {dt:.2f}s')\n",
        "    return txt\n",
        "\n",
        "def infer_transformers(inp: str):\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(HF_MODEL)\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(HF_MODEL)\n",
        "        gen = pipeline('text-generation', model=mdl, tokenizer=tok)\n",
        "        out = gen(inp, max_new_tokens=32)[0]['generated_text']\n",
        "        print(out)\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        print('Transformers fallback failed:', e)\n",
        "        return ''\n",
        "\n",
        "_ = chat_oss('Say hello in five words.') if RUNTIME=='gpt-oss' else infer_transformers('Hello, world')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Guided Steps\n",
        "1. Explore core parameters (e.g., temperature, max tokens).\n",
        "2. Add a structured output (JSON) or schema validation.\n",
        "3. Run a small batch; measure latency and (if exposed) tokens.\n",
        "4. Optional: simple RAG/fine\u2011tune illustration for this model family.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Release Notes & License\n",
        "Key details to know before you ship or evaluate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from huggingface_hub import HfApi\n",
        "    import re, requests\n",
        "    api=HfApi()\n",
        "    info=api.model_info(HF_MODEL)\n",
        "    print('Last modified:', getattr(info,'lastModified', None))\n",
        "    print('License:', getattr(info,'license', None))\n",
        "    url=f'https://huggingface.co/{HF_MODEL}/raw/main/README.md'\n",
        "    r=requests.get(url,timeout=10)\n",
        "    if r.status_code==200:\n",
        "        md=r.text\n",
        "        def extract(section):\n",
        "            m=re.search(r'(^|\\n)#+\\s*'+re.escape(section)+r'[^\\n]*\\n(.+?)(\\n#+|\\Z)', md, re.S|re.I)\n",
        "            return (m.group(2).strip() if m else None)\n",
        "        for sec in ['Intended Use','Use cases','Limitations','Risks','Training data','Model details','License']:\n",
        "            txt=extract(sec)\n",
        "            if txt:\n",
        "                print(f'--- {sec} ---')\n",
        "                print('\n'.join(txt.splitlines()[:20]))\n",
        "except Exception as e:\n",
        "    print('Could not load release notes/license details:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### License Snippet & What It Means\n",
        "Pulls license file text when available and summarizes practical implications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from huggingface_hub import HfApi, hf_hub_download\n",
        "    import os\n",
        "    api=HfApi()\n",
        "    info=api.model_info(HF_MODEL)\n",
        "    lic=getattr(info,'license',None)\n",
        "    print('License identifier:', lic)\n",
        "    path=None\n",
        "    for name in ['LICENSE','LICENSE.txt','LICENSE.md','LICENSE.MD','license','LICENSE.rst']:\n",
        "        try:\n",
        "            path=hf_hub_download(repo_id=HF_MODEL, filename=name, revision='main')\n",
        "            break\n",
        "        except Exception:\n",
        "            pass\n",
        "    if path and os.path.exists(path):\n",
        "        text=open(path,'r',encoding='utf-8',errors='ignore').read()\n",
        "        print('\n--- License snippet (first 1500 chars) ---\n')\n",
        "        print(text[:1500])\n",
        "    else:\n",
        "        print('License file not found; see model card.')\n",
        "    summary={\n",
        "      'mit':'Permissive: commercial use allowed, attribution and license notice required; no warranty.',\n",
        "      'apache-2.0':'Permissive with patent grant: commercial use allowed; keep NOTICE; mind patents/trademarks.',\n",
        "      'bsd-3-clause':'Permissive: commercial use allowed; attribution required; no endorsement.',\n",
        "      'gpl-3.0':'Copyleft: derivatives must be GPL; not suitable for closed-source distribution.',\n",
        "      'lgpl-3.0':'Weak copyleft: dynamic linking OK; modified library must remain LGPL.',\n",
        "      'agpl-3.0':'Network copyleft: providing as a service triggers source-sharing obligations.',\n",
        "      'cc-by-4.0':'Attribution required; commercial use allowed; keep notices.',\n",
        "      'cc-by-nc-4.0':'Non-commercial: no commercial use permitted; attribution required.',\n",
        "      'openrail':'Responsible AI License: usage restrictions apply; review terms for safety/commercial limits.',\n",
        "      'openrail++':'Responsible AI License: stricter usage constraints; check allowed and disallowed uses.',\n",
        "    }\n",
        "    key=(lic or '').lower()\n",
        "    explain=None\n",
        "    for k,v in summary.items():\n",
        "        if k in key:\n",
        "            explain=v; break\n",
        "    print('\n--- What this license means for you ---\n')\n",
        "    if explain:\n",
        "        print(explain)\n",
        "    else:\n",
        "        print('Review the license in the model card. Typical items: attribution, redistribution terms, commercial-use scope, and safety constraints.')\n",
        "except Exception as e:\n",
        "    print('License lookup error:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intended Use & Limitations (from Model Card)\n",
        "Quickly scan the most relevant sections from the README.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, requests\n",
        "def fetch_readme(org_model: str, timeout: int = 10):\n",
        "    url = f'https://huggingface.co/{org_model}/raw/main/README.md'\n",
        "    try:\n",
        "        r = requests.get(url, timeout=timeout)\n",
        "        if r.status_code == 200:\n",
        "            return r.text\n",
        "    except Exception as e:\n",
        "        print('Fetch error:', e)\n",
        "    return None\n",
        "def extract_sections(md_text: str, sections):\n",
        "    out = {}\n",
        "    for sec in sections:\n",
        "        m = re.search(r'(^|\\n)#+\\s*' + re.escape(sec) + r'[^\\n]*\\n(.+?)(\\n#+|\\Z)', md_text, re.S | re.I)\n",
        "        if m:\n",
        "            out[sec] = m.group(2).strip()\n",
        "    return out\n",
        "md = fetch_readme(HF_MODEL)\n",
        "secs = extract_sections(md or '', ['Intended Use','Use cases','Limitations','Risks','Training data'])\n",
        "for k,v in secs.items():\n",
        "    print(f'\n--- {k} ---\n')\n",
        "    print('\n'.join(v.splitlines()[:30]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Structured output hint (Pydantic)\n",
        "try:\n",
        "    from pydantic import BaseModel\n",
        "    class Item(BaseModel):\n",
        "        title: str\n",
        "        rating: int\n",
        "    print('Define JSON schema with Item.model_json_schema() and validate outputs.')\n",
        "except Exception:\n",
        "    print('Install pydantic to validate structured outputs.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation (Golden Set)\n",
        "Deterministic checks for success criteria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "golden = [\n",
        "    {'prompt':'2+2?','expect':'4'},\n",
        "    {'prompt':'Capital of France?','expect':'Paris'},\n",
        "]\n",
        "ok=0\n",
        "for ex in golden:\n",
        "    out = chat_oss(ex['prompt']) if RUNTIME=='gpt-oss' else infer_transformers(ex['prompt'])\n",
        "    ok += int(ex['expect'].lower() in (out or '').lower())\n",
        "acc = ok/len(golden)\n",
        "print(f'Accuracy: {acc:.2%} ({ok}/{len(golden)})')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MCQ \u2014 Understanding Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = 'Which parameter most reduces randomness?'\n",
        "options = ['top_p','temperature','max_tokens','presence_penalty']\n",
        "correct_index = 1\n",
        "explanation = 'Lower temperature yields more deterministic outputs.'\n",
        "print(question)\n",
        "for i,o in enumerate(options):\n",
        "    print(f'  {i}) {o}')\n",
        "try:\n",
        "    import ipywidgets as W\n",
        "    from IPython.display import display\n",
        "    dd = W.Dropdown(options=[(o,i) for i,o in enumerate(options)], description='Answer:')\n",
        "    btn = W.Button(description='Submit')\n",
        "    out = W.Output()\n",
        "    def on_click(_):\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            print('Correct!' if dd.value==correct_index else f'Not quite. {explanation}')\n",
        "    btn.on_click(on_click)\n",
        "    display(dd, btn, out)\n",
        "except Exception:\n",
        "    choice = int(input('Your choice (0-3): ').strip() or -1)\n",
        "    print('Correct!' if choice==correct_index else f'Not quite. {explanation}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost & Observability\n",
        "Show tokens (if exposed) and latency; run a small batch and summarize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = ['List 3 cities in 5 words','Name 3 fruits in 5 words']\n",
        "t0=time.time();\n",
        "outs=[]\n",
        "for p in prompts:\n",
        "    outs.append(chat_oss(p)) if RUNTIME=='gpt-oss' else outs.append(infer_transformers(p))\n",
        "dt=time.time()-t0\n",
        "print('Batch latency:', round(dt,2),'s for', len(prompts),'items')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "- Add a JSON schema and validate outputs.\n",
        "- Introduce a tool/function and parse its response.\n",
        "- Expand the golden set to 20 items and report accuracy.\n",
        "- Log average latency over 10 trials at two temperatures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "- Connection errors: verify `OPENAI_BASE_URL` and that your server exposes `/models`.\n",
        "- 401/403: check keys/permissions; never hardcode secrets.\n",
        "- OOM: reduce sequence length/batch; switch to CPU or smaller model.\n",
        "- JSON parse errors: strip code fences; validate/repair before loads.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostics\n",
        "import requests\n",
        "try:\n",
        "    r = requests.get(OPENAI_BASE_URL + '/models', timeout=5)\n",
        "    print(r.status_code, r.text[:200])\n",
        "except Exception as e:\n",
        "    print('Conn error:', e)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
