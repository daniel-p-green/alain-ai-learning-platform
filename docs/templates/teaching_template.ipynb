{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Teaching Template: Using New AI Models\n", "\n", "- Outcomes: Learn setup, make a minimal call, understand capabilities/limits, add an evaluation, and track cost.\n", "- Audience: Practitioners new to this model family.\n", "- Time: ~30\u201360 minutes.\n", "\n", "Prerequisites:\n", "- Python 3.10+; optional GPU for Unsloth/HF fine-tuning.\n", "- Environment variables: `OPENAI_API_KEY` and/or `ANTHROPIC_API_KEY` if using APIs.\n", "- Costs: API examples may incur small charges.\n", "\n", "> Tip: Run this notebook top-to-bottom on Colab or your environment.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Setup\n", "- Prints environment info\n", "- Installs optional packages if missing\n", "- Sets seeds for reproducibility\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Pinned installs (uncomment in Colab or fresh envs)\n", "# !pip install -q openai==1.43.0 anthropic==0.36.0 transformers==4.44.2 datasets==2.20.0\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os, sys, platform, random, time\n", "import subprocess\n", "\n", "print('Python:', sys.version)\n", "print('Platform:', platform.platform())\n", "\n", "def sh(cmd):\n", "    try:\n", "        return subprocess.check_output(cmd, shell=True, text=True).strip()\n", "    except Exception as e:\n", "        return f'ERR: {e}'\n", "\n", "print('pip openai:', sh('python -m pip show openai | sed -n \"s/^Version: //p\"'))\n", "print('pip anthropic:', sh('python -m pip show anthropic | sed -n \"s/^Version: //p\"'))\n", "print('pip transformers:', sh('python -m pip show transformers | sed -n \"s/^Version: //p\"'))\n", "print('pip unsloth:', sh('python -m pip show unsloth | sed -n \"s/^Version: //p\"'))\n", "\n", "# Optional: seed setting\n", "import numpy as np\n", "SEED = 42\n", "random.seed(SEED); np.random.seed(SEED)\n", "try:\n", "    import torch\n", "    torch.manual_seed(SEED)\n", "    if torch.cuda.is_available():\n", "        torch.cuda.manual_seed_all(SEED)\n", "        print('CUDA device:', torch.cuda.get_device_name(0))\n", "    else:\n", "        print('CUDA not available')\n", "except Exception as e:\n", "    print('Torch not installed; skipping torch seed.', e)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Secrets & Configuration\n", "Provide API keys via environment variables. This notebook reads them if present and skips API calls if missing.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from os import getenv\n", "OPENAI_API_KEY = getenv('OPENAI_API_KEY')\n", "ANTHROPIC_API_KEY = getenv('ANTHROPIC_API_KEY')\n", "\n", "def have(pkg):\n", "    try:\n", "        __import__(pkg)\n", "        return True\n", "    except Exception:\n", "        return False\n", "\n", "print('Have openai sdk:', have('openai'), 'Have key:', bool(OPENAI_API_KEY))\n", "print('Have anthropic sdk:', have('anthropic'), 'Have key:', bool(ANTHROPIC_API_KEY))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Quickstart: Minimal Calls\n", "Below are minimal examples for OpenAI and Anthropic. These cells gracefully no-op if SDK or keys are missing.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# OpenAI minimal chat completion\ntry:\n    if have('openai') and OPENAI_API_KEY:\n        import time as _t\n        from openai import OpenAI\n        client = OpenAI(api_key=OPENAI_API_KEY)\n        t0 = _t.time()\n        resp = client.chat.completions.create(\n            model='gpt-4o-mini',\n            messages=[{'role':'user','content':'Say hello in 5 words.'}],\n            temperature=0\n        )\n        print(resp.choices[0].message.content)\n        usage = resp.usage\n        if usage:\n            print('Tokens total:', usage.total_tokens)\n        print(f'Latency: {dt:.2f}s')\n    else:\n        print('OpenAI SDK or key missing; skipping.')\nexcept Exception as e:\n    print('OpenAI call error:', e)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Anthropic minimal message\n", "try:\n", "    if have('anthropic') and ANTHROPIC_API_KEY:\n", "        import anthropic\n", "        ac = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n", "        msg = ac.messages.create(\n", "            model='claude-3-5-sonnet-latest',\n", "            max_tokens=64,\n", "            messages=[{'role':'user','content':'Say hello in 5 words.'}]\n", "        )\n", "        print(msg.content[0].text)\n", "    else:\n", "        print('Anthropic SDK or key missing; skipping.')\n", "except Exception as e:\n", "    print('Anthropic call error:', e)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Capabilities & Limits\n", "Briefly summarize what the model is good at, known constraints (context, rate limits), and recommended parameters.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Guided Steps\n", "1. Define the task\n", "2. Design the prompt or schema\n", "3. Add a tool (if needed)\n", "4. Run a small batch\n", "5. Evaluate results and iterate\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Release Notes & License\n", "Key details to know before you ship or evaluate.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n    from huggingface_hub import HfApi\n    import re, requests\n    # Requires HF_MODEL like 'org/name' to be defined in your notebook context\n    HF_MODEL = globals().get('HF_MODEL', 'org/name')\n    api=HfApi()\n    info=api.model_info(HF_MODEL)\n    print('Last modified:', getattr(info,'lastModified', None))\n    print('License:', getattr(info,'license', None))\n    url=f'https://huggingface.co/{HF_MODEL}/raw/main/README.md'\n    r=requests.get(url,timeout=10)\n    if r.status_code==200:\n        md=r.text\n        def extract(section):\n            m=re.search(r'(^|\\n)#+\\s*'+re.escape(section)+r'[^\\n]*\\n(.+?)(\\n#+|\\Z)', md, re.S|re.I)\n            return (m.group(2).strip() if m else None)\n        for sec in ['Intended Use','Use cases','Limitations','Risks','Training data','Model details','License']:\n            txt=extract(sec)\n            if txt:\n                print(f'--- {sec} ---')\n                print('\\n'.join(txt.splitlines()[:20]))\nexcept Exception as e:\n    print('Could not load release notes/license details:', e)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### License Snippet & What It Means\n", "Pulls license file text when available and summarizes practical implications.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n    from huggingface_hub import HfApi, hf_hub_download\n    import os\n    HF_MODEL = globals().get('HF_MODEL', 'org/name')\n    api=HfApi()\n    info=api.model_info(HF_MODEL)\n    lic=getattr(info,'license',None)\n    print('License identifier:', lic)\n    path=None\n    for name in ['LICENSE','LICENSE.txt','LICENSE.md','LICENSE.MD','license','LICENSE.rst']:\n        try:\n            path=hf_hub_download(repo_id=HF_MODEL, filename=name, revision='main')\n            break\n        except Exception:\n            pass\n    if path and os.path.exists(path):\n        text=open(path,'r',encoding='utf-8',errors='ignore').read()\n        print('\\n--- License snippet (first 1500 chars) ---\\n')\n        print(text[:1500])\n    else:\n        print('License file not found; see model card.')\n    summary={\n      'mit':'Permissive: commercial use allowed, attribution and license notice required; no warranty.',\n      'apache-2.0':'Permissive with patent grant: commercial use allowed; keep NOTICE; mind patents/trademarks.',\n      'bsd-3-clause':'Permissive: commercial use allowed; attribution required; no endorsement.',\n      'gpl-3.0':'Copyleft: derivatives must be GPL; not suitable for closed-source distribution.',\n      'lgpl-3.0':'Weak copyleft: dynamic linking OK; modified library must remain LGPL.',\n      'agpl-3.0':'Network copyleft: providing as a service triggers source-sharing obligations.',\n      'cc-by-4.0':'Attribution required; commercial use allowed; keep notices.',\n      'cc-by-nc-4.0':'Non-commercial: no commercial use permitted; attribution required.',\n      'openrail':'Responsible AI License: usage restrictions apply; review terms for safety/commercial limits.',\n      'openrail++':'Responsible AI License: stricter usage constraints; check allowed and disallowed uses.',\n    }\n    key=(lic or '').lower()\n    explain=None\n    for k,v in summary.items():\n        if k in key:\n            explain=v; break\n    print('\\n--- What this license means for you ---\\n')\n    if explain:\n        print(explain)\n    else:\n        print('Review the license in the model card. Typical items: attribution, redistribution terms, commercial-use scope, and safety constraints.')\nexcept Exception as e:\n    print('License lookup error:', e)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Intended Use & Limitations (from Model Card)\n", "Quickly scan the most relevant sections from the README.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import re, requests\nHF_MODEL = globals().get('HF_MODEL', 'org/name')\ndef fetch_readme(org_model: str, timeout: int = 10):\n    url = f'https://huggingface.co/{org_model}/raw/main/README.md'\n    try:\n        r = requests.get(url, timeout=timeout)\n        if r.status_code == 200:\n            return r.text\n    except Exception as e:\n        print('Fetch error:', e)\n    return None\ndef extract_sections(md_text: str, sections):\n    out = {}\n    for sec in sections:\n        m = re.search(r'(^|\\n)#+\\s*' + re.escape(sec) + r'[^\\n]*\\n(.+?)(\\n#+|\\Z)', md_text, re.S | re.I)\n        if m:\n            out[sec] = m.group(2).strip()\n    return out\nmd = fetch_readme(HF_MODEL)\nsecs = extract_sections(md or '', ['Intended Use','Use cases','Limitations','Risks','Training data'])\nfor k,v in secs.items():\n    print(f'\\n--- {k} ---\\n')\n    print('\\n'.join(v.splitlines()[:30]))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Knowledge Check \u2014 License & Intended Use\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["q1='Which license allows commercial use with attribution and notice?'\nopts1=['CC-BY-NC-4.0','MIT','AGPL-3.0','GPL-3.0']\nans1=1\nq2='If the model card lists \"Limitations\" that include factual drift, what should you do?'\nopts2=['Ignore and deploy','Add guardrails/evals and consider RAG','Increase temperature','Hide errors from users']\nans2=1\nprint(q1)\nfor i,o in enumerate(opts1): print(f'  {i}) {o}')\ntry:\n    c1=int(input('Your choice (0-3): ').strip() or -1)\nexcept Exception:\n    c1=-1\nprint('Correct!' if c1==ans1 else 'Review license summaries above.')\nprint(); print(q2)\nfor i,o in enumerate(opts2): print(f'  {i}) {o}')\ntry:\n    c2=int(input('Your choice (0-3): ').strip() or -1)\nexcept Exception:\n    c2=-1\nprint('Correct!' if c2==ans2 else 'Check Intended Use & Limitations.')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example: structured output validation with pydantic (optional)\n", "try:\n", "    from pydantic import BaseModel\n", "    class Item(BaseModel):\n", "        title: str\n", "        rating: int\n", "    print('Pydantic available; define schemas for JSON outputs.')\n", "except Exception:\n", "    print('Pydantic not installed; skip schema validation example.')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Evaluation\n", "Add a tiny golden set or a simple acceptance test to verify success.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Simple acceptance test example\n", "def is_hello_five_words(text: str) -> bool:\n", "    return len(text.split()) == 5\n", "\n", "print('Example check:', is_hello_five_words('Hello from a friendly bot'))\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Minimal deterministic evaluation (assert)\n", "golden = ['Hello from a friendly bot']\n", "ok = sum(int(is_hello_five_words(s)) for s in golden)\n", "assert ok == 1, 'Eval check failed'\n", "print('Accuracy: 100% (1/1)')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cost & Observability\n", "Show how to access token usage/cost where supported.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example: OpenAI usage (if available on the response)\n", "# See quickstart cell above for usage fields.\n", "pass\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example: Anthropic admin usage/cost API (requires proper entitlements)\n", "print('Refer to Anthropic usage & cost API notebooks for end-to-end examples.')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Troubleshooting\n", "- 401/403: check API keys and org permissions\n", "- 429: add exponential backoff; reduce concurrency; shorten prompts\n", "- JSON errors: validate with schemas; enable JSON/structured modes if available\n", "- Timeouts: reduce payloads; increase timeout; retry idempotently\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Diagnostics (Copy/Paste Ready)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Connectivity check for OpenAI-compatible servers (e.g., GPT-OSS via Ollama/vLLM)\n", "try:\n", "    import os, requests\n", "    base = os.getenv('OPENAI_BASE_URL', 'http://localhost:11434/v1')\n", "    r = requests.get(base + '/models', timeout=5)\n", "    print(r.status_code, r.text[:200])\n", "except Exception as e:\n", "    print('Conn error:', e)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# GPU memory snapshot (if torch available)\n", "try:\n", "    import torch\n", "    if torch.cuda.is_available():\n", "        free, total = torch.cuda.mem_get_info()\n", "        print('VRAM (GiB):', round(free/2**30,2), '/', round(total/2**30,2))\n", "    else:\n", "        print('CUDA not available')\n", "except Exception as e:\n", "    print('Torch not installed or mem_get_info unsupported:', e)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Retry/backoff helper\n", "import time\n", "def with_retry(fn, tries=3, backoff=1.5):\n", "    for i in range(tries):\n", "        try:\n", "            return fn()\n", "        except Exception as e:\n", "            print(f'Attempt {i+1} error:', e)\n", "            if i == tries-1: raise\n", "            time.sleep(backoff**i)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# JSON repair example\n", "import json, re\n", "def try_json(text):\n", "    try:\n", "        return json.loads(text), None\n", "    except Exception as e:\n", "        cleaned = re.sub(r'```(json)?|```', '', text).strip()\n", "        cleaned = re.sub(r'[^\\{\\}\\[\\]\\:,\\\"\\-0-9a-zA-Z\\s]', '', cleaned)\n", "        try:\n", "            return json.loads(cleaned), None\n", "        except Exception as ee:\n", "            return None, (e, ee)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercises\n", "- Change the prompt to require a JSON schema and validate it\n", "- Add a tool/function call and parse its output\n", "- Create a 20-item golden set and compute accuracy\n", "- Measure average latency/tokens over 10 runs\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cleanup\n", "- Stop any background jobs or servers\n", "- Clear temp files or caches if created\n", "- Save artifacts/models if you trained locally\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}