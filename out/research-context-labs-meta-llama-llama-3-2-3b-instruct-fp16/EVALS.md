# EVALS

| Benchmark | Dataset | Split | Metric | Score | Source |
| --- | --- | --- | --- | --- | --- |
| MMLU |  | 5 | macro\_avg/acc\_char |  | README |
| AGIEval English |  | 3-5 | average/acc\_char |  | README |
| ARC-Challenge |  | 25 | acc\_char |  | README |
| SQuAD |  | 1 | em |  | README |
| QuAC (F1) |  | 1 | f1 |  | README |
| DROP (F1) |  | 3 | f1 |  | README |
| Needle in Haystack |  | 0 | em |  | README |
| MMLU |  | 5 | macro\_avg/acc | 49.3 | README |
| Open-rewrite eval |  | 0 | micro\_avg/rougeL | 41.6 | README |
| TLDR9+ (test) |  | 1 | rougeL | 16.8 | README |
| IFEval |  | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | README |
| GSM8K (CoT) |  | 8 | em\_maj1@1 | 44.4 | README |
| MATH (CoT) |  | 0 | final\_em | 30.6 | README |
| ARC-C |  | 0 | acc | 59.4 | README |
| GPQA |  | 0 | acc | 27.2 | README |
| Hellaswag |  | 0 | acc | 41.2 | README |
| BFCL V2 |  | 0 | acc | 25.7 | README |
| Nexus |  | 0 | macro\_avg/acc | 13.5 | README |
| InfiniteBench/En.QA |  | 0 | longbook\_qa/f1 | 20.3 | README |
| InfiniteBench/En.MC |  | 0 | longbook\_choice/acc | 38.0 | README |
| NIH/Multi-needle |  | 0 | recall | 75.0 | README |
| MGSM (CoT) |  | 0 | em | 24.5 | README |
| MMLU (5-shot, macro_avg/acc) |  |  |  |  | README |
| benchmark |  |  |  |  | README |
| llama |  |  |  | 3 | README |
| Visit one of the repos, for example [meta-llama/Llama |  |  | Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E). | 4- | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/README.md |
| benchmark |  |  |  | 420 | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| benchmark |  |  |  | 2,040 | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| benchmark |  |  |  | 8,930 | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| benchmark |  |  |  | 0 | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| MMLU |  | 5 | macro_avg/acc_char |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 5 |  | macro_avg/acc_char | 36.2 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 3-5 |  | average/acc_char | 47.1 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 7 |  | acc_char | 72.6 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 5 |  | acc_char | - |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 3 |  | average/em | 61.1 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 25 |  | acc_char | 79.4 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| TriviaQA-Wiki |  | 5 | em |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 1 |  | f1 | 44.4 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 0 |  | acc_char | 75.7 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 3 |  | f1 | 58.4 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| MMLU |  | 5 | macro_avg/acc |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 0 |  | macro_avg/acc | 65.3 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 5 |  | macro_avg/acc | 45.5 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| benchmark |  |  | 76.8 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| ARC-C |  | 0 | acc |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 0 |  | em | 34.6 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| HumanEval |  | 0 | pass@1 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 0 |  | pass@1 | 70.6 |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |
| 0 |  | pass@1 | - |  | link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md |

> Showing first 50 of 63 entries.