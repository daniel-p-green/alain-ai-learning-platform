{
  "model_id": "context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16",
  "revision": "4a7511c7510a87a493e7f30f51065a84b1a3943f",
  "files": {
    "hf_readme": true,
    "config": true,
    "generation_config": true,
    "tokenizer_json": true,
    "tokenizer_model": false,
    "gh_readme": true,
    "gh_license": true,
    "gh_citation": false,
    "gh_release": true,
    "arxiv_meta": true,
    "dataset_cards": false
  },
  "datasets": [],
  "manifest_entries": 20,
  "metadata": {
    "license": "llama3.2",
    "license_source": "README front-matter",
    "architecture": "LlamaForCausalLM",
    "context_window": "131072",
    "tokenizer_details": {
      "vocab_size": "128256"
    },
    "versioning": {
      "hf_revision": "4a7511c7510a87a493e7f30f51065a84b1a3943f"
    },
    "inference": {},
    "usage_sections": [
      "## Inference time\n\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\n\n| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |\n| :---- | ----- | ----- | ----- | ----- | ----- |\n| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |\n| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |\n| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |\n| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |\n| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |\n| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |\n\n(\\*) The performance measurement is done using an adb binary-based approach.\n(\\*\\*) It is measured on an Android OnePlus 12 device.\n(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64\n\n*Footnote:*\n\n- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*\n- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*\n- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*\n- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*\n- *RSS size \\- Memory usage in resident set size (RSS)*\n"
    ],
    "evals": [
      {
        "benchmark": "MMLU",
        "metric": "macro\\_avg/acc\\_char",
        "split": "5",
        "source": "README"
      },
      {
        "benchmark": "AGIEval English",
        "metric": "average/acc\\_char",
        "split": "3-5",
        "source": "README"
      },
      {
        "benchmark": "ARC-Challenge",
        "metric": "acc\\_char",
        "split": "25",
        "source": "README"
      },
      {
        "benchmark": "SQuAD",
        "metric": "em",
        "split": "1",
        "source": "README"
      },
      {
        "benchmark": "QuAC (F1)",
        "metric": "f1",
        "split": "1",
        "source": "README"
      },
      {
        "benchmark": "DROP (F1)",
        "metric": "f1",
        "split": "3",
        "source": "README"
      },
      {
        "benchmark": "Needle in Haystack",
        "metric": "em",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "MMLU",
        "metric": "macro\\_avg/acc",
        "score": "49.3",
        "split": "5",
        "source": "README"
      },
      {
        "benchmark": "Open-rewrite eval",
        "metric": "micro\\_avg/rougeL",
        "score": "41.6",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "TLDR9+ (test)",
        "metric": "rougeL",
        "score": "16.8",
        "split": "1",
        "source": "README"
      },
      {
        "benchmark": "IFEval",
        "metric": "Avg(Prompt/Instruction acc Loose/Strict)",
        "score": "59.5",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "GSM8K (CoT)",
        "metric": "em\\_maj1@1",
        "score": "44.4",
        "split": "8",
        "source": "README"
      },
      {
        "benchmark": "MATH (CoT)",
        "metric": "final\\_em",
        "score": "30.6",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "ARC-C",
        "metric": "acc",
        "score": "59.4",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "GPQA",
        "metric": "acc",
        "score": "27.2",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "Hellaswag",
        "metric": "acc",
        "score": "41.2",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "BFCL V2",
        "metric": "acc",
        "score": "25.7",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "Nexus",
        "metric": "macro\\_avg/acc",
        "score": "13.5",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "InfiniteBench/En.QA",
        "metric": "longbook\\_qa/f1",
        "score": "20.3",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "InfiniteBench/En.MC",
        "metric": "longbook\\_choice/acc",
        "score": "38.0",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "NIH/Multi-needle",
        "metric": "recall",
        "score": "75.0",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "MGSM (CoT)",
        "metric": "em",
        "score": "24.5",
        "split": "0",
        "source": "README"
      },
      {
        "benchmark": "MMLU (5-shot, macro_avg/acc)",
        "source": "README"
      },
      {
        "benchmark": "benchmark",
        "source": "README"
      },
      {
        "benchmark": "llama",
        "score": "3",
        "source": "README"
      },
      {
        "benchmark": "Visit one of the repos, for example [meta-llama/Llama",
        "metric": "Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E).",
        "score": "4-",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/README.md"
      },
      {
        "benchmark": "benchmark",
        "score": "420",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "benchmark",
        "score": "2,040",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "benchmark",
        "score": "8,930",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "benchmark",
        "score": "0",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "MMLU",
        "metric": "macro_avg/acc_char",
        "split": "5",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "5",
        "metric": "36.2",
        "split": "macro_avg/acc_char",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "3-5",
        "metric": "47.1",
        "split": "average/acc_char",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "7",
        "metric": "72.6",
        "split": "acc_char",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "5",
        "metric": "-",
        "split": "acc_char",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "3",
        "metric": "61.1",
        "split": "average/em",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "25",
        "metric": "79.4",
        "split": "acc_char",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "TriviaQA-Wiki",
        "metric": "em",
        "split": "5",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "1",
        "metric": "44.4",
        "split": "f1",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "0",
        "metric": "75.7",
        "split": "acc_char",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "3",
        "metric": "58.4",
        "split": "f1",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "MMLU",
        "metric": "macro_avg/acc",
        "split": "5",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "0",
        "metric": "65.3",
        "split": "macro_avg/acc",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "5",
        "metric": "45.5",
        "split": "macro_avg/acc",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "benchmark",
        "metric": "76.8",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "ARC-C",
        "metric": "acc",
        "split": "0",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "0",
        "metric": "34.6",
        "split": "em",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "HumanEval",
        "metric": "pass@1",
        "split": "0",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "0",
        "metric": "70.6",
        "split": "pass@1",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "0",
        "metric": "-",
        "split": "pass@1",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "GSM-8K (CoT)",
        "metric": "em_maj1@1",
        "split": "8",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "0",
        "metric": "29.1",
        "split": "final_em",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "API-Bank",
        "metric": "acc",
        "split": "0",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "0",
        "metric": "60.3",
        "split": "acc",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "0",
        "metric": "1.7",
        "split": "acc",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "0",
        "metric": "18.1",
        "split": "macro_avg/acc",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "Multilingual MGSM (CoT)",
        "metric": "em",
        "split": "0",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "62.45",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "61.63",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "60.59",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "62.34",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "50.88",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      },
      {
        "benchmark": "50.32",
        "source": "link:https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md"
      }
    ],
    "parameters": "1.23B",
    "coverage": {
      "filled": 6,
      "total": 10,
      "missing": [
        "tokenizer",
        "primary repo",
        "inference hardware",
        "license SPDX"
      ],
      "score": 60
    },
    "primary_repo": "meta-llama/llama-models",
    "notes": "Generated offline from harvested metadata (LM spec unavailable)."
  }
}