{
  "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
  "revision": "6a6f4aa4197940add57724a7707d069478df56b1",
  "files": {
    "hf_readme": true,
    "config": true,
    "generation_config": true,
    "tokenizer_json": true,
    "tokenizer_model": false,
    "gh_readme": true,
    "gh_license": true,
    "gh_citation": false,
    "gh_release": true,
    "arxiv_meta": true,
    "dataset_cards": false
  },
  "datasets": [],
  "manifest_entries": 12,
  "metadata": {
    "license": "MIT",
    "license_source": "README front-matter",
    "architecture": "LlamaForCausalLM",
    "context_window": "131072",
    "tokenizer_details": {
      "vocab_size": "128256"
    },
    "versioning": {
      "hf_revision": "6a6f4aa4197940add57724a7707d069478df56b1"
    },
    "inference": {},
    "usage_sections": [
      "### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n"
    ],
    "evals": [
      {
        "benchmark": "Architecture",
        "metric": "Architecture",
        "source": "README"
      },
      {
        "benchmark": "# Activated Params",
        "metric": "# Activated Params",
        "source": "README"
      },
      {
        "benchmark": "# Total Params",
        "metric": "# Total Params",
        "source": "README"
      },
      {
        "benchmark": "MMLU (Pass@1)",
        "metric": "MMLU (Pass@1)",
        "source": "README"
      },
      {
        "benchmark": "MMLU-Redux (EM)",
        "metric": "MMLU-Redux (EM)",
        "source": "README"
      },
      {
        "benchmark": "MMLU-Pro (EM)",
        "metric": "MMLU-Pro (EM)",
        "source": "README"
      },
      {
        "benchmark": "DROP (3-shot F1)",
        "metric": "DROP (3-shot F1)",
        "source": "README"
      },
      {
        "benchmark": "IF-Eval (Prompt Strict)",
        "metric": "IF-Eval (Prompt Strict)",
        "source": "README"
      },
      {
        "benchmark": "GPQA-Diamond (Pass@1)",
        "metric": "GPQA-Diamond (Pass@1)",
        "source": "README"
      },
      {
        "benchmark": "SimpleQA (Correct)",
        "metric": "SimpleQA (Correct)",
        "source": "README"
      },
      {
        "benchmark": "FRAMES (Acc.)",
        "metric": "FRAMES (Acc.)",
        "source": "README"
      },
      {
        "benchmark": "AlpacaEval2.0 (LC-winrate)",
        "metric": "AlpacaEval2.0 (LC-winrate)",
        "source": "README"
      },
      {
        "benchmark": "ArenaHard (GPT-4-1106)",
        "metric": "ArenaHard (GPT-4-1106)",
        "source": "README"
      },
      {
        "benchmark": "LiveCodeBench (Pass@1-COT)",
        "metric": "LiveCodeBench (Pass@1-COT)",
        "source": "README"
      },
      {
        "benchmark": "Codeforces (Percentile)",
        "metric": "Codeforces (Percentile)",
        "source": "README"
      },
      {
        "benchmark": "Codeforces (Rating)",
        "metric": "Codeforces (Rating)",
        "source": "README"
      },
      {
        "benchmark": "SWE Verified (Resolved)",
        "metric": "SWE Verified (Resolved)",
        "source": "README"
      },
      {
        "benchmark": "Aider-Polyglot (Acc.)",
        "metric": "Aider-Polyglot (Acc.)",
        "source": "README"
      },
      {
        "benchmark": "AIME 2024 (Pass@1)",
        "metric": "AIME 2024 (Pass@1)",
        "source": "README"
      },
      {
        "benchmark": "MATH-500 (Pass@1)",
        "metric": "MATH-500 (Pass@1)",
        "source": "README"
      },
      {
        "benchmark": "CNMO 2024 (Pass@1)",
        "metric": "CNMO 2024 (Pass@1)",
        "source": "README"
      },
      {
        "benchmark": "CLUEWSC (EM)",
        "metric": "CLUEWSC (EM)",
        "source": "README"
      },
      {
        "benchmark": "C-Eval (EM)",
        "metric": "C-Eval (EM)",
        "source": "README"
      },
      {
        "benchmark": "C-SimpleQA (Correct)",
        "metric": "C-SimpleQA (Correct)",
        "source": "README"
      },
      {
        "benchmark": "DeepSeek-R1-Distill-Qwen",
        "metric": "B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.",
        "score": "1.5",
        "source": "README"
      },
      {
        "benchmark": "DeepSeek-R1-Distill-Llama",
        "metric": "B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).",
        "score": "8",
        "source": "README"
      },
      {
        "benchmark": "DeepSeek-R1-Distill-Llama",
        "metric": "B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).",
        "score": "70",
        "source": "README"
      }
    ],
    "license_details": {
      "spdx": "MIT"
    },
    "coverage": {
      "filled": 6,
      "total": 10,
      "missing": [
        "parameters",
        "tokenizer",
        "primary repo",
        "inference hardware"
      ],
      "score": 60
    },
    "primary_repo": "deepseek-ai/DeepSeek-R1",
    "notes": "Generated offline from harvested metadata (LM spec unavailable).",
    "flags": [
      "needs_parameter_verification"
    ]
  }
}