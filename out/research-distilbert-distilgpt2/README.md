# distilbert/distilgpt2

This is an offline research capsule for distilbert/distilgpt2.

## At a Glance
- Architecture: GPT-2
- Parameters: 124M
- Context window: 1024
- Tokenizer: BPE
- License: Apache-2.0
- Primary sources captured: 37
- Primary repository: huggingface/transformers

## Coverage

- Coverage score: 9/10 (~90% )
- Missing metadata: evals
- Evaluation results not discovered; add benchmarks manually.

## Files
- Sources digest: sources_digest.md
- Raw sources under sources/
- See TECH_SPECS.md and LICENSE_NOTES.md for extracted details.