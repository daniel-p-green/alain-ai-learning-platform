# EVALS

| Benchmark | Dataset | Split | Metric | Score | Source |
| --- | --- | --- | --- | --- | --- |
| [MMLU](https://arxiv.org/abs/2009.03300) |  |  | 5-shot, top-1 |  | README |
| [HellaSwag](https://arxiv.org/abs/1905.07830) |  |  | 10-shot |  | README |
| [PIQA](https://arxiv.org/abs/1911.11641) |  |  | 0-shot |  | README |
| [BoolQ](https://arxiv.org/abs/1905.10044) |  |  | 0-shot |  | README |
| [WinoGrande](https://arxiv.org/abs/1907.10641) |  |  | partial score |  | README |
| [ARC-e](https://arxiv.org/abs/1911.01547) |  |  | 0-shot |  | README |
| [ARC-c](https://arxiv.org/abs/1911.01547) |  |  | 25-shot |  | README |
| [TriviaQA](https://arxiv.org/abs/1705.03551) |  |  | 5-shot |  | README |
| [Natural Questions](https://github.com/google-research-datasets/natural-questions) |  |  | 5-shot |  | README |
| [HumanEval](https://arxiv.org/abs/2107.03374) |  |  | pass@1 |  | README |
| [MBPP](https://arxiv.org/abs/2108.07732) |  |  | 3-shot |  | README |
| [GSM8K](https://arxiv.org/abs/2110.14168) |  |  | 5-shot, maj@1 |  | README |
| [MATH-500](https://arxiv.org/abs/2103.03874) |  |  | 4-shot |  | README |
| [AGIEval](https://arxiv.org/abs/2304.06364) |  |  | 3-5-shot |  | README |
| [BIG-Bench](https://arxiv.org/abs/2206.04615) |  |  | 3-shot, CoT |  | README |