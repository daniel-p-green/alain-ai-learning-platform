# EVALS

| Benchmark | Dataset | Split | Metric | Score | Source |
| --- | --- | --- | --- | --- | --- |
| llama |  |  |  | 3 | README |
| benchmark |  |  |  | 420 | README |
| benchmark |  |  |  | 2,040 | README |
| benchmark |  |  |  | 8,930 | README |
| benchmark |  |  |  | 0 | README |
| MMLU |  | 5 | macro_avg/acc_char |  | README |
| 5 |  | macro_avg/acc_char | 36.2 |  | README |
| 3-5 |  | average/acc_char | 47.1 |  | README |
| 7 |  | acc_char | 72.6 |  | README |
| 5 |  | acc_char | - |  | README |
| 3 |  | average/em | 61.1 |  | README |
| 25 |  | acc_char | 79.4 |  | README |
| TriviaQA-Wiki |  | 5 | em |  | README |
| SQuAD |  | 1 | em |  | README |
| 1 |  | f1 | 44.4 |  | README |
| 0 |  | acc_char | 75.7 |  | README |
| 3 |  | f1 | 58.4 |  | README |
| MMLU |  | 5 | macro_avg/acc |  | README |
| 0 |  | macro_avg/acc | 65.3 |  | README |
| 5 |  | micro_avg/acc_char | 45.5 |  | README |
| benchmark |  |  | 76.8 |  | README |
| ARC-C |  | 0 | acc |  | README |
| 0 |  | em | 34.6 |  | README |
| HumanEval |  | 0 | pass@1 |  | README |
| 0 |  | pass@1 | 70.6 |  | README |
| 0 |  | pass@1 | - |  | README |
| GSM-8K (CoT) |  | 8 | em_maj1@1 |  | README |
| 0 |  | final_em | 29.1 |  | README |
| API-Bank |  | 0 | acc |  | README |
| 0 |  | acc | 60.3 |  | README |
| 0 |  | acc | 1.7 |  | README |
| 0 |  | macro_avg/acc | 18.1 |  | README |
| Multilingual MGSM (CoT) |  | 0 | em |  | README |
| MMLU (5-shot, macro_avg/acc) |  |  |  |  | README |
| 62.45 |  |  |  |  | README |
| 61.63 |  |  |  |  | README |
| 60.59 |  |  |  |  | README |
| 62.34 |  |  |  |  | README |
| 50.88 |  |  |  |  | README |
| 50.32 |  |  |  |  | README |