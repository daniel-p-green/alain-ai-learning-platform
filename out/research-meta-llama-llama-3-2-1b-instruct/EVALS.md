# EVALS

| Benchmark | Dataset | Split | Metric | Score | Source |
| --- | --- | --- | --- | --- | --- |
| MMLU |  | 5 | macro\_avg/acc\_char |  | README |
| AGIEval English |  | 3-5 | average/acc\_char |  | README |
| ARC-Challenge |  | 25 | acc\_char |  | README |
| SQuAD |  | 1 | em |  | README |
| QuAC (F1) |  | 1 | f1 |  | README |
| DROP (F1) |  | 3 | f1 |  | README |
| Needle in Haystack |  | 0 | em |  | README |
| MMLU |  | 5 | macro\_avg/acc | 49.3 | README |
| Open-rewrite eval |  | 0 | micro\_avg/rougeL | 41.6 | README |
| TLDR9+ (test) |  | 1 | rougeL | 16.8 | README |
| IFEval |  | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | README |
| GSM8K (CoT) |  | 8 | em\_maj1@1 | 44.4 | README |
| MATH (CoT) |  | 0 | final\_em | 30.6 | README |
| ARC-C |  | 0 | acc | 59.4 | README |
| GPQA |  | 0 | acc | 27.2 | README |
| Hellaswag |  | 0 | acc | 41.2 | README |
| BFCL V2 |  | 0 | acc | 25.7 | README |
| Nexus |  | 0 | macro\_avg/acc | 13.5 | README |
| InfiniteBench/En.QA |  | 0 | longbook\_qa/f1 | 20.3 | README |
| InfiniteBench/En.MC |  | 0 | longbook\_choice/acc | 38.0 | README |
| NIH/Multi-needle |  | 0 | recall | 75.0 | README |
| MGSM (CoT) |  | 0 | em | 24.5 | README |
| MMLU (5-shot, macro_avg/acc) |  |  |  |  | README |
| benchmark |  |  |  |  | README |
| llama |  |  |  | 3 | README |