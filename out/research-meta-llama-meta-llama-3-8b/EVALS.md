# EVALS

| Benchmark | Dataset | Split | Metric | Score | Source |
| --- | --- | --- | --- | --- | --- |
| llama |  |  |  | 3 | README |
| benchmark |  |  |  | 390 | README |
| benchmark |  |  |  | 1900 | README |
| benchmark |  |  |  | 2290 | README |
| MMLU (5-shot) |  |  |  |  | README |
| 45.9 |  |  |  |  | README |
| 72.6 |  |  |  |  | README |
| 76.1 |  |  |  |  | README |
| 61.1 |  |  |  |  | README |
| 78.6 |  |  |  |  | README |
| TriviaQA-Wiki (5-shot) |  |  |  |  | README |
| SQuAD (1-shot) |  |  |  |  | README |
| 44.4 |  |  |  |  | README |
| 75.7 |  |  |  |  | README |
| 58.4 |  |  |  |  | README |
| GPQA (0-shot) |  |  |  |  | README |
| HumanEval (0-shot) |  |  |  |  | README |
| GSM-8K (8-shot, CoT) |  |  |  |  | README |
| MATH (4-shot, CoT) |  |  |  |  | README |