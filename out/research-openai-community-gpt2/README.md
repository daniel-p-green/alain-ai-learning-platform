# openai-community/gpt2

This is an offline research capsule for openai-community/gpt2.

## At a Glance
- Architecture: transformer
- Parameters: 124M
- Context window: 1024
- Tokenizer: byte-level BPE
- License: mit
- Primary sources captured: 8
- Primary repository: https://github.com/openai/gpt-2

## Coverage

- Coverage score: 8/10 (~80% )
- Missing metadata: evals, inference hardware
- Evaluation results not discovered; add benchmarks manually.

## Files
- Sources digest: sources_digest.md
- Raw sources under sources/
- See TECH_SPECS.md and LICENSE_NOTES.md for extracted details.