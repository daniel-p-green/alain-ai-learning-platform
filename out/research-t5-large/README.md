# t5-large

This is an offline research capsule for t5-large.

## At a Glance
- Architecture: T5
- Parameters: 770M
- Context window: 512
- Tokenizer: Not specified
- License: Apache 2.0
- Primary sources captured: 16
- Primary repository: https://github.com/google-research/text-to-text-transfer-transformer

## Coverage

- Coverage score: 7/10 (~70% )
- Missing metadata: tokenizer, inference hardware, license SPDX

## Files
- Sources digest: sources_digest.md
- Raw sources under sources/
- See TECH_SPECS.md and LICENSE_NOTES.md for extracted details.