{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TinyLlama 1.1B Chat: From GGUF to Interactive Inference\n\nLearn how to load the TinyLlama‚Äë1.1B‚ÄëChat‚Äëv1.0 GGUF model from Hugging Face, run fast inference, customize prompts, and deploy a lightweight chat API. This lesson covers environment setup, model loading, generation, batching, and basic fine‚Äëtuning using the Hugging Face ecosystem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Install and configure the required Python packages for GGUF inference.\n",
        "2. Load the TinyLlama‚Äë1.1B‚ÄëChat model and tokenizer from the Hugging Face Hub.\n",
        "3. Generate text with custom prompts and control generation parameters.\n",
        "4. Deploy the model as a simple REST API for real‚Äëtime chat.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic Python programming (3.8+).\n",
        "- Familiarity with Hugging Face Transformers API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create requirements.txt\n",
        "requirements = '''transformers==4.35.0\ntorch==2.0.1\naccelerate==0.21.0\nhuggingface_hub==0.20.3\nfastapi==0.110.0\nuvicorn==0.29.0'''\n\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(requirements)\n\n",
        "print('‚úÖ requirements.txt created!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment & Dependencies\n\nWelcome to the first step of your TinyLlama adventure! Think of this section as setting up a tiny kitchen before you start cooking a fancy dish. We‚Äôll install the right tools (Python packages), make sure your computer can talk to the Hugging Face Hub, and set up a safe place to keep your credentials.\n\n**Why do we need all this?**\n- `transformers` gives us the recipe for TinyLlama.\n- `torch` is the engine that runs the math.\n- `accelerate` helps us use your GPU (if you have one) or CPU efficiently.\n- `huggingface_hub` lets us download the GGUF file.\n- `fastapi` and `uvicorn` are for the future chat API.\n\nWe‚Äôll also show you how to create a virtual environment so your project stays tidy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Create a virtual environment (recommended)\n# This keeps your project dependencies isolated.\n# If you already have one, skip to the pip install step.\n\nimport subprocess, sys\n\ntry:\n    subprocess.check_call([sys.executable, \"-m\", \"venv\", \"venv\"])\n    print(\"‚úÖ Virtual environment 'venv' created.\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è  Virtual environment creation failed:\", e)\n\n# 2Ô∏è‚É£ Activate the environment (Unix/macOS)\n#    source venv/bin/activate\n#    (Windows) venv\\Scripts\\activate.bat\n\n# 3Ô∏è‚É£ Install the required packages\n# We pin exact versions for reproducibility.\n\nrequirements = [\n    \"transformers==4.35.0\",\n    \"torch==2.0.1\",\n    \"accelerate==0.21.0\",\n    \"huggingface_hub==0.20.3\",\n    \"fastapi==0.110.0\",\n    \"uvicorn==0.29.0\"\n]\n\ntry:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *requirements])\n    print(\"‚úÖ All packages installed successfully.\")\nexcept subprocess.CalledProcessError as e:\n    print(\"‚ö†Ô∏è  Package installation failed. Try running pip manually.\")\n    sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Set up your Hugging Face token\n\nIf you‚Äôre downloading a private model, you‚Äôll need a **HF_TOKEN**. For public models like TinyLlama‚Äë1.1B‚ÄëChat‚Äëv1.0, it‚Äôs optional, but it speeds up downloads.\n\n**How to get one:**\n1. Sign in to [Hugging Face](https://huggingface.co/).\n2. Go to *Settings ‚Üí Access Tokens*.\n3. Click *New token*, give it a name, and copy the value.\n\n**Why store it in an environment variable?**\n- Keeps it out of your code.\n- Lets `huggingface_hub` automatically pick it up.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Export the token for Unix/macOS\n# Replace YOUR_HF_TOKEN with the string you copied.\n# You can add this line to ~/.bashrc or ~/.zshrc for persistence.\n\nimport os\n\nos.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN\"  # <-- replace this\nprint(\"‚úÖ HF_TOKEN set in environment (for this session).\")\n\n# For Windows PowerShell:\n#   $env:HF_TOKEN = \"YOUR_HF_TOKEN\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ You‚Äôre all set!\n\nRun the cells above in a Jupyter notebook or a Python script. If everything prints `‚úÖ`, you‚Äôre ready to load TinyLlama in the next section. If you hit any errors, double‚Äëcheck:\n- Python version (`python --version` should be 3.8+).\n- Internet connection.\n- Correct spelling of package names.\n\nHappy coding! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Understanding GGUF and TinyLlama\n\nBefore we can load TinyLlama, let‚Äôs demystify two key concepts:\n\n1. **GGUF** ‚Äì Think of a GGUF file as a *tiny, super‚Äëcompressed cookbook*. It stores the model‚Äôs weights in a binary format that‚Äôs smaller than the usual `.bin` or `.pt` files, but still contains all the information the model needs to talk.\n2. **TinyLlama‚Äë1.1B‚ÄëChat** ‚Äì This is a lightweight version of the Llama family, trimmed down to 1.1‚ÄØbillion parameters. It‚Äôs like a compact smartphone that still runs most apps, but uses less memory and CPU.\n\nWhy GGUF? The main benefits are:\n- **Speed** ‚Äì Loading is faster because the file is smaller.\n- **Memory** ‚Äì Less RAM is required to keep the model in memory.\n- **Portability** ‚Äì You can ship the model to edge devices or cloud functions with minimal bandwidth.\n\nBelow we‚Äôll inspect the GGUF file, check its size, and confirm that the Hugging Face Hub can handle it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Inspect the GGUF file size and metadata\n# We‚Äôll use the `huggingface_hub` library to download the file locally\n# and then read a few bytes to confirm it‚Äôs a GGUF file.\n\nimport os\nfrom huggingface_hub import hf_hub_download\n\n# Path where the GGUF will be stored\nmodel_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\nfilename = \"TinyLlama-1.1B-Chat-v1.0.Q4_K_M.gguf\"  # one of the quantized variants\n\n# Download (or use cached copy)\nlocal_path = hf_hub_download(repo_id=model_id, filename=filename)\nprint(f\"‚úÖ GGUF file downloaded to: {local_path}\")\n\n# Show file size in megabytes\nsize_mb = os.path.getsize(local_path) / (1024 ** 2)\nprint(f\"üì¶ File size: {size_mb:.2f}‚ÄØMB\")\n\n# Peek at the first few bytes ‚Äì GGUF files start with the ASCII string 'GGUF'\nwith open(local_path, \"rb\") as f:\n    header = f.read(4)\nprint(f\"üóÇÔ∏è Header bytes: {header}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What we just did\n- **`hf_hub_download`** pulls the file from Hugging Face and caches it locally.\n- We printed the file size to see how lightweight it is.\n- The header check confirms the file is indeed a GGUF binary.\n\nIf you see `b'GGUF'` in the header, you‚Äôre good to go!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2Ô∏è‚É£ Quick sanity check: load the model with transformers\n# This will load the GGUF file into memory. It may take a few seconds.\n# We‚Äôll use the `trust_remote_code=True` flag to allow the model class to be loaded.\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load tokenizer ‚Äì TinyLlama uses the Llama tokenizer\nprint(\"üîÑ Loading tokenizer‚Ä¶\")\nmodel_name = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n# The tokenizer is the same as the original Llama model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nprint(\"‚úÖ Tokenizer loaded.\")\n\n# Load the GGUF model\nprint(\"üîÑ Loading GGUF model‚Ä¶\")\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\nprint(\"‚úÖ GGUF model loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TL;DR\n- GGUF is a compact, binary format for model weights.\n- TinyLlama‚Äë1.1B‚ÄëChat is a 1.1‚ÄØbillion‚Äëparameter Llama variant, great for quick inference.\n- The Hugging Face Hub and `transformers` make it trivial to download and load these files.\n\nIn the next section we‚Äôll actually generate text with this model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loading the Model & Tokenizer\n\nNow that we‚Äôve downloaded the TinyLlama GGUF file, it‚Äôs time to bring it into memory. Think of the model as a *recipe book* and the tokenizer as the *chef‚Äôs measuring spoon*. The tokenizer turns your words into numbers the model can understand, and the model uses those numbers to predict the next word.\n\nWe‚Äôll load both components with a single line each, using the `transformers` library. The `trust_remote_code=True` flag tells Hugging Face that it‚Äôs okay to run the custom code that ships with TinyLlama.\n\nIf you‚Äôre on a machine with a GPU, the model will automatically use it. If not, it will fall back to the CPU.\n\nLet‚Äôs get started!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Load the tokenizer\n# The tokenizer is the same as the original Llama tokenizer\n# It converts text to token ids and back.\n\nfrom transformers import AutoTokenizer\n\nmodel_name = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        use_fast=True  # fast tokenizers are a bit faster\n    )\n    print(\"‚úÖ Tokenizer loaded successfully.\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è  Failed to load tokenizer:\", e)\n    raise\n\n# Quick sanity check: encode a simple sentence\nsample = \"Hello, TinyLlama!\"\nencoded = tokenizer.encode(sample, add_special_tokens=True)\nprint(\"üî¢ Token IDs:\", encoded)\nprint(\"üß© Decoded back:\", tokenizer.decode(encoded))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What just happened?\n- `AutoTokenizer.from_pretrained` pulls the tokenizer files from the Hugging Face Hub.\n- `add_special_tokens=True` adds the beginning‚Äëof‚Äësentence token that TinyLlama expects.\n- The quick encode/decode round‚Äëtrip confirms the tokenizer works.\n\nNow we‚Äôre ready to load the heavy‚Äëlifting part: the model itself.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2Ô∏è‚É£ Load the GGUF model\n# This step can take a few seconds, especially on a CPU.\n# If you have a GPU, the model will automatically use it.\n\nfrom transformers import AutoModelForCausalLM\nimport torch\n\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        device_map=\"auto\",  # let accelerate decide where to place layers\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n    )\n    print(\"‚úÖ GGUF model loaded successfully.\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è  Failed to load model:\", e)\n    raise\n\n# Verify that the model can run a quick forward pass\ninput_ids = tokenizer.encode(\"Tell me a joke.\", return_tensors=\"pt\")\nif torch.cuda.is_available():\n    input_ids = input_ids.to(\"cuda\")\n\nwith torch.no_grad():\n    outputs = model.generate(input_ids, max_new_tokens=20)\n\nprint(\"ü§ñ Model output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why `device_map=\"auto\"`?\n- On a GPU, the model‚Äôs layers are split across memory to avoid overflow.\n- On a CPU, it keeps everything in RAM but still uses the most efficient layout.\n\nIf you want to force the model onto a specific device, you can replace `device_map=\"auto\"` with `device_map=\"cpu\"` or `device_map=\"cuda:0\"`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Basic Text Generation\n\nIn this step we‚Äôll actually ask TinyLlama‚Äë1.1B‚ÄëChat to write something. Think of the model as a very clever robot that has read a huge book. When you give it a *prompt* (the first few words or a question), it predicts the next word, then the next, and so on, until it reaches a stopping point.\n\nWe‚Äôll cover:\n\n1. **Setting a random seed** ‚Äì so you can reproduce the same answer.\n2. **Choosing generation parameters** ‚Äì how long the answer is, how creative it is, etc.\n3. **Running the generation** ‚Äì with a tiny code snippet.\n4. **Decoding the token IDs back to text** ‚Äì turning numbers into readable words.\n\nAll code is split into small cells so you can run them one by one and see the output immediately.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£  Set a reproducible random seed\n# This makes sure that every time you run the notebook you get the same \"random\" words.\n# If you want a different answer, change the number.\nimport random\nimport torch\n\nSEED = 42\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n# If you have a GPU, also set its seed for full reproducibility\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nprint(f\"Random seed set to {SEED}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "‚ö†Ô∏è **Warning**: Even with a fixed seed, the model may still produce slightly different outputs on different hardware or library versions. For strict reproducibility, pin the exact library versions as listed in `requirements.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2Ô∏è‚É£  Choose generation parameters\n# These control how the model behaves:\n#   - max_new_tokens: how many words (roughly) to generate\n#   - temperature: 0 = deterministic, higher = more random\n#   - top_p (nucleus sampling): keep the top p% of probability mass\n#   - top_k: keep only the top k most likely next words\n#   - do_sample: whether to sample or take the most likely word\n\ngeneration_kwargs = {\n    \"max_new_tokens\": 150,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"top_k\": 50,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,  # avoid warnings\n}\nprint(\"Generation parameters:\", generation_kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Tip**: Lower `temperature` (e.g., 0.3) for more factual, deterministic answers. Raise it (e.g., 1.0) for creative or whimsical responses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3Ô∏è‚É£  Run the generation\n# We‚Äôll use a simple prompt and let the model finish the sentence.\nprompt = \"Once upon a time in a quiet village, a mysterious traveler arrived\"\n\n# Encode the prompt into token IDs\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Move to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\ninput_ids = input_ids.to(device)\n\n# Generate\ngenerated_ids = model.generate(input_ids, **generation_kwargs)\n\n# Decode the output (skip the prompt part)\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\nprint(\"\\nGenerated text:\\n\")\nprint(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üìù **Note**: The `skip_special_tokens=True` flag removes invisible tokens like `<eos>` that the model uses internally. If you want to see the raw token IDs, set it to `False`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick sanity check\n\nIf the output looks garbled or the model hangs, try:\n\n1. **Restart the kernel** ‚Äì sometimes GPU memory gets stuck.\n2. **Reduce `max_new_tokens`** ‚Äì a very long generation can exhaust memory.\n3. **Check the model name** ‚Äì ensure you loaded the correct GGUF file.\n\nHappy generating!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Prompt Engineering & Generation Controls\n\nNow that you can get TinyLlama to write something, let‚Äôs learn how to *guide* it. Think of the model as a chef: the prompt is the recipe, and the generation parameters are the cooking settings (heat, time, seasoning). By tweaking these, you can make the output more factual, more creative, or just shorter.\n\nWe‚Äôll cover:\n\n1. **Prompt structure** ‚Äì how to format the prompt for chat.\n2. **Temperature & sampling** ‚Äì control randomness.\n3. **Top‚Äëp / Top‚Äëk** ‚Äì narrow the word choices.\n4. **Repetition penalty & length penalty** ‚Äì avoid loops.\n5. **Stopping sequences** ‚Äì tell the model when to stop.\n\nAll code snippets are tiny and ready to run.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£  Build a chat‚Äëstyle prompt\n# TinyLlama expects a conversation format:\n# \"User: <your question>\\nAssistant:\"  \n# This helps the model understand the role.\n\nuser_question = \"Explain quantum entanglement in simple terms.\"\nprompt = f\"User: {user_question}\\nAssistant:\"\nprint(\"Prompt:\\n\", prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Tip**: Adding a short instruction before the user question (e.g., \"You are a friendly teacher.\") can nudge the style.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2Ô∏è‚É£  Temperature & sampling\n# Temperature: 0 = deterministic, 1 = fully random\n# Do_sample: True means the model will sample from the distribution.\n\ngeneration_kwargs = {\n    \"max_new_tokens\": 200,\n    \"temperature\": 0.5,  # moderate creativity\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n}\nprint(\"Generation kwargs (temp=0.5):\", generation_kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "‚ö†Ô∏è **Warning**: Very low temperature (e.g., 0) can make the model repeat the same phrase over and over.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3Ô∏è‚É£  Top‚Äëp (nucleus) and Top‚Äëk sampling\n# These limit the pool of next‚Äëword candidates.\n\ngeneration_kwargs.update({\n    \"top_p\": 0.9,   # keep the top 90% probability mass\n    \"top_k\": 50,    # keep only the 50 most likely words\n})\nprint(\"Updated kwargs (top_p=0.9, top_k=50):\", generation_kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üìù **Note**: Setting both top_p and top_k is fine; the model will apply the stricter of the two.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4Ô∏è‚É£  Repetition & length penalties\n# These discourage the model from looping or being too short.\n\ngeneration_kwargs.update({\n    \"repetition_penalty\": 1.2,  # >1 discourages repeats\n    \"length_penalty\": 1.0,      # >1 favors longer outputs\n})\nprint(\"Final kwargs:\", generation_kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Tip**: If the model keeps saying \"Sure\" or \"Here is\", increase the repetition_penalty.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5Ô∏è‚É£  Stopping sequences\n# Tell the model to stop when it sees a certain token.\n# For chat, we often stop at a new \"User:\" prompt.\n\ngeneration_kwargs.update({\n    \"stop_strings\": [\"\\nUser:\"]\n})\nprint(\"With stop_strings:\", generation_kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the tuned generation\n\n```python\n# Encode prompt\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\ninput_ids = input_ids.to(device)\n\n# Generate\ngenerated_ids = model.generate(input_ids, **generation_kwargs)\n\n# Decode, skipping the prompt part\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\nprint(\"\\nGenerated answer:\\n\", generated_text)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to experiment, change one parameter at a time and compare the outputs. That‚Äôs the essence of prompt engineering!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Batch Inference for Performance\n\nContent for Step 6: Batch Inference for Performance will be added here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Fine‚ÄëTuning on a Tiny Dataset\n\nContent for Step 7: Fine‚ÄëTuning on a Tiny Dataset will be added here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ Deploying TinyLlama as a FastAPI Chat Service\n\nIn the previous steps we learned how to generate text locally. Now we‚Äôll expose that capability as a tiny, production‚Äëready web service using **FastAPI**. Think of FastAPI as a friendly waiter that takes a customer‚Äôs request (the prompt), asks the chef (TinyLlama) for a dish (the answer), and brings it back in a nicely formatted JSON.\n\nWe‚Äôll cover:\n\n1. **Environment setup** ‚Äì install FastAPI and Uvicorn.\n2. **Model loading** ‚Äì load the GGUF model once at startup.\n3. **Endpoint design** ‚Äì a `/chat` POST endpoint that accepts a prompt.\n4. **Streaming (optional)** ‚Äì how to stream tokens back to the client.\n5. **Running locally** ‚Äì launch the server with `uvicorn`.\n6. **Testing** ‚Äì a quick `curl` example.\n\nAll code is split into small cells so you can copy‚Äëpaste and run them one by one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£  Install FastAPI & Uvicorn (run once)\n# pip install fastapi uvicorn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "‚ö†Ô∏è **Warning**: If you‚Äôre on Windows, you may need to run `pip install uvicorn[standard]` to get the ASGI server working.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2Ô∏è‚É£  Import libraries and set a reproducible seed\nimport os\nimport random\nimport torch\nfrom fastapi import FastAPI, Request\nfrom pydantic import BaseModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nSEED = 42\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# Optional: use a Hugging Face token for private models\nHF_TOKEN = os.getenv(\"HF_TOKEN\", None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Tip**: Store your `HF_TOKEN` in a `.env` file and load it with `python-dotenv` for convenience.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3Ô∏è‚É£  Load the TinyLlama GGUF model once at startup\nMODEL_ID = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n\n# The GGUF file is automatically downloaded by the hub\n# `trust_remote_code=True` allows the model to load custom code\nprint(\"Loading tokenizer and model‚Ä¶\")\n\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_ID,\n    use_fast=False,\n    trust_remote_code=True,\n    token=HF_TOKEN,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    device_map=\"auto\",  # let transformers pick GPU/CPU\n    trust_remote_code=True,\n    token=HF_TOKEN,\n)\nmodel.eval()  # disable dropout\nprint(\"Model loaded on\", next(model.parameters()).device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "‚ö†Ô∏è **Warning**: Loading the model can take 30‚Äì60‚ÄØs on a laptop. Keep the process running; the server will start only after the model is ready.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£  Define the FastAPI app and request schema\n\nWe‚Äôll create a simple Pydantic model for the request body. The client sends a JSON object with a `prompt` field, and optionally a `max_new_tokens` field.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ChatRequest(BaseModel):\n    prompt: str\n    max_new_tokens: int | None = 150\n\napp = FastAPI(title=\"TinyLlama Chat API\", description=\"Generate chat responses with TinyLlama 1.1B\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£  Create the `/chat` endpoint\n\nThe endpoint will:\n\n1. Encode the prompt.\n2. Run inference inside `torch.no_grad()` to save memory.\n3. Return the decoded text.\n\nWe‚Äôll also support a very lightweight streaming mode by yielding tokens one by one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import AsyncGenerator\n\n@app.post(\"/chat\")\nasync def chat(request: ChatRequest):\n    \"\"\"Return a generated response for the given prompt.\n\n    Parameters\n    ----------\n    request.prompt: str\n        The user prompt.\n    request.max_new_tokens: int, optional\n        How many new tokens to generate.\n    \"\"\"\n    # Encode prompt\n    input_ids = tokenizer(request.prompt, return_tensors=\"pt\").input_ids\n    input_ids = input_ids.to(next(model.parameters()).device)\n\n    # Generation kwargs ‚Äì tweak as needed\n    gen_kwargs = {\n        \"max_new_tokens\": request.max_new_tokens or 150,\n        \"temperature\": 0.7,\n        \"top_p\": 0.9,\n        \"top_k\": 50,\n        \"do_sample\": True,\n        \"pad_token_id\": tokenizer.eos_token_id,\n    }\n\n    with torch.no_grad():\n        output_ids = model.generate(input_ids, **gen_kwargs)\n\n    response_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return {\"response\": response_text}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Streaming endpoint\n\nIf you want to stream tokens back to the client (e.g., for a chat UI), you can use the following endpoint. It yields one token at a time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@app.post(\"/chat-stream\")\nasync def chat_stream(request: ChatRequest) -> AsyncGenerator[str, None]:\n    input_ids = tokenizer(request.prompt, return_tensors=\"pt\").input_ids\n    input_ids = input_ids.to(next(model.parameters()).device)\n\n    gen_kwargs = {\n        \"max_new_tokens\": request.max_new_tokens or 150,\n        \"temperature\": 0.7,\n        \"top_p\": 0.9,\n        \"top_k\": 50,\n        \"do_sample\": True,\n        \"pad_token_id\": tokenizer.eos_token_id,\n        \"streaming\": True,  # enable streaming in transformers 4.35\n    }\n\n    with torch.no_grad():\n        for token_id in model.generate(input_ids, **gen_kwargs):\n            token_text = tokenizer.decode(token_id, skip_special_tokens=True)\n            yield token_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£  Run the server locally\n\n```bash\nuvicorn main:app --host 0.0.0.0 --port 8000 --reload\n```\n\n- `main` is the name of the Python file (e.g., `main.py`).\n- `--reload` restarts the server automatically when you edit the code.\n\nYou should see a message like:\n\n```\nINFO:     Started server process [12345]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£  Test the API with `curl`\n\n```bash\ncurl -X POST http://localhost:8000/chat \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"prompt\": \"Hello, TinyLlama! What is the capital of France?\"}'\n```\n\nYou should receive a JSON response:\n\n```json\n{\"response\": \"The capital of France is Paris.\"}\n```\n\nFor streaming, you can use `curl` with `-N` to see incremental output:\n\n```bash\ncurl -N -X POST http://localhost:8000/chat-stream \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"prompt\": \"Tell me a short joke.\"}'\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£  Deploying to the Cloud or Docker\n\nOnce you‚Äôre happy locally, you can:\n\n- **Dockerize** the app:\n  ```dockerfile\n  FROM python:3.10-slim\n  WORKDIR /app\n  COPY requirements.txt .\n  RUN pip install -r requirements.txt\n  COPY . .\n  CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n  ```\n- **Deploy** to a cloud platform (e.g., AWS Lambda via AWS‚ÄëSAM, Google Cloud Run, or Azure Functions). FastAPI is fully compatible with ASGI servers, so the transition is smooth.\n\nüí° **Tip**: Use environment variables to pass the `HF_TOKEN` securely in production.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check\n\nTest your understanding with these questions:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1\n\n",
        "Which library is required to load GGUF models in Hugging Face Transformers?\n\n",
        "A. torch\n",
        "B. transformers\n",
        "C. accelerate\n",
        "D. huggingface_hub\n",
        "\n**Answer:** B\n\n",
        "**Explanation:** The `transformers` library provides the `AutoModelForCausalLM.from_pretrained` method that supports GGUF format via the `trust_remote_code=True` flag.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}