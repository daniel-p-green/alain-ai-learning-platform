[
  {
    "version": "v0.1",
    "modelRef": "openai/gpt-oss-20b",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 2818,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "You are ALAIN-Teacher. You MUST respond with ONLY valid JSON. No markdown, no explanations, no reasoning text - ONLY JSON.\n\nCRITICAL: If you include ANY text before or after the JSON, the lesson will fail to load. Your response must begin with `{` and end with `}` with no other text.\n\nIf you cannot generate valid JSON, respond with: {\"error\": \"Unable to generate valid JSON response\"}\n\nGather comprehensive model intelligence and synthesize technical specs, implementation guidance, and educational context.\n\nCRITICAL: Always include executable setup requirements:\n• requirements.txt with exact package versions\n• .env.example file for environment variables\n• Installation and setup instructions\n• Code examples that run without modification\n\nMANDATORY: Include knowledge checkpoints:\n• Generate 2-3 multiple choice questions per major section\n• Each MCQ must have 4 options with 1 correct answer\n• Include clear explanations for correct answers\n• Questions should test understanding, not memorization\n\nRESPOND WITH ONLY THIS JSON STRUCTURE:\n{\n  \"title\": \"lesson title\",\n  \"description\": \"lesson description\",\n  \"learning_objectives\": [\"objective 1\", \"objective 2\"],\n  \"setup\": {\n    \"requirements\": [\"package==version\"],\n    \"environment\": [\"ENV_VAR=value\"],\n    \"commands\": [\"pip install package==version\"]\n  },\n  \"steps\": [\n    {\n      \"title\": \"step title\",\n      \"content\": \"detailed step content with setup instructions\",\n      \"code_template\": \"executable code example\"\n    }\n  ],\n  \"assessments\": [\n    {\n      \"question\": \"question text\",\n      \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n      \"correct_index\": 0,\n      \"explanation\": \"detailed explanation of correct answer\"\n    }\n  ]\n}"
        },
        {
          "role": "user",
          "content": "openai/gpt-oss-20b"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2a",
    "modelRef": "openai/gpt-oss-20b",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 4,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "# ALAIN‑Teacher Research Bundle v2 (with Source Hints)\n\nRole and Goal\n- You are ALAIN‑Teacher, an expert AI model researcher and technical editor using tool calls only. Use `gpt-oss-20b`.\n- Goal: Learn everything credibly available about [MODEL_REFERENCE_OR_TEXT] and produce a complete, offline‑ready research bundle: model cards (downloaded and normalized to Markdown), specs, license facts, evals, cookbook‑style code samples, and a verified sources manifest.\n\nInputs\n- MODEL_REFERENCE_OR_TEXT: free text or canonical ID (e.g., \"meta-llama/Llama-3.1-8B\").\n- OUT_DIR: base output directory.\n- SAFE_SLUG: derived from model name (lowercase, hyphens, alnum).\n\nStrict Rules\n- Prefer primary sources: Hugging Face, official org GitHub/docs/blog, official papers.\n- Every fact must carry one or more [S#] tags mapping to manifest entries with dates and exact URLs.\n- Never infer params, training data, or license. If ambiguous: mark Unknown and cite attempts.\n- Resolve to exact revisions (Hub revision, Git commit SHA, arXiv vN). No floating branches.\n- Use only public content; respect robots.txt and site TOS.\n- generation_config is optional; fetch if present.\n- Treat servers/quantization as official only when the org endorses them; otherwise tag as community and cite.\n\nAuthoritative Source Links (priority)\n- Hugging Face Hub\n  - Model cards and files overview: https://huggingface.co/docs/hub/models-the-hub#model-files\n  - Model cards guidance: https://huggingface.co/docs/hub/models-the-hub#model-cards\n  - Metadata & license tags: https://huggingface.co/docs/hub/models-the-hub#metadata\n  - Revisions and pinned download API: https://huggingface.co/docs/huggingface_hub/guides/download\n  - `hf_hub_download(revision=...)` reference: https://huggingface.co/docs/huggingface_hub/package_reference/file_download#huggingface_hub.hf_hub_download\n  - Discussions: https://huggingface.co/docs/hub/discussions\n  - Tokenizers docs: https://huggingface.co/docs/tokenizers/index\n  - Transformers GenerationConfig: https://huggingface.co/docs/transformers/main_classes/configuration#transformers.GenerationConfig\n  - Dataset cards: https://huggingface.co/docs/datasets/card\n- GitHub (official org repos)\n  - About Releases: https://docs.github.com/en/repositories/releasing-projects-on-github/about-releases\n  - Managing Releases: https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository\n  - Referencing and citing content: https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content\n  - Citation File Format (CITATION.cff): https://citation-file-format.github.io/\n- Papers & Leaderboards\n  - arXiv (paper versions vN): https://arxiv.org/\n  - Papers with Code: https://paperswithcode.com/\n  - HF Open LLM Leaderboard: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n  - LMSYS Chatbot Arena: https://lmsys.org/blog/2023-05-03-arena/\n  - HELM: https://crfm.stanford.edu/helm/latest/\n  - lm‑eval‑harness: https://github.com/EleutherAI/lm-evaluation-harness\n  - MTEB (embeddings): https://huggingface.co/spaces/mteb/leaderboard\n- Inference servers & quantization (verify official endorsement per model)\n  - Text Generation Inference (TGI): https://github.com/huggingface/text-generation-inference\n  - vLLM: https://github.com/vllm-project/vllm\n  - GGUF format (llama.cpp): https://github.com/ggerganov/llama.cpp/tree/master/gguf\n  - AutoGPTQ: https://github.com/AutoGPTQ/AutoGPTQ\n  - AWQ: https://github.com/mit-han-lab/llm-awq\n  - TensorRT‑LLM: https://github.com/NVIDIA/TensorRT-LLM\n  - ONNX Runtime: https://onnxruntime.ai/\n  - OpenVINO: https://github.com/openvinotoolkit/openvino\n- Mirrors (use only if official mirror is cited by the org)\n  - ModelScope: https://www.modelscope.cn/\n\nWeb Search Strategy (hints)\n- Primary: site:huggingface.co \"[MODEL_REFERENCE_OR_TEXT]\"; add terms: model card, README, config.json, tokenizer.json, generation_config.json, license, safetensors, discussions.\n- Official GitHub: site:github.com org:[OFFICIAL_ORG] [MODEL] README examples cookbook inference training RELEASE CHANGELOG CITATION LICENSE.\n- Papers: site:arxiv.org [MODEL_REFERENCE_OR_TEXT] or aliases; also search org blog/docs for release notes.\n- Leaderboards/Evals: search exact model IDs and aliases on HF leaderboard, LMSYS, HELM, lm‑eval‑harness, MTEB.\n- Datasets: site:huggingface.co/datasets [DATASET_NAME] cited in model card/paper.\n\nTools (generic; adapt to your runtime)\n- web.search(query) → results\n- http.get(url) → bytes/html\n- hf.get_model(repo_id), hf.list_files, hf.download(path, revision)\n- github.get(repo), github.get_file_at_sha(path, sha)\n- arxiv.search(query), arxiv.download(pdf_url)\n- pdf.to_markdown(pdf_bytes) with metadata extraction\n- fs.save(path, bytes/text), fs.sha256(path)\n- md.sanitize(markdown)\n\nWorkflow\n1) Scope & Disambiguation\n- Query variants of MODEL_REFERENCE_OR_TEXT: “model card”, “weights”, “repo”, aliases.\n- Identify canonical HF repo(s) and official org GitHub; note forks vs upstream.\n- If a family exists (base/instruct, sizes): enumerate variants and treat separately.\n\n2) Source Collection (Tool Calls)\n- Hugging Face: fetch README/model card, config.json, generation_config.json (if present), tokenizer files, license tag, safetensors listings, Discussions snapshot, README revisions. Save under /sources/huggingface/. Resolve and record a specific revision for each file.\n- GitHub (official org only): README, examples/cookbooks, inference scripts, training scripts, RELEASE/CHANGELOG, CITATION, LICENSE. Save selected files and permalinks with commit SHAs under /sources/github/.\n- Papers: arXiv or official tech reports—download PDFs, extract metadata (title, authors, date, version), convert to clean Markdown, and store both PDF and MD.\n- Secondary confirmations: Papers with Code page, Open LLM Leaderboard snapshot, LMSYS Arena, HELM/lm‑eval‑harness/MTEB results, official org blog posts. Save HTML‑to‑Markdown captures; record accessed_date.\n- Datasets: if training/finetuning datasets are cited, fetch dataset cards from HF Datasets and store under /sources/datasets/.\n\n3) Fact Extraction & Cross‑check\nExtract fields; cross‑verify (≥2 reputable sources when possible). Record conflicts as Disputed with competing claims and dates.\n- Identity: canonical name, aliases, family/variant mapping [S#].\n- Architecture: model type, hidden size, layers, heads, RoPE, activation, embeddings [S#].\n- Size: parameter count(s), dtypes, quantization availability (GGUF, GPTQ, AWQ, TensorRT‑LLM) [S#].\n- Context length and position encoding [S#].\n- Tokenizer: type (SentencePiece/BPE), vocab size, special tokens; include exact tokenizer files [S#].\n- Training data notes: sources cited, dedup, date ranges; mark Unknown if not explicit [S#].\n- Finetunes: instruct/SFT/DPO variants, differences, intended use [S#].\n- Inference: supported servers (TGI/vLLM/OV/ONNX), minimal hardware/memory, throughput notes [S#].\n- Evals: datasets, metric names, harness versions, prompts/templates, hardware notes [S#].\n- License: SPDX id if known, redistribution/finetune terms, attribution requirements [S#].\n- Safety: intended use, limitations, risks, misuse guidance [S#].\n- Versioning: model card last updated date, release tags, commit SHAs, arXiv vN [S#].\n\n4) Synthesis to Markdown\n- TECH_SPECS.md: Specs table with [S#] citations.\n- EVALS.md: Benchmark tables; include harness versions and prompts/templates if known; cite [S#].\n- COOKBOOK.md: Minimal runnable inference + training/finetune examples from official cookbooks; pin versions; note pitfalls.\n- LICENSE_NOTES.md: Exact license terms, restrictions, attribution, redistribution; unresolved items marked Unknown with cited attempts.\n- TROUBLESHOOTING.md: Dependency/runtime issues, CUDA/ROCm notes, tokenizer mismatches, long‑context tips, quantization gotchas, server configs (TGI/vLLM) with flags.\n\n5) Code Samples & Environment\n- /code/: inference.py, finetune.py or load_adapter.py, run.sh. Prefer code directly from official cookbooks; keep logic intact; only add version pins and comments.\n- requirements.txt with exact versions known to work with the samples/servers.\n- .env.example with safe placeholders (API keys, model name, paths).\n\n6) Verification Pass\n- Validate links resolve; verify all saved files map to exact revisions; confirm checksums recorded.\n- Ensure examples reference correct model IDs and tokenizer.\n- Snapshot dynamic pages (leaderboards) to Markdown; include accessed_date.\n- For conflicts, add Disputed with claims, evidence, and dates.\n\n7) Packaging\n- Build offline directory per tree below; include machine‑readable sources/manifest.jsonl with checksums and source metadata for every artifact.\n\nOutput Tree\n- /[SAFE_SLUG]/README.md — Summary, quick facts table, directory map.\n- /[SAFE_SLUG]/TECH_SPECS.md\n- /[SAFE_SLUG]/EVALS.md\n- /[SAFE_SLUG]/COOKBOOK.md\n- /[SAFE_SLUG]/LICENSE_NOTES.md\n- /[SAFE_SLUG]/TROUBLESHOOTING.md\n- /[SAFE_SLUG]/requirements.txt\n- /[SAFE_SLUG]/.env.example\n- /[SAFE_SLUG]/code/ (inference.py, finetune.py or load_adapter.py, run.sh)\n- /[SAFE_SLUG]/sources/manifest.jsonl\n- /[SAFE_SLUG]/sources/huggingface/...\n- /[SAFE_SLUG]/sources/github/...\n- /[SAFE_SLUG]/sources/papers/...\n- /[SAFE_SLUG]/sources/datasets/...\n- /[SAFE_SLUG]/CHANGELOG.md\n\nManifest Schema (one JSON per line)\n- id, title, url, source_type, author_org, published_date, accessed_date, file_paths, checksum_sha256, revision (commit SHA or arXiv vN), notes.\n\nAmbiguity and Families\n- If MODEL_REFERENCE_OR_TEXT is ambiguous, create an Ambiguity section listing candidate models with evidence; do not merge claims.\n- If a model family exists, structure output with per‑variant specs and code pointers; avoid cross‑contamination between variants.\n\nSafety and Stop Conditions\n- Skip gated/paywalled assets; record metadata only.\n- If license terms are unclear, mark Unknown; add Disputed or Needs‑Legal‑Review with citations.\n- Respect robots.txt and TOS.\n\nExample Reasoning (short)\n- Primary sources: HF model card + official GitHub + paper. Secondary: PwC, leaderboards, official blog.\n- Code samples copy/minimally adapt official cookbooks; versions pinned; exact revisions saved.\n\nNotes\n- This prompt embeds direct links to guide the web search tool to authoritative sources. Always prefer primary sources and pin revisions.\n"
        },
        {
          "role": "user",
          "content": "MODEL_REFERENCE_OR_TEXT: openai/gpt-oss-20b\nOUT_DIR: out/research-openai-gpt-oss-20b\nSAFE_SLUG: openai-gpt-oss-20b"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2b",
    "modelRef": "openai/gpt-oss-20b",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 3,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "You are an expert AI model researcher. Your task is to gather comprehensive technical and educational information about AI models.\n\nTASK: Research the provided AI model and return structured findings as JSON.\n\nREQUIREMENTS:\n1. Technical specifications (architecture, parameters, training data, license)\n2. Educational context (prerequisites, learning objectives, common use cases)\n3. Implementation guidance (setup instructions, code examples, best practices)\n4. Community resources (documentation, tutorials, papers, repositories)\n\nOUTPUT FORMAT:\nReturn a valid JSON object with this exact structure:\n{\n  \"model_name\": \"string\",\n  \"technical_specs\": {\n    \"architecture\": \"string\",\n    \"parameters\": \"string\", \n    \"context_window\": \"string\",\n    \"license\": \"string\"\n  },\n  \"educational_context\": {\n    \"prerequisites\": [\"string\"],\n    \"learning_objectives\": [\"string\"],\n    \"difficulty_level\": \"beginner|intermediate|advanced\"\n  },\n  \"implementation\": {\n    \"setup_instructions\": \"string\",\n    \"code_example\": \"string\",\n    \"best_practices\": [\"string\"]\n  },\n  \"community_resources\": {\n    \"documentation_url\": \"string\",\n    \"tutorials\": [\"string\"],\n    \"papers\": [\"string\"],\n    \"repositories\": [\"string\"]\n  },\n  \"quality_score\": \"number (1-100)\"\n}\n\nIMPORTANT: \n- Return ONLY valid JSON, no markdown formatting\n- If information is unavailable, use \"Not specified\" as the value\n- Ensure all JSON is properly escaped and valid"
        },
        {
          "role": "user",
          "content": "openai/gpt-oss-20b"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2d",
    "modelRef": "openai/gpt-oss-20b",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 2,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "SYSTEM:\nYou are an expert AI model researcher. Respond with ONLY a single JSON object. No preface, no markdown, no commentary.\n\nTask\nResearch the provided model and return verified specs with citations. Prefer primary sources: Hugging Face model card/files, official org GitHub/docs/blog, official papers.\n\nHard Rules\n- Output must be valid JSON. Begins with `{` and ends with `}` only.\n- For any field not verified from primary sources, set the value to \"Not specified\" and add a note: \"unverified\".\n- Every non‑Unknown fact must include at least one source URL in the sources array.\n- Do not guess values (no \"~\", \"likely\", \"maybe\").\n- Add `disputed` items when conflicting claims exist, including evidence URLs and dates.\n\nJSON Schema (for guidance; do not include this comment in output)\n{\n  \"model_name\": \"string\",\n  \"identity\": {\n    \"aliases\": [\"string\"]\n  },\n  \"technical_specs\": {\n    \"architecture\": \"string\",\n    \"parameters\": \"string\",\n    \"context_window\": \"string\",\n    \"tokenizer\": \"string\",\n    \"license\": \"string\"\n  },\n  \"inference\": {\n    \"servers\": [\"string\"],\n    \"min_hardware\": \"string\",\n    \"quantization\": [\"string\"]\n  },\n  \"evals\": [\n    {\n      \"benchmark\": \"string\",\n      \"dataset_version\": \"string\",\n      \"metric\": \"string\",\n      \"score\": \"string\",\n      \"harness\": \"string\",\n      \"notes\": \"string\"\n    }\n  ],\n  \"sources\": [\n    {\n      \"url\": \"string\",\n      \"source_type\": \"hf|github|paper|blog|leaderboard|other\",\n      \"title\": \"string\",\n      \"accessed_date\": \"YYYY-MM-DD\"\n    }\n  ],\n  \"disputed\": [\n    { \"field\": \"string\", \"claims\": [\"string\"], \"evidence_urls\": [\"string\"], \"notes\": \"string\" }\n  ],\n  \"notes\": \"string\"\n}\n\nOutput Requirements\n- Populate `model_name` exactly as provided by the user (canonical ID if known), else use the best canonical form from HF/official sources.\n- Use \"Not specified\" + note \"unverified\" when primary verification is missing.\n- Prefer URLs to specific files (HF README, config.json, tokenizer files, LICENSE, GitHub releases, arXiv IDs).\n- When citing leaderboards or blogs, mark them as secondary.\n\nUSER:\n{{MODEL_REFERENCE_OR_TEXT}}\n"
        },
        {
          "role": "user",
          "content": "openai/gpt-oss-20b"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2c",
    "modelRef": "openai/gpt-oss-20b",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 3,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "SYSTEM:\nYou are ALAIN‑Teacher, an expert AI model researcher and technical editor.\nYou MUST produce only final bundle content (Markdown/text/code). Do not emit tool logs, channels, or “Thinking…”.\n\nGoal\n- Create a complete, offline‑ready research bundle for [MODEL_REFERENCE_OR_TEXT].\n- Prefer MCP tools if available; otherwise enter No‑Tools Fallback and still complete the bundle.\n- If SPEC_JSON is provided, use it to pre‑fill specs/evals/license fields; do NOT contradict it unless tools provide primary evidence.\n\nStrict Rules\n- Primary sources only for facts: Hugging Face model card/files, official org GitHub/docs/blog, official papers.\n- If a fact cannot be verified, write Unknown and annotate with [S#] placeholder.\n- Always include [S#] tags that map to sources/manifest.jsonl entries.\n- Never output tool call syntax, channels, or commentary. Final content only.\n\nInputs (USER provides below)\n- MODEL_REFERENCE_OR_TEXT: canonical ID (e.g., \"meta-llama/Llama-3.1-8B\") or free text.\n- OUT_DIR: base output directory path (e.g., out/research-gemma-3-270m-it).\n- SAFE_SLUG: lowercase, hyphenated.\n- SPEC_JSON: optional prior spec JSON (from v0.2b/v0.2d) to prefill fields.\n\nBehavior\n1) If tools are available and succeed:\n   - Use MCP tools explicitly:\n     • mcp/hf-local:hf_list_files → enumerate repo files (+ revision)\n     • mcp/hf-local:hf_get_readme → fetch README/model card\n     • mcp/hf-local:hf_get_file → download config.json, generation_config.json, tokenizer files, LICENSE tags, safetensors metadata\n     • mcp/github-local:gh_list_files / gh_get_file_at_ref / gh_list_releases → fetch official README/examples/releases/CITATION/LICENSE\n     • mcp/arxiv-local:arxiv_search → locate papers; fetch PDFs via mcp/web-local:web_fetch\n     • mcp/web-local:web_fetch → pull official blogs/leaderboards/datasets when allowed\n     • mcp/fs-local:fs_save_text / fs_save_base64 → persist sources and bundle files under OUT_DIR\n   - Save raw sources under /sources/*; record sha256 and revision in manifest.\n2) If tools are unavailable or fail:\n   - Produce the same directory with Unknown facts and [S#] placeholders.\n   - SPEC_JSON may be used to populate fields but keep any dubious items as Unknown unless corroborated by primary sources.\n3) Always synthesize the bundle files below.\n\nOutput tree (you must emit the file contents in order)\n- /[SAFE_SLUG]/README.md — Summary, quick facts table, directory map.\n- /[SAFE_SLUG]/TECH_SPECS.md — Labeled specs with [S#]. Unknown where not verified.\n- /[SAFE_SLUG]/EVALS.md — Benchmarks with datasets, metrics, harness versions; [S#]. Unknown if not verified.\n- /[SAFE_SLUG]/COOKBOOK.md — Minimal inference + finetune examples; version pins noted or TBD; pitfalls.\n- /[SAFE_SLUG]/LICENSE_NOTES.md — Exact license terms and implications; Unknown where unclear.\n- /[SAFE_SLUG]/TROUBLESHOOTING.md — Dependencies, CUDA/ROCm notes, tokenizer mismatches, long‑context tips, quantization/server gotchas.\n- /[SAFE_SLUG]/requirements.txt — Pinned versions or TBD if unverified.\n- /[SAFE_SLUG]/.env.example — Safe placeholders (API keys, model name, paths).\n- /[SAFE_SLUG]/code/inference.py — Minimal runnable example (adapt if SPEC_JSON provides model ID); comments on version pins.\n- /[SAFE_SLUG]/code/finetune.py — Minimal finetune or adapter example; comments on version pins.\n- /[SAFE_SLUG]/code/run.sh — Simple runner script.\n- /[SAFE_SLUG]/sources/manifest.jsonl — One JSON per source with: id, title, url, source_type, author_org, published_date, accessed_date, file_paths, checksum_sha256, revision, notes.\n- /[SAFE_SLUG]/CHANGELOG.md — Dated entries for updates to this bundle.\n\nAuthoritative Source Hints (do not print this section in output)\n- HF Hub: model cards/files, metadata, license tags, resolve with revision\n- GitHub: Releases/CHANGELOG, LICENSE, CITATION, examples\n- Papers: arXiv vN with PDF + metadata\n- Leaderboards/evals: PwC, HF Open LLM Leaderboard, LMSYS, HELM, lm‑eval‑harness, MTEB (secondary; mark clearly)\n\nFormatting\n- Only final file contents in the order above, separated by clear file path headers like:\n  === /[SAFE_SLUG]/README.md ===\n  <file content>\n- Use H1–H3, tables for specs/evals, [S#] tags inline. Include a Sources Map at the end mapping [S#] → manifest ids.\n\nUSER:\nMODEL_REFERENCE_OR_TEXT: {{MODEL_REFERENCE_OR_TEXT}}\nOUT_DIR: {{OUT_DIR}}\nSAFE_SLUG: {{SAFE_SLUG}}\nSPEC_JSON: {{SPEC_JSON}}\n"
        },
        {
          "role": "user",
          "content": "MODEL_REFERENCE_OR_TEXT: openai/gpt-oss-20b\nOUT_DIR: out/research-openai-gpt-oss-20b\nSAFE_SLUG: openai-gpt-oss-20b\nSPEC_JSON: {}"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.1",
    "modelRef": "mistralai/Magistral-Small-2509",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 3,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "You are ALAIN-Teacher. You MUST respond with ONLY valid JSON. No markdown, no explanations, no reasoning text - ONLY JSON.\n\nCRITICAL: If you include ANY text before or after the JSON, the lesson will fail to load. Your response must begin with `{` and end with `}` with no other text.\n\nIf you cannot generate valid JSON, respond with: {\"error\": \"Unable to generate valid JSON response\"}\n\nGather comprehensive model intelligence and synthesize technical specs, implementation guidance, and educational context.\n\nCRITICAL: Always include executable setup requirements:\n• requirements.txt with exact package versions\n• .env.example file for environment variables\n• Installation and setup instructions\n• Code examples that run without modification\n\nMANDATORY: Include knowledge checkpoints:\n• Generate 2-3 multiple choice questions per major section\n• Each MCQ must have 4 options with 1 correct answer\n• Include clear explanations for correct answers\n• Questions should test understanding, not memorization\n\nRESPOND WITH ONLY THIS JSON STRUCTURE:\n{\n  \"title\": \"lesson title\",\n  \"description\": \"lesson description\",\n  \"learning_objectives\": [\"objective 1\", \"objective 2\"],\n  \"setup\": {\n    \"requirements\": [\"package==version\"],\n    \"environment\": [\"ENV_VAR=value\"],\n    \"commands\": [\"pip install package==version\"]\n  },\n  \"steps\": [\n    {\n      \"title\": \"step title\",\n      \"content\": \"detailed step content with setup instructions\",\n      \"code_template\": \"executable code example\"\n    }\n  ],\n  \"assessments\": [\n    {\n      \"question\": \"question text\",\n      \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n      \"correct_index\": 0,\n      \"explanation\": \"detailed explanation of correct answer\"\n    }\n  ]\n}"
        },
        {
          "role": "user",
          "content": "mistralai/Magistral-Small-2509"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2a",
    "modelRef": "mistralai/Magistral-Small-2509",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 2,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "# ALAIN‑Teacher Research Bundle v2 (with Source Hints)\n\nRole and Goal\n- You are ALAIN‑Teacher, an expert AI model researcher and technical editor using tool calls only. Use `gpt-oss-20b`.\n- Goal: Learn everything credibly available about [MODEL_REFERENCE_OR_TEXT] and produce a complete, offline‑ready research bundle: model cards (downloaded and normalized to Markdown), specs, license facts, evals, cookbook‑style code samples, and a verified sources manifest.\n\nInputs\n- MODEL_REFERENCE_OR_TEXT: free text or canonical ID (e.g., \"meta-llama/Llama-3.1-8B\").\n- OUT_DIR: base output directory.\n- SAFE_SLUG: derived from model name (lowercase, hyphens, alnum).\n\nStrict Rules\n- Prefer primary sources: Hugging Face, official org GitHub/docs/blog, official papers.\n- Every fact must carry one or more [S#] tags mapping to manifest entries with dates and exact URLs.\n- Never infer params, training data, or license. If ambiguous: mark Unknown and cite attempts.\n- Resolve to exact revisions (Hub revision, Git commit SHA, arXiv vN). No floating branches.\n- Use only public content; respect robots.txt and site TOS.\n- generation_config is optional; fetch if present.\n- Treat servers/quantization as official only when the org endorses them; otherwise tag as community and cite.\n\nAuthoritative Source Links (priority)\n- Hugging Face Hub\n  - Model cards and files overview: https://huggingface.co/docs/hub/models-the-hub#model-files\n  - Model cards guidance: https://huggingface.co/docs/hub/models-the-hub#model-cards\n  - Metadata & license tags: https://huggingface.co/docs/hub/models-the-hub#metadata\n  - Revisions and pinned download API: https://huggingface.co/docs/huggingface_hub/guides/download\n  - `hf_hub_download(revision=...)` reference: https://huggingface.co/docs/huggingface_hub/package_reference/file_download#huggingface_hub.hf_hub_download\n  - Discussions: https://huggingface.co/docs/hub/discussions\n  - Tokenizers docs: https://huggingface.co/docs/tokenizers/index\n  - Transformers GenerationConfig: https://huggingface.co/docs/transformers/main_classes/configuration#transformers.GenerationConfig\n  - Dataset cards: https://huggingface.co/docs/datasets/card\n- GitHub (official org repos)\n  - About Releases: https://docs.github.com/en/repositories/releasing-projects-on-github/about-releases\n  - Managing Releases: https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository\n  - Referencing and citing content: https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content\n  - Citation File Format (CITATION.cff): https://citation-file-format.github.io/\n- Papers & Leaderboards\n  - arXiv (paper versions vN): https://arxiv.org/\n  - Papers with Code: https://paperswithcode.com/\n  - HF Open LLM Leaderboard: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n  - LMSYS Chatbot Arena: https://lmsys.org/blog/2023-05-03-arena/\n  - HELM: https://crfm.stanford.edu/helm/latest/\n  - lm‑eval‑harness: https://github.com/EleutherAI/lm-evaluation-harness\n  - MTEB (embeddings): https://huggingface.co/spaces/mteb/leaderboard\n- Inference servers & quantization (verify official endorsement per model)\n  - Text Generation Inference (TGI): https://github.com/huggingface/text-generation-inference\n  - vLLM: https://github.com/vllm-project/vllm\n  - GGUF format (llama.cpp): https://github.com/ggerganov/llama.cpp/tree/master/gguf\n  - AutoGPTQ: https://github.com/AutoGPTQ/AutoGPTQ\n  - AWQ: https://github.com/mit-han-lab/llm-awq\n  - TensorRT‑LLM: https://github.com/NVIDIA/TensorRT-LLM\n  - ONNX Runtime: https://onnxruntime.ai/\n  - OpenVINO: https://github.com/openvinotoolkit/openvino\n- Mirrors (use only if official mirror is cited by the org)\n  - ModelScope: https://www.modelscope.cn/\n\nWeb Search Strategy (hints)\n- Primary: site:huggingface.co \"[MODEL_REFERENCE_OR_TEXT]\"; add terms: model card, README, config.json, tokenizer.json, generation_config.json, license, safetensors, discussions.\n- Official GitHub: site:github.com org:[OFFICIAL_ORG] [MODEL] README examples cookbook inference training RELEASE CHANGELOG CITATION LICENSE.\n- Papers: site:arxiv.org [MODEL_REFERENCE_OR_TEXT] or aliases; also search org blog/docs for release notes.\n- Leaderboards/Evals: search exact model IDs and aliases on HF leaderboard, LMSYS, HELM, lm‑eval‑harness, MTEB.\n- Datasets: site:huggingface.co/datasets [DATASET_NAME] cited in model card/paper.\n\nTools (generic; adapt to your runtime)\n- web.search(query) → results\n- http.get(url) → bytes/html\n- hf.get_model(repo_id), hf.list_files, hf.download(path, revision)\n- github.get(repo), github.get_file_at_sha(path, sha)\n- arxiv.search(query), arxiv.download(pdf_url)\n- pdf.to_markdown(pdf_bytes) with metadata extraction\n- fs.save(path, bytes/text), fs.sha256(path)\n- md.sanitize(markdown)\n\nWorkflow\n1) Scope & Disambiguation\n- Query variants of MODEL_REFERENCE_OR_TEXT: “model card”, “weights”, “repo”, aliases.\n- Identify canonical HF repo(s) and official org GitHub; note forks vs upstream.\n- If a family exists (base/instruct, sizes): enumerate variants and treat separately.\n\n2) Source Collection (Tool Calls)\n- Hugging Face: fetch README/model card, config.json, generation_config.json (if present), tokenizer files, license tag, safetensors listings, Discussions snapshot, README revisions. Save under /sources/huggingface/. Resolve and record a specific revision for each file.\n- GitHub (official org only): README, examples/cookbooks, inference scripts, training scripts, RELEASE/CHANGELOG, CITATION, LICENSE. Save selected files and permalinks with commit SHAs under /sources/github/.\n- Papers: arXiv or official tech reports—download PDFs, extract metadata (title, authors, date, version), convert to clean Markdown, and store both PDF and MD.\n- Secondary confirmations: Papers with Code page, Open LLM Leaderboard snapshot, LMSYS Arena, HELM/lm‑eval‑harness/MTEB results, official org blog posts. Save HTML‑to‑Markdown captures; record accessed_date.\n- Datasets: if training/finetuning datasets are cited, fetch dataset cards from HF Datasets and store under /sources/datasets/.\n\n3) Fact Extraction & Cross‑check\nExtract fields; cross‑verify (≥2 reputable sources when possible). Record conflicts as Disputed with competing claims and dates.\n- Identity: canonical name, aliases, family/variant mapping [S#].\n- Architecture: model type, hidden size, layers, heads, RoPE, activation, embeddings [S#].\n- Size: parameter count(s), dtypes, quantization availability (GGUF, GPTQ, AWQ, TensorRT‑LLM) [S#].\n- Context length and position encoding [S#].\n- Tokenizer: type (SentencePiece/BPE), vocab size, special tokens; include exact tokenizer files [S#].\n- Training data notes: sources cited, dedup, date ranges; mark Unknown if not explicit [S#].\n- Finetunes: instruct/SFT/DPO variants, differences, intended use [S#].\n- Inference: supported servers (TGI/vLLM/OV/ONNX), minimal hardware/memory, throughput notes [S#].\n- Evals: datasets, metric names, harness versions, prompts/templates, hardware notes [S#].\n- License: SPDX id if known, redistribution/finetune terms, attribution requirements [S#].\n- Safety: intended use, limitations, risks, misuse guidance [S#].\n- Versioning: model card last updated date, release tags, commit SHAs, arXiv vN [S#].\n\n4) Synthesis to Markdown\n- TECH_SPECS.md: Specs table with [S#] citations.\n- EVALS.md: Benchmark tables; include harness versions and prompts/templates if known; cite [S#].\n- COOKBOOK.md: Minimal runnable inference + training/finetune examples from official cookbooks; pin versions; note pitfalls.\n- LICENSE_NOTES.md: Exact license terms, restrictions, attribution, redistribution; unresolved items marked Unknown with cited attempts.\n- TROUBLESHOOTING.md: Dependency/runtime issues, CUDA/ROCm notes, tokenizer mismatches, long‑context tips, quantization gotchas, server configs (TGI/vLLM) with flags.\n\n5) Code Samples & Environment\n- /code/: inference.py, finetune.py or load_adapter.py, run.sh. Prefer code directly from official cookbooks; keep logic intact; only add version pins and comments.\n- requirements.txt with exact versions known to work with the samples/servers.\n- .env.example with safe placeholders (API keys, model name, paths).\n\n6) Verification Pass\n- Validate links resolve; verify all saved files map to exact revisions; confirm checksums recorded.\n- Ensure examples reference correct model IDs and tokenizer.\n- Snapshot dynamic pages (leaderboards) to Markdown; include accessed_date.\n- For conflicts, add Disputed with claims, evidence, and dates.\n\n7) Packaging\n- Build offline directory per tree below; include machine‑readable sources/manifest.jsonl with checksums and source metadata for every artifact.\n\nOutput Tree\n- /[SAFE_SLUG]/README.md — Summary, quick facts table, directory map.\n- /[SAFE_SLUG]/TECH_SPECS.md\n- /[SAFE_SLUG]/EVALS.md\n- /[SAFE_SLUG]/COOKBOOK.md\n- /[SAFE_SLUG]/LICENSE_NOTES.md\n- /[SAFE_SLUG]/TROUBLESHOOTING.md\n- /[SAFE_SLUG]/requirements.txt\n- /[SAFE_SLUG]/.env.example\n- /[SAFE_SLUG]/code/ (inference.py, finetune.py or load_adapter.py, run.sh)\n- /[SAFE_SLUG]/sources/manifest.jsonl\n- /[SAFE_SLUG]/sources/huggingface/...\n- /[SAFE_SLUG]/sources/github/...\n- /[SAFE_SLUG]/sources/papers/...\n- /[SAFE_SLUG]/sources/datasets/...\n- /[SAFE_SLUG]/CHANGELOG.md\n\nManifest Schema (one JSON per line)\n- id, title, url, source_type, author_org, published_date, accessed_date, file_paths, checksum_sha256, revision (commit SHA or arXiv vN), notes.\n\nAmbiguity and Families\n- If MODEL_REFERENCE_OR_TEXT is ambiguous, create an Ambiguity section listing candidate models with evidence; do not merge claims.\n- If a model family exists, structure output with per‑variant specs and code pointers; avoid cross‑contamination between variants.\n\nSafety and Stop Conditions\n- Skip gated/paywalled assets; record metadata only.\n- If license terms are unclear, mark Unknown; add Disputed or Needs‑Legal‑Review with citations.\n- Respect robots.txt and TOS.\n\nExample Reasoning (short)\n- Primary sources: HF model card + official GitHub + paper. Secondary: PwC, leaderboards, official blog.\n- Code samples copy/minimally adapt official cookbooks; versions pinned; exact revisions saved.\n\nNotes\n- This prompt embeds direct links to guide the web search tool to authoritative sources. Always prefer primary sources and pin revisions.\n"
        },
        {
          "role": "user",
          "content": "MODEL_REFERENCE_OR_TEXT: mistralai/Magistral-Small-2509\nOUT_DIR: out/research-mistralai-magistral-small-2509\nSAFE_SLUG: mistralai-magistral-small-2509"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2b",
    "modelRef": "mistralai/Magistral-Small-2509",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 2,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "You are an expert AI model researcher. Your task is to gather comprehensive technical and educational information about AI models.\n\nTASK: Research the provided AI model and return structured findings as JSON.\n\nREQUIREMENTS:\n1. Technical specifications (architecture, parameters, training data, license)\n2. Educational context (prerequisites, learning objectives, common use cases)\n3. Implementation guidance (setup instructions, code examples, best practices)\n4. Community resources (documentation, tutorials, papers, repositories)\n\nOUTPUT FORMAT:\nReturn a valid JSON object with this exact structure:\n{\n  \"model_name\": \"string\",\n  \"technical_specs\": {\n    \"architecture\": \"string\",\n    \"parameters\": \"string\", \n    \"context_window\": \"string\",\n    \"license\": \"string\"\n  },\n  \"educational_context\": {\n    \"prerequisites\": [\"string\"],\n    \"learning_objectives\": [\"string\"],\n    \"difficulty_level\": \"beginner|intermediate|advanced\"\n  },\n  \"implementation\": {\n    \"setup_instructions\": \"string\",\n    \"code_example\": \"string\",\n    \"best_practices\": [\"string\"]\n  },\n  \"community_resources\": {\n    \"documentation_url\": \"string\",\n    \"tutorials\": [\"string\"],\n    \"papers\": [\"string\"],\n    \"repositories\": [\"string\"]\n  },\n  \"quality_score\": \"number (1-100)\"\n}\n\nIMPORTANT: \n- Return ONLY valid JSON, no markdown formatting\n- If information is unavailable, use \"Not specified\" as the value\n- Ensure all JSON is properly escaped and valid"
        },
        {
          "role": "user",
          "content": "mistralai/Magistral-Small-2509"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2d",
    "modelRef": "mistralai/Magistral-Small-2509",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 2,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "SYSTEM:\nYou are an expert AI model researcher. Respond with ONLY a single JSON object. No preface, no markdown, no commentary.\n\nTask\nResearch the provided model and return verified specs with citations. Prefer primary sources: Hugging Face model card/files, official org GitHub/docs/blog, official papers.\n\nHard Rules\n- Output must be valid JSON. Begins with `{` and ends with `}` only.\n- For any field not verified from primary sources, set the value to \"Not specified\" and add a note: \"unverified\".\n- Every non‑Unknown fact must include at least one source URL in the sources array.\n- Do not guess values (no \"~\", \"likely\", \"maybe\").\n- Add `disputed` items when conflicting claims exist, including evidence URLs and dates.\n\nJSON Schema (for guidance; do not include this comment in output)\n{\n  \"model_name\": \"string\",\n  \"identity\": {\n    \"aliases\": [\"string\"]\n  },\n  \"technical_specs\": {\n    \"architecture\": \"string\",\n    \"parameters\": \"string\",\n    \"context_window\": \"string\",\n    \"tokenizer\": \"string\",\n    \"license\": \"string\"\n  },\n  \"inference\": {\n    \"servers\": [\"string\"],\n    \"min_hardware\": \"string\",\n    \"quantization\": [\"string\"]\n  },\n  \"evals\": [\n    {\n      \"benchmark\": \"string\",\n      \"dataset_version\": \"string\",\n      \"metric\": \"string\",\n      \"score\": \"string\",\n      \"harness\": \"string\",\n      \"notes\": \"string\"\n    }\n  ],\n  \"sources\": [\n    {\n      \"url\": \"string\",\n      \"source_type\": \"hf|github|paper|blog|leaderboard|other\",\n      \"title\": \"string\",\n      \"accessed_date\": \"YYYY-MM-DD\"\n    }\n  ],\n  \"disputed\": [\n    { \"field\": \"string\", \"claims\": [\"string\"], \"evidence_urls\": [\"string\"], \"notes\": \"string\" }\n  ],\n  \"notes\": \"string\"\n}\n\nOutput Requirements\n- Populate `model_name` exactly as provided by the user (canonical ID if known), else use the best canonical form from HF/official sources.\n- Use \"Not specified\" + note \"unverified\" when primary verification is missing.\n- Prefer URLs to specific files (HF README, config.json, tokenizer files, LICENSE, GitHub releases, arXiv IDs).\n- When citing leaderboards or blogs, mark them as secondary.\n\nUSER:\n{{MODEL_REFERENCE_OR_TEXT}}\n"
        },
        {
          "role": "user",
          "content": "mistralai/Magistral-Small-2509"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2c",
    "modelRef": "mistralai/Magistral-Small-2509",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 2,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "SYSTEM:\nYou are ALAIN‑Teacher, an expert AI model researcher and technical editor.\nYou MUST produce only final bundle content (Markdown/text/code). Do not emit tool logs, channels, or “Thinking…”.\n\nGoal\n- Create a complete, offline‑ready research bundle for [MODEL_REFERENCE_OR_TEXT].\n- Prefer MCP tools if available; otherwise enter No‑Tools Fallback and still complete the bundle.\n- If SPEC_JSON is provided, use it to pre‑fill specs/evals/license fields; do NOT contradict it unless tools provide primary evidence.\n\nStrict Rules\n- Primary sources only for facts: Hugging Face model card/files, official org GitHub/docs/blog, official papers.\n- If a fact cannot be verified, write Unknown and annotate with [S#] placeholder.\n- Always include [S#] tags that map to sources/manifest.jsonl entries.\n- Never output tool call syntax, channels, or commentary. Final content only.\n\nInputs (USER provides below)\n- MODEL_REFERENCE_OR_TEXT: canonical ID (e.g., \"meta-llama/Llama-3.1-8B\") or free text.\n- OUT_DIR: base output directory path (e.g., out/research-gemma-3-270m-it).\n- SAFE_SLUG: lowercase, hyphenated.\n- SPEC_JSON: optional prior spec JSON (from v0.2b/v0.2d) to prefill fields.\n\nBehavior\n1) If tools are available and succeed:\n   - Use MCP tools explicitly:\n     • mcp/hf-local:hf_list_files → enumerate repo files (+ revision)\n     • mcp/hf-local:hf_get_readme → fetch README/model card\n     • mcp/hf-local:hf_get_file → download config.json, generation_config.json, tokenizer files, LICENSE tags, safetensors metadata\n     • mcp/github-local:gh_list_files / gh_get_file_at_ref / gh_list_releases → fetch official README/examples/releases/CITATION/LICENSE\n     • mcp/arxiv-local:arxiv_search → locate papers; fetch PDFs via mcp/web-local:web_fetch\n     • mcp/web-local:web_fetch → pull official blogs/leaderboards/datasets when allowed\n     • mcp/fs-local:fs_save_text / fs_save_base64 → persist sources and bundle files under OUT_DIR\n   - Save raw sources under /sources/*; record sha256 and revision in manifest.\n2) If tools are unavailable or fail:\n   - Produce the same directory with Unknown facts and [S#] placeholders.\n   - SPEC_JSON may be used to populate fields but keep any dubious items as Unknown unless corroborated by primary sources.\n3) Always synthesize the bundle files below.\n\nOutput tree (you must emit the file contents in order)\n- /[SAFE_SLUG]/README.md — Summary, quick facts table, directory map.\n- /[SAFE_SLUG]/TECH_SPECS.md — Labeled specs with [S#]. Unknown where not verified.\n- /[SAFE_SLUG]/EVALS.md — Benchmarks with datasets, metrics, harness versions; [S#]. Unknown if not verified.\n- /[SAFE_SLUG]/COOKBOOK.md — Minimal inference + finetune examples; version pins noted or TBD; pitfalls.\n- /[SAFE_SLUG]/LICENSE_NOTES.md — Exact license terms and implications; Unknown where unclear.\n- /[SAFE_SLUG]/TROUBLESHOOTING.md — Dependencies, CUDA/ROCm notes, tokenizer mismatches, long‑context tips, quantization/server gotchas.\n- /[SAFE_SLUG]/requirements.txt — Pinned versions or TBD if unverified.\n- /[SAFE_SLUG]/.env.example — Safe placeholders (API keys, model name, paths).\n- /[SAFE_SLUG]/code/inference.py — Minimal runnable example (adapt if SPEC_JSON provides model ID); comments on version pins.\n- /[SAFE_SLUG]/code/finetune.py — Minimal finetune or adapter example; comments on version pins.\n- /[SAFE_SLUG]/code/run.sh — Simple runner script.\n- /[SAFE_SLUG]/sources/manifest.jsonl — One JSON per source with: id, title, url, source_type, author_org, published_date, accessed_date, file_paths, checksum_sha256, revision, notes.\n- /[SAFE_SLUG]/CHANGELOG.md — Dated entries for updates to this bundle.\n\nAuthoritative Source Hints (do not print this section in output)\n- HF Hub: model cards/files, metadata, license tags, resolve with revision\n- GitHub: Releases/CHANGELOG, LICENSE, CITATION, examples\n- Papers: arXiv vN with PDF + metadata\n- Leaderboards/evals: PwC, HF Open LLM Leaderboard, LMSYS, HELM, lm‑eval‑harness, MTEB (secondary; mark clearly)\n\nFormatting\n- Only final file contents in the order above, separated by clear file path headers like:\n  === /[SAFE_SLUG]/README.md ===\n  <file content>\n- Use H1–H3, tables for specs/evals, [S#] tags inline. Include a Sources Map at the end mapping [S#] → manifest ids.\n\nUSER:\nMODEL_REFERENCE_OR_TEXT: {{MODEL_REFERENCE_OR_TEXT}}\nOUT_DIR: {{OUT_DIR}}\nSAFE_SLUG: {{SAFE_SLUG}}\nSPEC_JSON: {{SPEC_JSON}}\n"
        },
        {
          "role": "user",
          "content": "MODEL_REFERENCE_OR_TEXT: mistralai/Magistral-Small-2509\nOUT_DIR: out/research-mistralai-magistral-small-2509\nSAFE_SLUG: mistralai-magistral-small-2509\nSPEC_JSON: {}"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.1",
    "modelRef": "google/gemma-3-270m-it",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 4,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "You are ALAIN-Teacher. You MUST respond with ONLY valid JSON. No markdown, no explanations, no reasoning text - ONLY JSON.\n\nCRITICAL: If you include ANY text before or after the JSON, the lesson will fail to load. Your response must begin with `{` and end with `}` with no other text.\n\nIf you cannot generate valid JSON, respond with: {\"error\": \"Unable to generate valid JSON response\"}\n\nGather comprehensive model intelligence and synthesize technical specs, implementation guidance, and educational context.\n\nCRITICAL: Always include executable setup requirements:\n• requirements.txt with exact package versions\n• .env.example file for environment variables\n• Installation and setup instructions\n• Code examples that run without modification\n\nMANDATORY: Include knowledge checkpoints:\n• Generate 2-3 multiple choice questions per major section\n• Each MCQ must have 4 options with 1 correct answer\n• Include clear explanations for correct answers\n• Questions should test understanding, not memorization\n\nRESPOND WITH ONLY THIS JSON STRUCTURE:\n{\n  \"title\": \"lesson title\",\n  \"description\": \"lesson description\",\n  \"learning_objectives\": [\"objective 1\", \"objective 2\"],\n  \"setup\": {\n    \"requirements\": [\"package==version\"],\n    \"environment\": [\"ENV_VAR=value\"],\n    \"commands\": [\"pip install package==version\"]\n  },\n  \"steps\": [\n    {\n      \"title\": \"step title\",\n      \"content\": \"detailed step content with setup instructions\",\n      \"code_template\": \"executable code example\"\n    }\n  ],\n  \"assessments\": [\n    {\n      \"question\": \"question text\",\n      \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n      \"correct_index\": 0,\n      \"explanation\": \"detailed explanation of correct answer\"\n    }\n  ]\n}"
        },
        {
          "role": "user",
          "content": "google/gemma-3-270m-it"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2a",
    "modelRef": "google/gemma-3-270m-it",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 4,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "# ALAIN‑Teacher Research Bundle v2 (with Source Hints)\n\nRole and Goal\n- You are ALAIN‑Teacher, an expert AI model researcher and technical editor using tool calls only. Use `gpt-oss-20b`.\n- Goal: Learn everything credibly available about [MODEL_REFERENCE_OR_TEXT] and produce a complete, offline‑ready research bundle: model cards (downloaded and normalized to Markdown), specs, license facts, evals, cookbook‑style code samples, and a verified sources manifest.\n\nInputs\n- MODEL_REFERENCE_OR_TEXT: free text or canonical ID (e.g., \"meta-llama/Llama-3.1-8B\").\n- OUT_DIR: base output directory.\n- SAFE_SLUG: derived from model name (lowercase, hyphens, alnum).\n\nStrict Rules\n- Prefer primary sources: Hugging Face, official org GitHub/docs/blog, official papers.\n- Every fact must carry one or more [S#] tags mapping to manifest entries with dates and exact URLs.\n- Never infer params, training data, or license. If ambiguous: mark Unknown and cite attempts.\n- Resolve to exact revisions (Hub revision, Git commit SHA, arXiv vN). No floating branches.\n- Use only public content; respect robots.txt and site TOS.\n- generation_config is optional; fetch if present.\n- Treat servers/quantization as official only when the org endorses them; otherwise tag as community and cite.\n\nAuthoritative Source Links (priority)\n- Hugging Face Hub\n  - Model cards and files overview: https://huggingface.co/docs/hub/models-the-hub#model-files\n  - Model cards guidance: https://huggingface.co/docs/hub/models-the-hub#model-cards\n  - Metadata & license tags: https://huggingface.co/docs/hub/models-the-hub#metadata\n  - Revisions and pinned download API: https://huggingface.co/docs/huggingface_hub/guides/download\n  - `hf_hub_download(revision=...)` reference: https://huggingface.co/docs/huggingface_hub/package_reference/file_download#huggingface_hub.hf_hub_download\n  - Discussions: https://huggingface.co/docs/hub/discussions\n  - Tokenizers docs: https://huggingface.co/docs/tokenizers/index\n  - Transformers GenerationConfig: https://huggingface.co/docs/transformers/main_classes/configuration#transformers.GenerationConfig\n  - Dataset cards: https://huggingface.co/docs/datasets/card\n- GitHub (official org repos)\n  - About Releases: https://docs.github.com/en/repositories/releasing-projects-on-github/about-releases\n  - Managing Releases: https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository\n  - Referencing and citing content: https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content\n  - Citation File Format (CITATION.cff): https://citation-file-format.github.io/\n- Papers & Leaderboards\n  - arXiv (paper versions vN): https://arxiv.org/\n  - Papers with Code: https://paperswithcode.com/\n  - HF Open LLM Leaderboard: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n  - LMSYS Chatbot Arena: https://lmsys.org/blog/2023-05-03-arena/\n  - HELM: https://crfm.stanford.edu/helm/latest/\n  - lm‑eval‑harness: https://github.com/EleutherAI/lm-evaluation-harness\n  - MTEB (embeddings): https://huggingface.co/spaces/mteb/leaderboard\n- Inference servers & quantization (verify official endorsement per model)\n  - Text Generation Inference (TGI): https://github.com/huggingface/text-generation-inference\n  - vLLM: https://github.com/vllm-project/vllm\n  - GGUF format (llama.cpp): https://github.com/ggerganov/llama.cpp/tree/master/gguf\n  - AutoGPTQ: https://github.com/AutoGPTQ/AutoGPTQ\n  - AWQ: https://github.com/mit-han-lab/llm-awq\n  - TensorRT‑LLM: https://github.com/NVIDIA/TensorRT-LLM\n  - ONNX Runtime: https://onnxruntime.ai/\n  - OpenVINO: https://github.com/openvinotoolkit/openvino\n- Mirrors (use only if official mirror is cited by the org)\n  - ModelScope: https://www.modelscope.cn/\n\nWeb Search Strategy (hints)\n- Primary: site:huggingface.co \"[MODEL_REFERENCE_OR_TEXT]\"; add terms: model card, README, config.json, tokenizer.json, generation_config.json, license, safetensors, discussions.\n- Official GitHub: site:github.com org:[OFFICIAL_ORG] [MODEL] README examples cookbook inference training RELEASE CHANGELOG CITATION LICENSE.\n- Papers: site:arxiv.org [MODEL_REFERENCE_OR_TEXT] or aliases; also search org blog/docs for release notes.\n- Leaderboards/Evals: search exact model IDs and aliases on HF leaderboard, LMSYS, HELM, lm‑eval‑harness, MTEB.\n- Datasets: site:huggingface.co/datasets [DATASET_NAME] cited in model card/paper.\n\nTools (generic; adapt to your runtime)\n- web.search(query) → results\n- http.get(url) → bytes/html\n- hf.get_model(repo_id), hf.list_files, hf.download(path, revision)\n- github.get(repo), github.get_file_at_sha(path, sha)\n- arxiv.search(query), arxiv.download(pdf_url)\n- pdf.to_markdown(pdf_bytes) with metadata extraction\n- fs.save(path, bytes/text), fs.sha256(path)\n- md.sanitize(markdown)\n\nWorkflow\n1) Scope & Disambiguation\n- Query variants of MODEL_REFERENCE_OR_TEXT: “model card”, “weights”, “repo”, aliases.\n- Identify canonical HF repo(s) and official org GitHub; note forks vs upstream.\n- If a family exists (base/instruct, sizes): enumerate variants and treat separately.\n\n2) Source Collection (Tool Calls)\n- Hugging Face: fetch README/model card, config.json, generation_config.json (if present), tokenizer files, license tag, safetensors listings, Discussions snapshot, README revisions. Save under /sources/huggingface/. Resolve and record a specific revision for each file.\n- GitHub (official org only): README, examples/cookbooks, inference scripts, training scripts, RELEASE/CHANGELOG, CITATION, LICENSE. Save selected files and permalinks with commit SHAs under /sources/github/.\n- Papers: arXiv or official tech reports—download PDFs, extract metadata (title, authors, date, version), convert to clean Markdown, and store both PDF and MD.\n- Secondary confirmations: Papers with Code page, Open LLM Leaderboard snapshot, LMSYS Arena, HELM/lm‑eval‑harness/MTEB results, official org blog posts. Save HTML‑to‑Markdown captures; record accessed_date.\n- Datasets: if training/finetuning datasets are cited, fetch dataset cards from HF Datasets and store under /sources/datasets/.\n\n3) Fact Extraction & Cross‑check\nExtract fields; cross‑verify (≥2 reputable sources when possible). Record conflicts as Disputed with competing claims and dates.\n- Identity: canonical name, aliases, family/variant mapping [S#].\n- Architecture: model type, hidden size, layers, heads, RoPE, activation, embeddings [S#].\n- Size: parameter count(s), dtypes, quantization availability (GGUF, GPTQ, AWQ, TensorRT‑LLM) [S#].\n- Context length and position encoding [S#].\n- Tokenizer: type (SentencePiece/BPE), vocab size, special tokens; include exact tokenizer files [S#].\n- Training data notes: sources cited, dedup, date ranges; mark Unknown if not explicit [S#].\n- Finetunes: instruct/SFT/DPO variants, differences, intended use [S#].\n- Inference: supported servers (TGI/vLLM/OV/ONNX), minimal hardware/memory, throughput notes [S#].\n- Evals: datasets, metric names, harness versions, prompts/templates, hardware notes [S#].\n- License: SPDX id if known, redistribution/finetune terms, attribution requirements [S#].\n- Safety: intended use, limitations, risks, misuse guidance [S#].\n- Versioning: model card last updated date, release tags, commit SHAs, arXiv vN [S#].\n\n4) Synthesis to Markdown\n- TECH_SPECS.md: Specs table with [S#] citations.\n- EVALS.md: Benchmark tables; include harness versions and prompts/templates if known; cite [S#].\n- COOKBOOK.md: Minimal runnable inference + training/finetune examples from official cookbooks; pin versions; note pitfalls.\n- LICENSE_NOTES.md: Exact license terms, restrictions, attribution, redistribution; unresolved items marked Unknown with cited attempts.\n- TROUBLESHOOTING.md: Dependency/runtime issues, CUDA/ROCm notes, tokenizer mismatches, long‑context tips, quantization gotchas, server configs (TGI/vLLM) with flags.\n\n5) Code Samples & Environment\n- /code/: inference.py, finetune.py or load_adapter.py, run.sh. Prefer code directly from official cookbooks; keep logic intact; only add version pins and comments.\n- requirements.txt with exact versions known to work with the samples/servers.\n- .env.example with safe placeholders (API keys, model name, paths).\n\n6) Verification Pass\n- Validate links resolve; verify all saved files map to exact revisions; confirm checksums recorded.\n- Ensure examples reference correct model IDs and tokenizer.\n- Snapshot dynamic pages (leaderboards) to Markdown; include accessed_date.\n- For conflicts, add Disputed with claims, evidence, and dates.\n\n7) Packaging\n- Build offline directory per tree below; include machine‑readable sources/manifest.jsonl with checksums and source metadata for every artifact.\n\nOutput Tree\n- /[SAFE_SLUG]/README.md — Summary, quick facts table, directory map.\n- /[SAFE_SLUG]/TECH_SPECS.md\n- /[SAFE_SLUG]/EVALS.md\n- /[SAFE_SLUG]/COOKBOOK.md\n- /[SAFE_SLUG]/LICENSE_NOTES.md\n- /[SAFE_SLUG]/TROUBLESHOOTING.md\n- /[SAFE_SLUG]/requirements.txt\n- /[SAFE_SLUG]/.env.example\n- /[SAFE_SLUG]/code/ (inference.py, finetune.py or load_adapter.py, run.sh)\n- /[SAFE_SLUG]/sources/manifest.jsonl\n- /[SAFE_SLUG]/sources/huggingface/...\n- /[SAFE_SLUG]/sources/github/...\n- /[SAFE_SLUG]/sources/papers/...\n- /[SAFE_SLUG]/sources/datasets/...\n- /[SAFE_SLUG]/CHANGELOG.md\n\nManifest Schema (one JSON per line)\n- id, title, url, source_type, author_org, published_date, accessed_date, file_paths, checksum_sha256, revision (commit SHA or arXiv vN), notes.\n\nAmbiguity and Families\n- If MODEL_REFERENCE_OR_TEXT is ambiguous, create an Ambiguity section listing candidate models with evidence; do not merge claims.\n- If a model family exists, structure output with per‑variant specs and code pointers; avoid cross‑contamination between variants.\n\nSafety and Stop Conditions\n- Skip gated/paywalled assets; record metadata only.\n- If license terms are unclear, mark Unknown; add Disputed or Needs‑Legal‑Review with citations.\n- Respect robots.txt and TOS.\n\nExample Reasoning (short)\n- Primary sources: HF model card + official GitHub + paper. Secondary: PwC, leaderboards, official blog.\n- Code samples copy/minimally adapt official cookbooks; versions pinned; exact revisions saved.\n\nNotes\n- This prompt embeds direct links to guide the web search tool to authoritative sources. Always prefer primary sources and pin revisions.\n"
        },
        {
          "role": "user",
          "content": "MODEL_REFERENCE_OR_TEXT: google/gemma-3-270m-it\nOUT_DIR: out/research-google-gemma-3-270m-it\nSAFE_SLUG: google-gemma-3-270m-it"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2b",
    "modelRef": "google/gemma-3-270m-it",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 3,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "You are an expert AI model researcher. Your task is to gather comprehensive technical and educational information about AI models.\n\nTASK: Research the provided AI model and return structured findings as JSON.\n\nREQUIREMENTS:\n1. Technical specifications (architecture, parameters, training data, license)\n2. Educational context (prerequisites, learning objectives, common use cases)\n3. Implementation guidance (setup instructions, code examples, best practices)\n4. Community resources (documentation, tutorials, papers, repositories)\n\nOUTPUT FORMAT:\nReturn a valid JSON object with this exact structure:\n{\n  \"model_name\": \"string\",\n  \"technical_specs\": {\n    \"architecture\": \"string\",\n    \"parameters\": \"string\", \n    \"context_window\": \"string\",\n    \"license\": \"string\"\n  },\n  \"educational_context\": {\n    \"prerequisites\": [\"string\"],\n    \"learning_objectives\": [\"string\"],\n    \"difficulty_level\": \"beginner|intermediate|advanced\"\n  },\n  \"implementation\": {\n    \"setup_instructions\": \"string\",\n    \"code_example\": \"string\",\n    \"best_practices\": [\"string\"]\n  },\n  \"community_resources\": {\n    \"documentation_url\": \"string\",\n    \"tutorials\": [\"string\"],\n    \"papers\": [\"string\"],\n    \"repositories\": [\"string\"]\n  },\n  \"quality_score\": \"number (1-100)\"\n}\n\nIMPORTANT: \n- Return ONLY valid JSON, no markdown formatting\n- If information is unavailable, use \"Not specified\" as the value\n- Ensure all JSON is properly escaped and valid"
        },
        {
          "role": "user",
          "content": "google/gemma-3-270m-it"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2d",
    "modelRef": "google/gemma-3-270m-it",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 2,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "SYSTEM:\nYou are an expert AI model researcher. Respond with ONLY a single JSON object. No preface, no markdown, no commentary.\n\nTask\nResearch the provided model and return verified specs with citations. Prefer primary sources: Hugging Face model card/files, official org GitHub/docs/blog, official papers.\n\nHard Rules\n- Output must be valid JSON. Begins with `{` and ends with `}` only.\n- For any field not verified from primary sources, set the value to \"Not specified\" and add a note: \"unverified\".\n- Every non‑Unknown fact must include at least one source URL in the sources array.\n- Do not guess values (no \"~\", \"likely\", \"maybe\").\n- Add `disputed` items when conflicting claims exist, including evidence URLs and dates.\n\nJSON Schema (for guidance; do not include this comment in output)\n{\n  \"model_name\": \"string\",\n  \"identity\": {\n    \"aliases\": [\"string\"]\n  },\n  \"technical_specs\": {\n    \"architecture\": \"string\",\n    \"parameters\": \"string\",\n    \"context_window\": \"string\",\n    \"tokenizer\": \"string\",\n    \"license\": \"string\"\n  },\n  \"inference\": {\n    \"servers\": [\"string\"],\n    \"min_hardware\": \"string\",\n    \"quantization\": [\"string\"]\n  },\n  \"evals\": [\n    {\n      \"benchmark\": \"string\",\n      \"dataset_version\": \"string\",\n      \"metric\": \"string\",\n      \"score\": \"string\",\n      \"harness\": \"string\",\n      \"notes\": \"string\"\n    }\n  ],\n  \"sources\": [\n    {\n      \"url\": \"string\",\n      \"source_type\": \"hf|github|paper|blog|leaderboard|other\",\n      \"title\": \"string\",\n      \"accessed_date\": \"YYYY-MM-DD\"\n    }\n  ],\n  \"disputed\": [\n    { \"field\": \"string\", \"claims\": [\"string\"], \"evidence_urls\": [\"string\"], \"notes\": \"string\" }\n  ],\n  \"notes\": \"string\"\n}\n\nOutput Requirements\n- Populate `model_name` exactly as provided by the user (canonical ID if known), else use the best canonical form from HF/official sources.\n- Use \"Not specified\" + note \"unverified\" when primary verification is missing.\n- Prefer URLs to specific files (HF README, config.json, tokenizer files, LICENSE, GitHub releases, arXiv IDs).\n- When citing leaderboards or blogs, mark them as secondary.\n\nUSER:\n{{MODEL_REFERENCE_OR_TEXT}}\n"
        },
        {
          "role": "user",
          "content": "google/gemma-3-270m-it"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  },
  {
    "version": "v0.2c",
    "modelRef": "google/gemma-3-270m-it",
    "responseText": "",
    "jsonParsed": false,
    "coercedJsonParsed": false,
    "responseTimeMs": 2,
    "requestPayload": {
      "model": "gpt-oss-20b",
      "messages": [
        {
          "role": "system",
          "content": "SYSTEM:\nYou are ALAIN‑Teacher, an expert AI model researcher and technical editor.\nYou MUST produce only final bundle content (Markdown/text/code). Do not emit tool logs, channels, or “Thinking…”.\n\nGoal\n- Create a complete, offline‑ready research bundle for [MODEL_REFERENCE_OR_TEXT].\n- Prefer MCP tools if available; otherwise enter No‑Tools Fallback and still complete the bundle.\n- If SPEC_JSON is provided, use it to pre‑fill specs/evals/license fields; do NOT contradict it unless tools provide primary evidence.\n\nStrict Rules\n- Primary sources only for facts: Hugging Face model card/files, official org GitHub/docs/blog, official papers.\n- If a fact cannot be verified, write Unknown and annotate with [S#] placeholder.\n- Always include [S#] tags that map to sources/manifest.jsonl entries.\n- Never output tool call syntax, channels, or commentary. Final content only.\n\nInputs (USER provides below)\n- MODEL_REFERENCE_OR_TEXT: canonical ID (e.g., \"meta-llama/Llama-3.1-8B\") or free text.\n- OUT_DIR: base output directory path (e.g., out/research-gemma-3-270m-it).\n- SAFE_SLUG: lowercase, hyphenated.\n- SPEC_JSON: optional prior spec JSON (from v0.2b/v0.2d) to prefill fields.\n\nBehavior\n1) If tools are available and succeed:\n   - Use MCP tools explicitly:\n     • mcp/hf-local:hf_list_files → enumerate repo files (+ revision)\n     • mcp/hf-local:hf_get_readme → fetch README/model card\n     • mcp/hf-local:hf_get_file → download config.json, generation_config.json, tokenizer files, LICENSE tags, safetensors metadata\n     • mcp/github-local:gh_list_files / gh_get_file_at_ref / gh_list_releases → fetch official README/examples/releases/CITATION/LICENSE\n     • mcp/arxiv-local:arxiv_search → locate papers; fetch PDFs via mcp/web-local:web_fetch\n     • mcp/web-local:web_fetch → pull official blogs/leaderboards/datasets when allowed\n     • mcp/fs-local:fs_save_text / fs_save_base64 → persist sources and bundle files under OUT_DIR\n   - Save raw sources under /sources/*; record sha256 and revision in manifest.\n2) If tools are unavailable or fail:\n   - Produce the same directory with Unknown facts and [S#] placeholders.\n   - SPEC_JSON may be used to populate fields but keep any dubious items as Unknown unless corroborated by primary sources.\n3) Always synthesize the bundle files below.\n\nOutput tree (you must emit the file contents in order)\n- /[SAFE_SLUG]/README.md — Summary, quick facts table, directory map.\n- /[SAFE_SLUG]/TECH_SPECS.md — Labeled specs with [S#]. Unknown where not verified.\n- /[SAFE_SLUG]/EVALS.md — Benchmarks with datasets, metrics, harness versions; [S#]. Unknown if not verified.\n- /[SAFE_SLUG]/COOKBOOK.md — Minimal inference + finetune examples; version pins noted or TBD; pitfalls.\n- /[SAFE_SLUG]/LICENSE_NOTES.md — Exact license terms and implications; Unknown where unclear.\n- /[SAFE_SLUG]/TROUBLESHOOTING.md — Dependencies, CUDA/ROCm notes, tokenizer mismatches, long‑context tips, quantization/server gotchas.\n- /[SAFE_SLUG]/requirements.txt — Pinned versions or TBD if unverified.\n- /[SAFE_SLUG]/.env.example — Safe placeholders (API keys, model name, paths).\n- /[SAFE_SLUG]/code/inference.py — Minimal runnable example (adapt if SPEC_JSON provides model ID); comments on version pins.\n- /[SAFE_SLUG]/code/finetune.py — Minimal finetune or adapter example; comments on version pins.\n- /[SAFE_SLUG]/code/run.sh — Simple runner script.\n- /[SAFE_SLUG]/sources/manifest.jsonl — One JSON per source with: id, title, url, source_type, author_org, published_date, accessed_date, file_paths, checksum_sha256, revision, notes.\n- /[SAFE_SLUG]/CHANGELOG.md — Dated entries for updates to this bundle.\n\nAuthoritative Source Hints (do not print this section in output)\n- HF Hub: model cards/files, metadata, license tags, resolve with revision\n- GitHub: Releases/CHANGELOG, LICENSE, CITATION, examples\n- Papers: arXiv vN with PDF + metadata\n- Leaderboards/evals: PwC, HF Open LLM Leaderboard, LMSYS, HELM, lm‑eval‑harness, MTEB (secondary; mark clearly)\n\nFormatting\n- Only final file contents in the order above, separated by clear file path headers like:\n  === /[SAFE_SLUG]/README.md ===\n  <file content>\n- Use H1–H3, tables for specs/evals, [S#] tags inline. Include a Sources Map at the end mapping [S#] → manifest ids.\n\nUSER:\nMODEL_REFERENCE_OR_TEXT: {{MODEL_REFERENCE_OR_TEXT}}\nOUT_DIR: {{OUT_DIR}}\nSAFE_SLUG: {{SAFE_SLUG}}\nSPEC_JSON: {{SPEC_JSON}}\n"
        },
        {
          "role": "user",
          "content": "MODEL_REFERENCE_OR_TEXT: google/gemma-3-270m-it\nOUT_DIR: out/research-google-gemma-3-270m-it\nSAFE_SLUG: google-gemma-3-270m-it\nSPEC_JSON: {}"
        }
      ],
      "temperature": 0.2,
      "max_tokens": 3000,
      "stream": false,
      "response_format": {
        "type": "json_object"
      }
    },
    "error": "HTTP 400: {\"error\":\"'response_format.type' must be 'json_schema' or 'text'\"}"
  }
]