{
  "version": "v0.1",
  "model_reference": "openai/gpt-oss-20b",
  "response_time_ms": 25036,
  "json_parsed": true,
  "coerced_json_parsed": false,
  "error": null,
  "response_text": "{\"title\":\"Building and Running GPT‑OSS‑20B with OpenAI’s Repository\",\"description\":\"Learn how to set up the 20 B parameter GPT model from OpenAI’s GPT‑OSS repository, fine‑tune it on a custom dataset, and deploy it locally for inference. This lesson covers environment configuration, dependency installation, data preprocessing, training loop basics, and serving the model via a simple API.\",\"learning_objectives\":[\"Understand the prerequisites for running a 20 B transformer model\",\"Configure a GPU‑enabled environment with the correct CUDA version\",\"Prepare data for fine‑tuning GPT‑OSS\",\"Execute a basic training script and monitor loss\",\"Deploy the trained model using FastAPI\"],\"setup\":{\"requirements\":[\"torch==2.1.0+cu118\",\"transformers==4.41.2\",\"datasets==2.20.0\",\"accelerate==0.30.0\",\"bitsandbytes==0.43.1\",\"fastapi==0.112.0\",\"uvicorn==0.30.6\"],\"environment\":[\"CUDA_VISIBLE_DEVICES=0\",\"HF_TOKEN=your_huggingface_token\",\"MODEL_NAME=openai/gpt-oss-20b\"],\"commands\":[\"pip install torch==2.1.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\",\"pip install transformers==4.41.2 datasets==2.20.0 accelerate==0.30.0 bitsandbytes==0.43.1 fastapi==0.112.0 uvicorn==0.30.6\"]},\"steps\":[{\"title\":\"Clone the GPT‑OSS Repository\",\"content\":\"Download the source code and navigate into the project directory.\\n```bash\\ngit clone https://github.com/openai/gpt-oss.git\\ncd gpt-oss\\n```\",\"code_template\":\"# No executable code needed for this step\"},{\"title\":\"Set Up a Conda Environment (Optional)\",\"content\":\"Create and activate a new conda environment to isolate dependencies.\\n```bash\\nconda create -n gpt20b python=3.10 -y\\nconda activate gpt20b\\n```\",\"code_template\":\"# No executable code needed for this step\"},{\"title\":\"Install Dependencies\",\"content\":\"Run the pip commands listed in the setup section to install all required packages.\\n```bash\\npip install torch==2.1.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\npip install transformers==4.41.2 datasets==2.20.0 accelerate==0.30.0 bitsandbytes==0.43.1 fastapi==0.112.0 uvicorn==0.30.6\\n```\",\"code_template\":\"# No executable code needed for this step\"},{\"title\":\"Prepare Your Dataset\",\"content\":\"Create a text file where each line is a training example. Use the `datasets` library to load it.\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset('text', data_files={'train': 'my_data.txt'}, split='train')\\n```\\nTokenize with the GPT‑OSS tokenizer:\\n```python\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"openai/gpt-oss-20b\\\")\\n\\ndef tokenize_function(examples):\\n    return tokenizer(examples['text'], truncation=True, max_length=1024)\\n\\ndataset = dataset.map(tokenize_function, batched=True)\\n```\",\"code_template\":\"from datasets import load_dataset\\nfrom transformers import AutoTokenizer\\n\\ndataset = load_dataset('text', data_files={'train': 'my_data.txt'}, split='train')\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"openai/gpt-oss-20b\\\")\\n\\ndef tokenize_function(examples):\\n    return tokenizer(examples['text'], truncation=True, max_length=1024)\\n\\ndataset = dataset.map(tokenize_function, batched=True)\"},{\"title\":\"Fine‑Tune the Model\",\"content\":\"Use `accelerate` to launch distributed training. Create a config file `accelerate_config.yaml` with your GPU settings.\\n```yaml\\ncompute_environment: LOCAL_MACHINE\\ndeepspeed_config: {}\\nhardware_type: NVML\\nmixed_precision: fp16\\n```\\nRun the training script:\\n```bash\\naccelerate launch train.py \\\\\\n  --model_name_or_path openai/gpt-oss-20b \\\\\\n  --dataset_name my_data.txt \\\\\\n  --output_dir ./gpt20b_finetuned \\\\\\n  --per_device_train_batch_size 1 \\\\\\n  --gradient_accumulation_steps 8 \\\\\\n  --learning_rate 5e-6 \\\\\\n  --num_train_epochs 3\\n```\",\"code_template\":\"# train.py (simplified)\\nfrom transformers import AutoModelForCausalLM, Trainer, TrainingArguments\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset('text', data_files={'train': 'my_data.txt'}, split='train')\\nmodel = AutoModelForCausalLM.from_pretrained(\\\"openai/gpt-oss-20b\\\")\\ntraining_args = TrainingArguments(\\n    output_dir='./gpt20b_finetuned',\\n    per_device_train_batch_size=1,\\n    gradient_accumulation_steps=8,\\n    learning_rate=5e-6,\\n    num_train_epochs=3,\\n    fp16=True,\\n)\\ntrainer = Trainer(model=model, args=training_args, train_dataset=dataset)\\ntrainer.train()\\ntrainer.save_model('./gpt20b_finetuned')\"},{\"title\":\"Serve the Model with FastAPI\",\"content\":\"Create a simple API that loads the fine‑tuned model and returns completions.\\n```python\\nfrom fastapi import FastAPI\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\n\\napp = FastAPI()\\nmodel_name = \\\"./gpt20b_finetuned\\\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')\\n\\n@app.post(\\\"/generate\\\")\\ndef generate(prompt: str):\\n    inputs = tokenizer(prompt, return_tensors=\\\"pt\\\").to('cuda')\\n    outputs = model.generate(**inputs, max_new_tokens=50)\\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n    return {\\\"completion\\\": text}\\n```\\nRun the server:\\n```bash\\nuvicorn main:app --host 0.0.0.0 --port 8000\\n```\",\"code_template\":\"from fastapi import FastAPI\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\n\\napp = FastAPI()\\nmodel_name = \\\"./gpt20b_finetuned\\\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')\\n\\n@app.post(\\\"/generate\\\")\\ndef generate(prompt: str):\\n    inputs = tokenizer(prompt, return_tensors=\\\"pt\\\").to('cuda')\\n    outputs = model.generate(**inputs, max_new_tokens=50)\\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n    return {\\\"completion\\\": text}\\n\"}],\"assessments\":[{\"question\":\"Which command installs the correct CUDA-enabled PyTorch wheel for cu118?\",\"options\":[\"pip install torch==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118\",\"pip install torch==2.1.0+cu117 --index-url https://download.pytorch.org/whl/cu118\",\"pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cu118\",\"pip install torch==2.1.0+cu118\"],\"correct_index\":0,\"explanation\":\"The wheel name must match the CUDA version; cu118 indicates CUDA 11.8. The index URL points to PyTorch’s CUDA‑specific wheels.\"},{\"question\":\"What is the purpose of gradient_accumulation_steps in training?\",\"options\":[\"To increase batch size without using more GPU memory\",\"To reduce learning rate automatically\",\"To split the dataset into smaller chunks\",\"To enable mixed precision training\"],\"correct_index\":0,\"explanation\":\"Gradient accumulation sums gradients over multiple forward passes, effectively simulating a larger batch size while keeping per‑GPU memory usage low.\"},{\"question\":\"Which FastAPI endpoint returns generated text?\",\"options\":[\"GET /generate\",\"POST /predict\",\"POST /generate\",\"GET /completion\"],\"correct_index\":2,\"explanation\":\"The example defines a POST route at /generate that accepts a prompt and returns the model’s completion.\"}]}"
}