{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment Detection\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Generated by ALAIN (Applied Learning AI Notebooks) — 2025-09-19.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔧 Environment Detection and Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "env_label = 'Google Colab' if IN_COLAB else 'Local'\n",
        "print(f'Environment: {env_label}')\n",
        "\n",
        "# Setup environment-specific configurations\n",
        "if IN_COLAB:\n",
        "    print('📝 Colab-specific optimizations enabled')\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys and .env Files\n",
        "\n",
        "Many providers require API keys. Do not hardcode secrets in notebooks. Use a local .env file that the notebook loads at runtime.\n",
        "\n",
        "- Why .env? Keeps secrets out of source control and tutorials.\n",
        "- Where? Place `.env.local` (preferred) or `.env` in the same folder as this notebook. `.env.local` overrides `.env`.\n",
        "- What keys? Common: `POE_API_KEY` (Poe-compatible servers), `OPENAI_API_KEY` (OpenAI-compatible), `HF_TOKEN` (Hugging Face).\n",
        "- Find your keys:\n",
        "  - Poe-compatible providers: see your provider's dashboard for an API key.\n",
        "  - Hugging Face: create a token at https://huggingface.co/settings/tokens (read scope is usually enough).\n",
        "  - Local servers: you may not need a key; set `OPENAI_BASE_URL` instead (e.g., http://localhost:1234/v1).\n",
        "\n",
        "The next cell will: load `.env.local`/`.env`, prompt for missing keys, and optionally write `.env.local` with secure permissions so future runs just work.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔐 Load and manage secrets from .env\\n# This cell will: (1) load .env.local/.env, (2) prompt for missing keys, (3) optionally write .env.local (0600).\\n# Location: place your .env files next to this notebook (recommended) or at project root.\\n# Disable writing: set SAVE_TO_ENV = False below.\\nimport os, pathlib\\nfrom getpass import getpass\\n\\n# Install python-dotenv if missing\\ntry:\\n    import dotenv  # type: ignore\\nexcept Exception:\\n    import sys, subprocess\\n    if 'IN_COLAB' in globals() and IN_COLAB:\\n        try:\\n            import IPython\\n            ip = IPython.get_ipython()\\n            if ip is not None:\\n                ip.run_line_magic('pip', 'install -q python-dotenv>=1.0.0')\\n            else:\\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n        except Exception as colab_exc:\\n            print('⚠️ Colab pip fallback failed:', colab_exc)\\n            raise\\n    else:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'python-dotenv>=1.0.0'])\\n    import dotenv  # type: ignore\\n\\n# Prefer .env.local over .env\\ncwd = pathlib.Path.cwd()\\nenv_local = cwd / '.env.local'\\nenv_file = cwd / '.env'\\nchosen = env_local if env_local.exists() else (env_file if env_file.exists() else None)\\nif chosen:\\n    dotenv.load_dotenv(dotenv_path=str(chosen))\\n    print(f'Loaded env from {chosen.name}')\\nelse:\\n    print('No .env.local or .env found; will prompt for keys.')\\n\\n# Keys we might use in this notebook\\nkeys = ['POE_API_KEY', 'OPENAI_API_KEY', 'HF_TOKEN']\\nmissing = [k for k in keys if not os.environ.get(k)]\\nfor k in missing:\\n    val = getpass(f'Enter {k} (hidden, press Enter to skip): ')\\n    if val:\\n        os.environ[k] = val\\n\\n# Decide whether to persist to .env.local for convenience\\nSAVE_TO_ENV = True  # set False to disable writing\\nif SAVE_TO_ENV:\\n    target = env_local\\n    existing = {}\\n    if target.exists():\\n        try:\\n            for line in target.read_text().splitlines():\\n                if not line.strip() or line.strip().startswith('#') or '=' not in line:\\n                    continue\\n                k,v = line.split('=',1)\\n                existing[k.strip()] = v.strip()\\n        except Exception:\\n            pass\\n    for k in keys:\\n        v = os.environ.get(k)\\n        if v:\\n            existing[k] = v\\n    lines = []\\n    for k,v in existing.items():\\n        # Always quote; escape backslashes and double quotes for safety\\n        escaped = v.replace(\"\\\\\", \"\\\\\\\\\")\\n        escaped = escaped.replace(\"\\\"\", \"\\\\\"\")\\n        vv = f'\"{escaped}\"'\\n        lines.append(f\"{k}={vv}\")\\n    target.write_text('\\\\n'.join(lines) + '\\\\n')\\n    try:\\n        target.chmod(0o600)  # 600\\n    except Exception:\\n        pass\\n    print(f'🔏 Wrote secrets to {target.name} (permissions 600)')\\n\\n# Simple recap (masked)\\ndef mask(v):\\n    if not v: return '∅'\\n    return v[:3] + '…' + v[-2:] if len(v) > 6 else '•••'\\nfor k in keys:\\n    print(f'{k}:', mask(os.environ.get(k)))\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🌐 ALAIN Provider Setup (Poe/OpenAI-compatible)\n",
        "# About keys: If you have POE_API_KEY, this cell maps it to OPENAI_API_KEY and sets OPENAI_BASE_URL to Poe.\n",
        "# Otherwise, set OPENAI_API_KEY (and optionally OPENAI_BASE_URL for local/self-hosted servers).\n",
        "import os\n",
        "try:\n",
        "    # Prefer Poe; fall back to OPENAI_API_KEY if set\n",
        "    poe = os.environ.get('POE_API_KEY')\n",
        "    if poe:\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "        os.environ.setdefault('OPENAI_API_KEY', poe)\n",
        "    # Prompt if no key present\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        from getpass import getpass\n",
        "        os.environ['OPENAI_API_KEY'] = getpass('Enter POE_API_KEY (input hidden): ')\n",
        "        os.environ.setdefault('OPENAI_BASE_URL', 'https://api.poe.com/v1')\n",
        "    # Ensure openai client is installed\n",
        "    try:\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB:\n",
        "            try:\n",
        "                import IPython\n",
        "                ip = IPython.get_ipython()\n",
        "                if ip is not None:\n",
        "                    ip.run_line_magic('pip', 'install -q openai>=1.34.0')\n",
        "                else:\n",
        "                    subprocess.check_call([sys.executable, '-m', ''pip'', ''install'', ''-q'', ''openai>=1.34.0'])\n",
        "            except Exception as colab_exc:\n",
        "                print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                raise\n",
        "        else:\n",
        "            subprocess.check_call([sys.executable, '-m', ''pip'', ''install'', ''-q'', ''openai>=1.34.0'])\n",
        "        from openai import OpenAI  # type: ignore\n",
        "    # Create client\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n",
        "    print('✅ Provider ready:', os.environ.get('OPENAI_BASE_URL'))\n",
        "except Exception as e:\n",
        "    print('⚠️ Provider setup failed:', e)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🔎 Provider Smoke Test (1-token)\n",
        "import os\n",
        "model = os.environ.get('ALAIN_MODEL') or 'gpt-4o-mini'\n",
        "if 'client' not in globals():\n",
        "    print('⚠️ Provider client not available; skipping smoke test')\n",
        "else:\n",
        "    try:\n",
        "        resp = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1)\n",
        "        print('✅ Smoke OK:', resp.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print('⚠️ Smoke test failed:', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Local Transformer Runtime Tips\n\n",
        "- Install the optimized stack with `pip install -U transformers kernels accelerate triton` (PyTorch >= 2.8 already bundles Triton 3.4).\n",
        "- Load GPT-OSS with downloadable kernels to compare bf16 vs MXFP4 memory usage:\n",
        "```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"openai/gpt-oss-20b\",\n    dtype=\"auto\",\n    device_map=\"auto\",\n    use_kernels=True,\n)\n```\n",
        "- Hopper GPUs can enable Flash Attention 3 sinks via `attn_implementation=\"kernels-community/vllm-flash-attn3\"`.\n",
        "- If MXFP4 kernels are unavailable, Transformers automatically falls back to bf16; monitor VRAM and throughput to pick the best mode.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Magistral‑Small‑2509 Introduction\n\nA gentle, analogy‑driven walk‑through of the small Magistral model, covering installation, basic usage, and licensing. Designed for absolute beginners who want to see a model in action without deep coding knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\n",
        "By the end of this tutorial, you will be able to:\n\n",
        "1. Explain what the Magistral‑Small‑2509 model is and why it’s useful.\n",
        "2. Show how to install the required libraries and launch the model with vLLM.\n",
        "3. Demonstrate how to call the model from a simple Python client.\n",
        "4. Clarify the Apache‑2.0 license and proper attribution practices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- Basic familiarity with a terminal or command prompt.\n",
        "- Anaconda/Miniconda or a recent Python 3.10+ installation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nLet's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install packages (Colab-compatible)\n",
        "# Check if we're in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q ipywidgets>=8.0.0 vllm mistralai transformers\n",
        "else:\n",
        "    import subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [\"ipywidgets>=8.0.0\",\"vllm\",\"mistralai\",\"transformers\"]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print('✅ Packages installed!')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure ipywidgets is installed for interactive MCQs\n",
        "try:\n",
        "    import ipywidgets  # type: ignore\n",
        "    print('ipywidgets available')\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", '-q', 'ipywidgets>=8.0.0']\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Welcome to the first step of our journey into large language models. Think of a language model as a very sophisticated autocomplete that has read billions of words and can predict what comes next in a sentence. It works by learning statistical patterns in text: the probability of a word given the words that precede it. In practice, a language model takes a prompt, tokenizes it into sub‑word units, feeds those tokens through a deep neural network, and outputs a probability distribution over the next token. By sampling from that distribution repeatedly, the model can generate coherent paragraphs, code, or even poetry.\n",
        "\n",
        "A key concept is the *token*. Tokens are the smallest units the model processes; they can be characters, sub‑words, or whole words. Modern models use Byte‑Pair Encoding (BPE) or similar sub‑word tokenizers to balance vocabulary size and expressiveness. Training a language model involves minimizing the cross‑entropy loss between the model’s predictions and the actual next token in a massive corpus. The result is a model that can generalize to new prompts, even if it has never seen them before. In this notebook we will see how to load such a model, run inference locally, and understand the parameters that control its behavior.\n",
        "\n",
        "We’ll start with a quick environment check, then move on to a simple inference demo. By the end of this section you’ll understand what a language model is, how it works under the hood, and how to get it running on your machine.\n",
        "\n",
        "---\n",
        "\n",
        "**Quick verification steps**:\n",
        "\n",
        "1. Run the environment verification cell below.\n",
        "2. After it finishes, you should see the installed library versions and whether a CUDA device is available.\n",
        "3. If you see \"No CUDA device detected\", the demo will fall back to CPU.\n",
        "4. Once the environment is verified, the inference demo will generate a short story fragment.\n",
        "\n",
        "Feel free to experiment with the prompt or generation parameters in the demo cell.\n",
        "\n",
        "---\n",
        "\n",
        "In the next step we’ll install the required packages and set up the environment for running the model locally.\n",
        "\n",
        "Explain how the `HF_HOME` cache directory is configured so learners can control checkpoint storage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section we also discuss the difference between running a model locally versus using a hosted API like Poe. A local run gives you full control over the model, no network latency, and no usage limits, but it requires a compatible GPU and sufficient VRAM. Hosted APIs abstract away the hardware, provide instant access, and often include safety filters, but they incur network costs and may have rate limits.\n",
        "\n",
        "When you run locally, you must manage environment variables such as `HF_HOME` (where Hugging Face caches models) and `HF_TOKEN` (if the model is private). Setting `HF_HOME` to a directory with ample disk space ensures that large checkpoints do not clutter your home directory. For GPU drivers, make sure you have CUDA 12.1 or later and the corresponding cuDNN version; mismatched drivers are a common source of errors.\n",
        "\n",
        "If you prefer a quick start, the hosted API route is simpler: you just send a prompt and receive a response. However, for reproducibility and privacy, a local setup is preferable. The code cells below will show you how to verify your environment and run a small inference demo.\n",
        "\n",
        "---\n",
        "\n",
        "**Troubleshooting checklist**:\n",
        "\n",
        "- **GPU memory**: If you get an out‑of‑memory error, reduce `max_new_tokens` or use a smaller batch.\n",
        "- **Driver mismatch**: Verify `nvcc --version` matches the CUDA toolkit used by PyTorch.\n",
        "- **Tokenizer errors**: Ensure the tokenizer name matches the model name; mismatches can lead to decoding errors.\n",
        "\n",
        "---\n",
        "\n",
        "We’ll now dive into the code cells that perform these checks and demonstrate inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next up\n",
        "We will transition into Install the Required Packages. Verify that the environment verification cell prints GPU details, the inference demo returns coherent text, and telemetry has been captured for Harmony playback. Bring any questions about model configuration to the next section so we can compare advanced scheduling options.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### License and Model Details\n",
        "\n",
        "- **Model ID:** `mistralai/Magistral-Small-2509`\n",
        "- **License:** Apache-2.0 (commercial and non-commercial use permitted); see the model card for terms and attribution.\n",
        "- **Context Window:** 128k (performance may degrade past ~40k; keep max length unless you observe slowdowns).\n",
        "- **Reasoning:** Uses special THINK tokens; system prompt provided in `SYSTEM_PROMPT.txt` (loaded via `hf_hub_download`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment verification\n",
        "# Pinning versions ensures reproducibility\n",
        "import subprocess, sys\n",
        "\n",
        "def install(package):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", package]\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except Exception as exc:\n",
        "        if IN_COLAB:\n",
        "            packages = [arg for arg in cmd[4:] if isinstance(arg, str)]\n",
        "            if packages:\n",
        "                try:\n",
        "                    import IPython\n",
        "                    ip = IPython.get_ipython()\n",
        "                    if ip is not None:\n",
        "                        ip.run_line_magic('pip', 'install ' + ' '.join(packages))\n",
        "                    else:\n",
        "                        import subprocess as _subprocess\n",
        "                        _subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + packages)\n",
        "                except Exception as colab_exc:\n",
        "                    print('⚠️ Colab pip fallback failed:', colab_exc)\n",
        "                    raise\n",
        "            else:\n",
        "                print('No packages specified for pip install; skipping fallback')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "packages = [\n",
        "    \"transformers==4.41.2\",\n",
        "    \"accelerate==0.28.0\",\n",
        "    \"torch==2.2.0\",\n",
        "    \"ipywidgets==8.1.2\"\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        __import__(pkg.split(\"==\")[0])\n",
        "    except ImportError:\n",
        "        install(pkg)\n",
        "\n",
        "# Verify imports\n",
        "import torch\n",
        "from transformers import __version__ as transformers_version\n",
        "from accelerate import __version__ as accelerate_version\n",
        "import ipywidgets\n",
        "\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"transformers version: {transformers_version}\")\n",
        "print(f\"accelerate version: {accelerate_version}\")\n",
        "print(f\"ipywidgets version: {ipywidgets.__version__}\")\n",
        "\n",
        "# GPU status\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    props = torch.cuda.get_device_properties(device)\n",
        "    print(f\"CUDA device: {props.name}\")\n",
        "    print(f\"Total memory: {props.total_memory / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(\"No CUDA device detected. Using CPU.\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference demo with gpt-oss-20b\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Set deterministic seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "model_name = \"gpt-oss-20b\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Simple prompt\n",
        "prompt = \"Once upon a time in a distant galaxy,\"\n",
        "\n",
        "# Encode prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Generate text\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Check (Interactive)\n\n",
        "Use the widgets below to select an answer and click Grade to see feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MCQ helper (ipywidgets)\nimport ipywidgets as widgets\nfrom IPython.display import display, Markdown\n\ndef render_mcq(question, options, correct_index, explanation):\n    cleaned = []\n    for idx, raw in enumerate(options):\n        text = str(raw or \"\").strip()\n        prefix = f\"{chr(65+idx)}. \"\n        if not text.lower().startswith(prefix.lower()):\n            text = prefix + text\n        cleaned.append(text)\n    rb = widgets.RadioButtons(options=[(label, idx) for idx, label in enumerate(cleaned)], description=\"\")\n    grade_btn = widgets.Button(description='Grade', button_style='primary')\n    feedback = widgets.HTML(value='')\n    def on_grade(_):\n        sel = rb.value\n        if sel is None:\n            feedback.value = '<p>⚠️ Please select an option.</p>'\n            return\n        if sel == correct_index:\n            feedback.value = '<p>✅ Correct!</p>'\n        else:\n            feedback.value = f'<p>❌ Incorrect. Correct answer is {chr(65+correct_index)}.</p>'\n        feedback.value += f'<div><em>Explanation:</em> {explanation}</div>'\n    grade_btn.on_click(on_grade)\n    display(Markdown('### ' + question))\n    display(rb)\n    display(grade_btn)\n    display(feedback)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knowledge Check – Step 1: Welcome & What Is a Language Model?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which command is used to start the vLLM server for Magistral‑Small‑2509?\", [\"vllm serve mistralai/Magistral-Small-2509\",\"vllm start mistralai/Magistral-Small-2509\",\"vllm run mistralai/Magistral-Small-2509\",\"vllm launch mistralai/Magistral-Small-2509\"], 0, \"The correct syntax is `vllm serve <model_name>`. The other options are not valid vLLM commands.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"What does a temperature of 0.7 do in text generation?\", [\"Makes the output deterministic.\",\"Makes the output more random.\",\"Limits the length of the output.\",\"Enforces strict adherence to the prompt.\"], 1, \"Temperature controls randomness; 0.7 is a moderate value that balances creativity and coherence.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Under the Apache‑2.0 license, which of the following is required when redistributing the model?\", [\"You must pay a royalty fee.\",\"You must include the original license text.\",\"You must provide a link to the source code.\",\"You must rename the model.\"], 1, \"Apache‑2.0 requires that the license text be included with any redistribution.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "render_mcq(\"Which of these is a common pitfall when running vLLM on a GPU with limited memory?\", [\"Using a very high batch size.\",\"Setting temperature to 0.\",\"Running the server in CPU mode.\",\"Using the wrong model name.\"], 0, \"Large batch sizes can exhaust GPU memory; adjust batch size or use CPU mode if necessary.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Troubleshooting Guide\n\n",
        "### Common Issues:\n\n",
        "1. **Out of Memory Error**\n",
        "   - Enable GPU: Runtime → Change runtime type → GPU\n",
        "   - Restart runtime if needed\n\n",
        "2. **Package Installation Issues**\n",
        "   - Restart runtime after installing packages\n",
        "   - Use `!pip install -q` for quiet installation\n\n",
        "3. **Model Loading Fails**\n",
        "   - Check internet connection\n",
        "   - Verify authentication tokens\n",
        "   - Try CPU-only mode if GPU fails\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "alain": {
      "schemaVersion": "1.0.0",
      "createdAt": "2025-09-19T06:35:58.349Z",
      "title": "Magistral‑Small‑2509 Introduction",
      "builder": {
        "name": "alain-kit",
        "version": "0.2.0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}